{
  "components": {
    "comp-query-and-preprocess": {
      "executorLabel": "exec-query-and-preprocess",
      "inputDefinitions": {
        "parameters": {
          "attribute_columns": {
            "description": "List of categorical columns to group by",
            "parameterType": "LIST"
          },
          "bq_dataset": {
            "description": "BigQuery dataset name",
            "parameterType": "STRING"
          },
          "bq_source_table": {
            "description": "Source table name",
            "parameterType": "STRING"
          },
          "project_id": {
            "description": "GCP project ID",
            "parameterType": "STRING"
          },
          "project_location": {
            "description": "GCP project location/region",
            "parameterType": "STRING"
          },
          "target_column": {
            "parameterType": "STRING"
          },
          "time_column": {
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "output_dataset": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          }
        }
      }
    },
    "comp-split-rolling-forecast": {
      "executorLabel": "exec-split-rolling-forecast",
      "inputDefinitions": {
        "artifacts": {
          "input_dataset": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            },
            "description": "Input path for the preprocessed CSV dataset"
          }
        },
        "parameters": {
          "time_column": {
            "description": "Name of the timestamp column",
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "output_test1": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          },
          "output_test2": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          },
          "output_test3": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          },
          "output_test4": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          },
          "output_train1": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          },
          "output_train2": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          },
          "output_train3": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          },
          "output_train4": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          }
        }
      }
    }
  },
  "deploymentSpec": {
    "executors": {
      "exec-query-and-preprocess": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "query_and_preprocess"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==2.0.0' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef query_and_preprocess(\n    project_id: str,\n    project_location: str,\n    bq_dataset: str,\n    bq_source_table: str,\n    time_column: str,\n    target_column: str,\n    attribute_columns: List[str],\n    output_dataset: Output[Dataset]\n):\n    \"\"\" \n    Queries BigQuery data, performs preprocessing, and exports to a CSV dataset for training.\n    The function aggregates data by time and attributes, creates a unique series identifier,\n    and handles categorical features appropriately.\n\n    Args:\n        project_id: GCP project ID\n        project_location: GCP project location/region\n        bq_dataset: BigQuery dataset name\n        bq_source_table: Source table name\n        attribute_columns: List of categorical columns to group by\n        output_dataset: Output path for the preprocessed CSV dataset\n\n    Returns:\n        Writes a preprocessed CSV file to the output_dataset path containing:\n        - Series_Identifier: Concatenated string of attribute values\n        - Appointment_Day: Timestamp column\n        - Attribute columns: Original categorical features\n        - SWT: Aggregated target variable\n    \"\"\"\n\n    import datetime\n    import pandas as pd\n    from google.cloud import bigquery\n    from google.cloud import aiplatform\n\n\n    def create_series_identifier(columns):\n        coalesce_parts = [f\"COALESCE({column}, 'None')\" for column in columns]\n        separator = \"' '\"\n        return f\"CONCAT({f', {separator}, '.join(coalesce_parts)}) AS Series_Identifier\"\n\n    time_column = \"Appointment_Day\"\n    target_column = \"SWT\"\n\n    FORECAST_TIMESTAMP = datetime.datetime.now()\n    ATTRIBUTE_STRING = ','.join(attribute_columns)\n\n    COLUMN_SPECS = {\n        time_column:             \"timestamp\",\n        target_column:           \"numeric\"\n    }\n\n    for category in attribute_columns:\n        COLUMN_SPECS[category] = \"categorical\"\n\n    experiment_train_data_query = f\"\"\"\n    WITH historical_table AS (\n        SELECT \n            {time_column},\n            {ATTRIBUTE_STRING},\n            SUM({target_column}) AS {target_column}\n        FROM `{project_id}.{bq_dataset}.{bq_source_table}`\n        WHERE {time_column} <= DATE('2025-03-31')\n        GROUP BY {time_column},{ATTRIBUTE_STRING}\n    )\n    SELECT \n        {create_series_identifier(attribute_columns)},\n        {time_column},\n        {ATTRIBUTE_STRING},\n        {target_column}\n    FROM historical_table\n    \"\"\"\n\n    client = bigquery.Client(\n        project=project_id,\n        location=project_location)\n\n    processed_data = client.query(experiment_train_data_query).to_dataframe()\n\n    processed_data.to_csv(output_dataset.path, index=False)\n    print(f\"CSV file written to {output_dataset.path}\")\n\n"
          ],
          "image": "northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/kfp-preprocess-slim:2.0.1"
        }
      },
      "exec-split-rolling-forecast": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "split_rolling_forecast"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==2.0.0' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef split_rolling_forecast(\n    input_dataset: Input[Dataset],\n    time_column: str,\n    output_train1: Output[Dataset],\n    output_test1: Output[Dataset],\n    output_train2: Output[Dataset],\n    output_test2: Output[Dataset],\n    output_train3: Output[Dataset],\n    output_test3: Output[Dataset],\n    output_train4: Output[Dataset],\n    output_test4: Output[Dataset],\n):\n    \"\"\"\n    Splits the preprocessed data into rolling forecast datasets for model training.\n\n    Args:\n        input_dataset: Input path for the preprocessed CSV dataset\n        time_column: Name of the timestamp column\n    \"\"\"\n\n    import pandas as pd\n\n    forecast_processed_data = pd.read_csv(input_dataset.path, index_col=False)\n    forecast_processed_data[time_column] = pd.to_datetime(forecast_processed_data[time_column])\n\n    splits = [\n        {\n            \"train_start\": \"2022-01-01\",\n            \"train_end\":   \"2024-03-31\",\n            \"test_start\":  \"2024-04-01\",\n            \"test_end\":    \"2024-06-30\"\n        },\n        {\n            \"train_start\": \"2022-01-01\",\n            \"train_end\":   \"2024-06-30\",\n            \"test_start\":  \"2024-07-01\",\n            \"test_end\":    \"2024-09-30\"\n        },\n        {\n            \"train_start\": \"2022-01-01\",\n            \"train_end\":   \"2024-09-30\",\n            \"test_start\":  \"2024-10-01\",\n            \"test_end\":    \"2024-12-31\"\n        },\n        {\n            \"train_start\": \"2022-01-01\",\n            \"train_end\":   \"2024-12-31\",\n            \"test_start\":  \"2025-01-01\",\n            \"test_end\":    \"2025-03-31\"\n        },\n    ]\n\n    for s in splits:\n        s[\"train_start\"] = pd.to_datetime(s[\"train_start\"])\n        s[\"train_end\"]   = pd.to_datetime(s[\"train_end\"])\n        s[\"test_start\"]  = pd.to_datetime(s[\"test_start\"])\n        s[\"test_end\"]    = pd.to_datetime(s[\"test_end\"])\n\n    train_test_pairs = []\n    for s in splits:\n        train_mask = (forecast_processed_data[time_column] >= s[\"train_start\"]) \\\n                   & (forecast_processed_data[time_column] <= s[\"train_end\"])\n\n        test_mask = (forecast_processed_data[time_column] >= s[\"test_start\"]) \\\n                  & (forecast_processed_data[time_column] <= s[\"test_end\"])\n\n        train_df = forecast_processed_data.loc[train_mask].copy()\n        test_df = forecast_processed_data.loc[test_mask].copy()\n        train_test_pairs.append((train_df, test_df))\n\n\n    train1_df, test1_df = train_test_pairs[0]\n    train2_df, test2_df = train_test_pairs[1]\n    train3_df, test3_df = train_test_pairs[2]\n    train4_df, test4_df = train_test_pairs[3]\n\n    train1_df.to_csv(output_train1.path, index=False)\n    test1_df.to_csv(output_test1.path, index=False)\n\n    train2_df.to_csv(output_train2.path, index=False)\n    test2_df.to_csv(output_test2.path, index=False)\n\n    train3_df.to_csv(output_train3.path, index=False)\n    test3_df.to_csv(output_test3.path, index=False)\n\n    train4_df.to_csv(output_train4.path, index=False)\n    test4_df.to_csv(output_test4.path, index=False)\n\n"
          ],
          "image": "northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/mlops/kfp-2.0.0/kfp-load-model-slim:1.0.0"
        }
      }
    }
  },
  "pipelineInfo": {
    "description": "A Kubeflow pipeline for training forecast models using AutoML Forecast on Vertex AI Pipelines from a BigQuery view.",
    "name": "forecast-training-pipeline"
  },
  "root": {
    "dag": {
      "tasks": {
        "query-and-preprocess": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-query-and-preprocess"
          },
          "inputs": {
            "parameters": {
              "attribute_columns": {
                "componentInputParameter": "attribute_columns"
              },
              "bq_dataset": {
                "componentInputParameter": "bq_dataset"
              },
              "bq_source_table": {
                "componentInputParameter": "bq_source_table"
              },
              "project_id": {
                "componentInputParameter": "project_id"
              },
              "project_location": {
                "componentInputParameter": "project_location"
              },
              "target_column": {
                "componentInputParameter": "target_column"
              },
              "time_column": {
                "componentInputParameter": "time_column"
              }
            }
          },
          "taskInfo": {
            "name": "query-and-preprocess"
          }
        },
        "split-rolling-forecast": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-split-rolling-forecast"
          },
          "dependentTasks": [
            "query-and-preprocess"
          ],
          "inputs": {
            "artifacts": {
              "input_dataset": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "output_dataset",
                  "producerTask": "query-and-preprocess"
                }
              }
            },
            "parameters": {
              "time_column": {
                "componentInputParameter": "time_column"
              }
            }
          },
          "taskInfo": {
            "name": "split-rolling-forecast"
          }
        }
      }
    },
    "inputDefinitions": {
      "parameters": {
        "attribute_columns": {
          "parameterType": "LIST"
        },
        "bq_dataset": {
          "parameterType": "STRING"
        },
        "bq_source_table": {
          "parameterType": "STRING"
        },
        "project_id": {
          "parameterType": "STRING"
        },
        "project_location": {
          "parameterType": "STRING"
        },
        "target_column": {
          "parameterType": "STRING"
        },
        "time_column": {
          "parameterType": "STRING"
        }
      }
    }
  },
  "schemaVersion": "2.1.0",
  "sdkVersion": "kfp-2.0.0"
}