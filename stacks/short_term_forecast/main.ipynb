{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a01939f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.compiler as compiler\n",
    "from kfp.dsl import component, pipeline, Input, Output, Dataset\n",
    "\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google_cloud_pipeline_components.v1.custom_job import create_custom_training_job_from_component\n",
    "\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from src.ShortTermForecast.constants import *\n",
    "from src.ShortTermForecast.utils.common import read_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9d6d8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-15 19:55:53,501: INFO: common] yaml file: config/config.yaml loaded successfully\n"
     ]
    }
   ],
   "source": [
    "CONFIG = read_yaml(CONFIG_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "298f2f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_ingestion_op\n",
    "@component(\n",
    "    base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/b2b_ai/wf_pipeline/training:1.0.1-rc\"\n",
    ")\n",
    "def data_ingestion_op(\n",
    "    project_id: str,\n",
    "    project_location: str,\n",
    "    bq_dataset: str,\n",
    "    bq_source_table: str,\n",
    "    time_column: str,\n",
    "    target_column: str,\n",
    "    series_identifier: str,\n",
    "    attribute_columns: List[str],\n",
    "    output_dataset: Output[Dataset]\n",
    "):\n",
    "    from dataclasses import dataclass\n",
    "    from google.cloud import bigquery\n",
    "    from pathlib import Path\n",
    "    from typing import List\n",
    "    import os\n",
    "    import sys\n",
    "    import logging\n",
    "\n",
    "\n",
    "    logging_str = \"[%(asctime)s: %(levelname)s: %(module)s] %(message)s\"\n",
    "\n",
    "    log_dir = 'logs'\n",
    "    log_pathfile = os.path.join(log_dir, 'running_logs.log')\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=logging_str,\n",
    "\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_pathfile),\n",
    "            logging.StreamHandler(sys.stdout)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    logger = logging.getLogger(\"Match-Analysis\")\n",
    "\n",
    "    @dataclass(frozen=True)\n",
    "    class DataIngestionConfig:\n",
    "        root_dir: Path\n",
    "        local_file_name: str\n",
    "        project_id: str\n",
    "        project_location: str\n",
    "        bq_dataset: str\n",
    "        bq_source_table: str\n",
    "        time_column: str\n",
    "        target_column: str\n",
    "        series_identifier: str\n",
    "        attribute_columns: List[str]\n",
    "\n",
    "    class ConfigurationManager:\n",
    "        def __init__(self):\n",
    "            pass\n",
    "        def get_data_ingestion_kfp_config(\n",
    "                self,\n",
    "                project_id: str,\n",
    "                project_location: str,\n",
    "                bq_dataset: str,\n",
    "                bq_source_table: str,\n",
    "                time_column: str,\n",
    "                target_column: str,\n",
    "                series_identifier: str,\n",
    "                attribute_columns: List[str]\n",
    "        ) -> DataIngestionConfig:\n",
    "            \"\"\"\n",
    "            Returns a DataIngestionConfig configured for Kubeflow Pipeline runs.\n",
    "            \"\"\"\n",
    "\n",
    "            return DataIngestionConfig(\n",
    "                root_dir=None,\n",
    "                local_file_name=None,\n",
    "                project_id=project_id,\n",
    "                project_location=project_location,\n",
    "                bq_dataset=bq_dataset,\n",
    "                bq_source_table=bq_source_table,\n",
    "                time_column=time_column,\n",
    "                target_column=target_column,\n",
    "                series_identifier=series_identifier,\n",
    "                attribute_columns=attribute_columns\n",
    "            )        \n",
    "        \n",
    "    class DataIngestion:\n",
    "        def __init__(self, config: DataIngestionConfig):\n",
    "            self.config = config\n",
    "            self.client = None\n",
    "            self.data = None\n",
    "            logger.info(\"DataIngestion instance initialized with provided configuration.\")\n",
    "\n",
    "        def _create_series_identifier(self) -> str:\n",
    "            coalesce_parts = [f\"COALESCE({column}, 'None')\" for column in self.config.attribute_columns]\n",
    "            separator = \"' '\"\n",
    "            series_identifier = f\"CONCAT({f', {separator}, '.join(coalesce_parts)}) AS {self.config.series_identifier}\"\n",
    "            logger.debug(f\"Created series identifier: {series_identifier}\")\n",
    "            return series_identifier\n",
    "\n",
    "        def load(self):\n",
    "            logger.info(\"Initializing BigQuery client and loading data.\")\n",
    "            self.client = bigquery.Client(\n",
    "                project=self.config.project_id,\n",
    "                location=self.config.project_location\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                self.data = self.client.query(self.data_ingestion_query).to_dataframe()\n",
    "                logger.info(f\"Data loaded successfully. Shape: {self.data.shape}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading data from BigQuery: {str(e)}\")\n",
    "                raise\n",
    "\n",
    "        def save(self, save_path: str = None):\n",
    "            if save_path is None:\n",
    "                save_path = Path(self.config.root_dir, self.config.local_file_name)\n",
    "            \n",
    "            logger.info(f\"Saving data to {save_path}\")\n",
    "            try:\n",
    "                self.data.to_csv(save_path, index=False)\n",
    "                logger.info(\"Data saved successfully.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error saving data to CSV: {str(e)}\")\n",
    "                raise\n",
    "\n",
    "        @property\n",
    "        def data_ingestion_query(self) -> str:\n",
    "            query = f\"\"\"\n",
    "            WITH historical_table AS (\n",
    "                SELECT \n",
    "                    {self.config.time_column},\n",
    "                    {self.attribute_string},\n",
    "                    SUM({self.config.target_column}) AS {self.config.target_column}\n",
    "                FROM \n",
    "                    `{self.config.project_id}.{self.config.bq_dataset}.{self.config.bq_source_table}`\n",
    "                WHERE \n",
    "                    {self.config.time_column} <= DATE('2025-03-31')\n",
    "                GROUP BY \n",
    "                    {self.config.time_column},\n",
    "                    {self.attribute_string}\n",
    "            )\n",
    "            SELECT \n",
    "                {self._create_series_identifier()},\n",
    "                {self.config.time_column},\n",
    "                {self.attribute_string},\n",
    "                {self.config.target_column}\n",
    "            FROM historical_table\n",
    "            \"\"\"\n",
    "            logger.debug(\"Generated data ingestion query.\")\n",
    "            return query\n",
    "\n",
    "        @property\n",
    "        def attribute_string(self) -> str:\n",
    "            return ','.join(self.config.attribute_columns)\n",
    "\n",
    "    data_ingestion_config = ConfigurationManager().get_data_ingestion_kfp_config(\n",
    "        project_id=project_id,\n",
    "        project_location=project_location,\n",
    "        bq_dataset=bq_dataset,\n",
    "        bq_source_table=bq_source_table,\n",
    "        time_column=time_column,\n",
    "        target_column=target_column,\n",
    "        series_identifier=series_identifier,\n",
    "        attribute_columns=attribute_columns\n",
    "    )\n",
    "\n",
    "\n",
    "    data_ingestion = DataIngestion(data_ingestion_config)\n",
    "\n",
    "    data_ingestion.load()\n",
    "    data_ingestion.save(output_dataset.path)\n",
    "\n",
    "data_ingestion_job = create_custom_training_job_from_component(\n",
    "    data_ingestion_op,\n",
    "    display_name='data-ingestion-component',\n",
    "    machine_type='e2-standard-4'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5042c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_time_series_cv_op\n",
    "@component(\n",
    "    base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/b2b_ai/wf_pipeline/training:1.0.1-rc\"\n",
    ")\n",
    "def generate_time_series_cv_op(\n",
    "    input_dataset: Input[Dataset],\n",
    "    time_column: str,\n",
    "    forecast_horizon: int,\n",
    "    output_train: Output[Dataset],\n",
    "    output_test: Output[Dataset]\n",
    "):\n",
    "    from src.ShortTermForecast.components.time_series_cv import TimeSeriesCV\n",
    "    from src.ShortTermForecast.config.configuration import ConfigurationManager\n",
    "\n",
    "    config = ConfigurationManager(\n",
    "        config_filepath=None, \n",
    "        params_filepath=None\n",
    "    )\n",
    "\n",
    "    time_series_cv_config = config.get_cross_validation_split_kfp_config(\n",
    "        input_dataset=input_dataset.path,\n",
    "        time_column=time_column,\n",
    "        forecast_horizon=forecast_horizon,\n",
    "    )\n",
    "\n",
    "    time_series_cv = TimeSeriesCV(time_series_cv_config)\n",
    "\n",
    "    time_series_cv.load()\n",
    "    time_series_cv.generate_splits()\n",
    "    time_series_cv.process_splits()\n",
    "    time_series_cv.save(\n",
    "        save_train_path=output_train.path,\n",
    "        save_test_path=output_test.path\n",
    "    )\n",
    "\n",
    "generate_time_series_cv_job = create_custom_training_job_from_component(\n",
    "    generate_time_series_cv_op,\n",
    "    display_name='generate-timeseries-cv-job',\n",
    "    machine_type='e2-standard-4'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86b3527e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline(\n",
    "    name=\"b2b-wf-short-term-prediction-experiments\",\n",
    "    description=\"A Kubeflow pipeline for training forecast models using AutoML Forecast on Vertex AI Pipelines from a BigQuery view.\"\n",
    ")\n",
    "def forecast_pipeline(\n",
    "    project_id: str,\n",
    "    project_location: str,\n",
    "    bq_dataset: str,\n",
    "    bq_source_table: str,\n",
    "    time_column: str,\n",
    "    target_column: str,\n",
    "    series_identifier: str,\n",
    "    attribute_columns: List[str],\n",
    "    forecast_horizon: int\n",
    "):\n",
    "    \n",
    "    data_ingestion_task = data_ingestion_job(\n",
    "        project_id=project_id,\n",
    "        project_location=project_location,\n",
    "        bq_dataset=bq_dataset,\n",
    "        bq_source_table=bq_source_table,\n",
    "        time_column=time_column,\n",
    "        target_column=target_column,\n",
    "        series_identifier=series_identifier,\n",
    "        attribute_columns=attribute_columns\n",
    "    )\n",
    "    \n",
    "    generate_time_series_cv_task = generate_time_series_cv_job(\n",
    "        input_dataset=data_ingestion_task.outputs['output_dataset'],\n",
    "        time_column=time_column,\n",
    "        forecast_horizon=forecast_horizon,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedd6b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "[2025-04-15 19:55:56,866: INFO: base] Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/7796273458/locations/northamerica-northeast1/pipelineJobs/b2b-wf-short-term-prediction-experiments-20250415195553\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/7796273458/locations/northamerica-northeast1/pipelineJobs/b2b-wf-short-term-prediction-experiments-20250415195553')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/northamerica-northeast1/pipelines/runs/b2b-wf-short-term-prediction-experiments-20250415195553?project=7796273458\n",
      "PipelineJob projects/7796273458/locations/northamerica-northeast1/pipelineJobs/b2b-wf-short-term-prediction-experiments-20250415195553 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/7796273458/locations/northamerica-northeast1/pipelineJobs/b2b-wf-short-term-prediction-experiments-20250415195553 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/7796273458/locations/northamerica-northeast1/pipelineJobs/b2b-wf-short-term-prediction-experiments-20250415195553 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "aiplatform.init(\n",
    "    project=CONFIG.general_setup.project_id,\n",
    "    location=CONFIG.general_setup.project_location,\n",
    "    staging_bucket=f\"gs://{CONFIG.general_setup.project_id}_{CONFIG.general_setup.gcs_bucket}\"\n",
    ")\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=forecast_pipeline,\n",
    "    package_path=CONFIG.general_setup.pipeline_package_path\n",
    ")\n",
    "\n",
    "\n",
    "job = pipeline_jobs.PipelineJob(\n",
    "    display_name=\"b2b_wf_short_term_prediction_sma_pipeline\",\n",
    "    template_path=CONFIG.general_setup.pipeline_package_path,\n",
    "    parameter_values={\n",
    "        'project_id':        CONFIG.general_setup.project_id,\n",
    "        'project_location':  CONFIG.general_setup.project_location,\n",
    "        'bq_dataset':        CONFIG.data_ingestion.bq_dataset,\n",
    "        'bq_source_table':   CONFIG.data_ingestion.bq_source_table,\n",
    "        'time_column':       CONFIG.general_setup.time_column,\n",
    "        'target_column':     CONFIG.general_setup.target_column,\n",
    "        'series_identifier': CONFIG.general_setup.series_identifier,\n",
    "        'attribute_columns': CONFIG.general_setup.attribute_columns,\n",
    "        'forecast_horizon':  CONFIG.general_setup.forecast_horizon\n",
    "    }\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd70954b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
