{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a01939f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.compiler as compiler\n",
    "from kfp.dsl import component, pipeline, Input, Output, Dataset\n",
    "\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google_cloud_pipeline_components.v1.custom_job import create_custom_training_job_from_component\n",
    "\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from src.ShortTermForecast.constants import *\n",
    "from src.ShortTermForecast.utils.common import read_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9d6d8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-15 20:25:12,065: INFO: common] yaml file: config/config.yaml loaded successfully\n"
     ]
    }
   ],
   "source": [
    "CONFIG = read_yaml(CONFIG_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298f2f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_ingestion_op\n",
    "@component(\n",
    "    base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/b2b_ai/wf_pipeline/training:1.0.1-rc\"\n",
    ")\n",
    "def data_ingestion_op(\n",
    "    project_id: str,\n",
    "    project_location: str,\n",
    "    bq_dataset: str,\n",
    "    bq_source_table: str,\n",
    "    time_column: str,\n",
    "    target_column: str,\n",
    "    series_identifier: str,\n",
    "    attribute_columns: List[str],\n",
    "    output_dataset: Output[Dataset]\n",
    "):\n",
    "    from dataclasses import dataclass\n",
    "    from google.cloud import bigquery\n",
    "    from pathlib import Path\n",
    "    from typing import List\n",
    "    import logging\n",
    "\n",
    "\n",
    "    logging_str = \"[%(asctime)s: %(levelname)s: %(module)s] %(message)s\"\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=logging_str\n",
    "    )\n",
    "\n",
    "    logger = logging.getLogger(\"Match-Analysis\")\n",
    "\n",
    "    @dataclass(frozen=True)\n",
    "    class DataIngestionConfig:\n",
    "        root_dir: Path\n",
    "        local_file_name: str\n",
    "        project_id: str\n",
    "        project_location: str\n",
    "        bq_dataset: str\n",
    "        bq_source_table: str\n",
    "        time_column: str\n",
    "        target_column: str\n",
    "        series_identifier: str\n",
    "        attribute_columns: List[str]\n",
    "\n",
    "    class ConfigurationManager:\n",
    "        def __init__(self):\n",
    "            pass\n",
    "        def get_data_ingestion_kfp_config(\n",
    "                self,\n",
    "                project_id: str,\n",
    "                project_location: str,\n",
    "                bq_dataset: str,\n",
    "                bq_source_table: str,\n",
    "                time_column: str,\n",
    "                target_column: str,\n",
    "                series_identifier: str,\n",
    "                attribute_columns: List[str]\n",
    "        ) -> DataIngestionConfig:\n",
    "            \"\"\"\n",
    "            Returns a DataIngestionConfig configured for Kubeflow Pipeline runs.\n",
    "            \"\"\"\n",
    "\n",
    "            return DataIngestionConfig(\n",
    "                root_dir=None,\n",
    "                local_file_name=None,\n",
    "                project_id=project_id,\n",
    "                project_location=project_location,\n",
    "                bq_dataset=bq_dataset,\n",
    "                bq_source_table=bq_source_table,\n",
    "                time_column=time_column,\n",
    "                target_column=target_column,\n",
    "                series_identifier=series_identifier,\n",
    "                attribute_columns=attribute_columns\n",
    "            )        \n",
    "        \n",
    "    class DataIngestion:\n",
    "        def __init__(self, config: DataIngestionConfig):\n",
    "            self.config = config\n",
    "            self.client = None\n",
    "            self.data = None\n",
    "            logger.info(\"DataIngestion instance initialized with provided configuration.\")\n",
    "\n",
    "        def _create_series_identifier(self) -> str:\n",
    "            coalesce_parts = [f\"COALESCE({column}, 'None')\" for column in self.config.attribute_columns]\n",
    "            separator = \"' '\"\n",
    "            series_identifier = f\"CONCAT({f', {separator}, '.join(coalesce_parts)}) AS {self.config.series_identifier}\"\n",
    "            logger.debug(f\"Created series identifier: {series_identifier}\")\n",
    "            return series_identifier\n",
    "\n",
    "        def load(self):\n",
    "            logger.info(\"Initializing BigQuery client and loading data.\")\n",
    "            self.client = bigquery.Client(\n",
    "                project=self.config.project_id,\n",
    "                location=self.config.project_location\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                self.data = self.client.query(self.data_ingestion_query).to_dataframe()\n",
    "                logger.info(f\"Data loaded successfully. Shape: {self.data.shape}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading data from BigQuery: {str(e)}\")\n",
    "                raise\n",
    "\n",
    "        def save(self, save_path: str = None):\n",
    "            if save_path is None:\n",
    "                save_path = Path(self.config.root_dir, self.config.local_file_name)\n",
    "            \n",
    "            logger.info(f\"Saving data to {save_path}\")\n",
    "            try:\n",
    "                self.data.to_csv(save_path, index=False)\n",
    "                logger.info(\"Data saved successfully.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error saving data to CSV: {str(e)}\")\n",
    "                raise\n",
    "\n",
    "        @property\n",
    "        def data_ingestion_query(self) -> str:\n",
    "            query = f\"\"\"\n",
    "            WITH historical_table AS (\n",
    "                SELECT \n",
    "                    {self.config.time_column},\n",
    "                    {self.attribute_string},\n",
    "                    SUM({self.config.target_column}) AS {self.config.target_column}\n",
    "                FROM \n",
    "                    `{self.config.project_id}.{self.config.bq_dataset}.{self.config.bq_source_table}`\n",
    "                WHERE \n",
    "                    {self.config.time_column} <= DATE('2025-03-31')\n",
    "                GROUP BY \n",
    "                    {self.config.time_column},\n",
    "                    {self.attribute_string}\n",
    "            )\n",
    "            SELECT \n",
    "                {self._create_series_identifier()},\n",
    "                {self.config.time_column},\n",
    "                {self.attribute_string},\n",
    "                {self.config.target_column}\n",
    "            FROM historical_table\n",
    "            \"\"\"\n",
    "            logger.debug(\"Generated data ingestion query.\")\n",
    "            return query\n",
    "\n",
    "        @property\n",
    "        def attribute_string(self) -> str:\n",
    "            return ','.join(self.config.attribute_columns)\n",
    "\n",
    "    data_ingestion_config = ConfigurationManager().get_data_ingestion_kfp_config(\n",
    "        project_id=project_id,\n",
    "        project_location=project_location,\n",
    "        bq_dataset=bq_dataset,\n",
    "        bq_source_table=bq_source_table,\n",
    "        time_column=time_column,\n",
    "        target_column=target_column,\n",
    "        series_identifier=series_identifier,\n",
    "        attribute_columns=attribute_columns\n",
    "    )\n",
    "\n",
    "\n",
    "    data_ingestion = DataIngestion(data_ingestion_config)\n",
    "\n",
    "    data_ingestion.load()\n",
    "    data_ingestion.save(output_dataset.path)\n",
    "\n",
    "data_ingestion_job = create_custom_training_job_from_component(\n",
    "    data_ingestion_op,\n",
    "    display_name='data-ingestion-component',\n",
    "    machine_type='e2-standard-4'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5042c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_time_series_cv_op\n",
    "@component(\n",
    "    base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/b2b_ai/wf_pipeline/training:1.0.1-rc\"\n",
    ")\n",
    "def generate_time_series_cv_op(\n",
    "    input_dataset: Input[Dataset],\n",
    "    time_column: str,\n",
    "    forecast_horizon: int,\n",
    "    output_train: Output[Dataset],\n",
    "    output_test: Output[Dataset]\n",
    "):\n",
    "    from dataclasses import dataclass\n",
    "    from pathlib import Path\n",
    "    import logging\n",
    "    import pandas as pd\n",
    "    from datetime import timedelta\n",
    "    from pathlib import Path\n",
    "\n",
    "\n",
    "    logging_str = \"[%(asctime)s: %(levelname)s: %(module)s] %(message)s\"\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=logging_str\n",
    "    )\n",
    "\n",
    "    logger = logging.getLogger(\"Match-Analysis\")\n",
    "    \n",
    "    @dataclass(frozen=True)\n",
    "    class CrossValSplitConfig:\n",
    "        root_dir: Path\n",
    "        input_dataset: Path\n",
    "        time_column: str\n",
    "        forecast_horizon: int\n",
    "        train_file_name: str\n",
    "        test_file_name: str\n",
    "    \n",
    "    class ConfigurationManager:\n",
    "        def __init__(self):\n",
    "            pass\n",
    "        def get_cross_validation_split_kfp_config(\n",
    "                self,\n",
    "                input_dataset: str,\n",
    "                time_column: str,\n",
    "                forecast_horizon: int\n",
    "        ) -> CrossValSplitConfig:\n",
    "\n",
    "            return CrossValSplitConfig(\n",
    "                root_dir=None,\n",
    "                input_dataset=input_dataset,\n",
    "                time_column=time_column,\n",
    "                forecast_horizon=forecast_horizon,\n",
    "                train_file_name=None,\n",
    "                test_file_name=None\n",
    "            )   \n",
    "\n",
    "    class TimeSeriesCV:\n",
    "        def __init__(self, config: CrossValSplitConfig) -> None:\n",
    "            self.config = config\n",
    "            self.data = None\n",
    "            self.splits = None\n",
    "            logger.info(\"TimeSeriesCV instance initialized with provided configuration.\")\n",
    "\n",
    "        def load(self):\n",
    "            logger.info(f\"Reading input dataset from: {self.config.input_dataset}\")\n",
    "            try:\n",
    "                self.data = pd.read_csv(self.config.input_dataset)\n",
    "                self.data[self.config.time_column] = pd.to_datetime(self.data[self.config.time_column])\n",
    "                logger.info(f\"Data loaded successfully. Shape: {self.data.shape}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading data: {str(e)}\")\n",
    "                raise\n",
    "\n",
    "        def generate_splits(self):\n",
    "            logger.info(\"Generating time series cross-validation splits\")\n",
    "            \n",
    "            train_start = pd.to_datetime(\"2022-01-01\")\n",
    "            splits = []\n",
    "            current_train_end = pd.to_datetime(\"2024-03-31\")\n",
    "            \n",
    "            for split_index in range(1, 5):\n",
    "                test_start = current_train_end + timedelta(days=1)\n",
    "                test_end = test_start + timedelta(days=self.config.forecast_horizon-1)\n",
    "                \n",
    "                splits.append({\n",
    "                    \"split_index\": split_index,\n",
    "                    \"train_start\": train_start,\n",
    "                    \"train_end\": current_train_end,\n",
    "                    \"test_start\": test_start,\n",
    "                    \"test_end\": test_end\n",
    "                })\n",
    "                \n",
    "                current_train_end = test_end\n",
    "            \n",
    "            self.splits = splits\n",
    "            \n",
    "            logger.info(\"Generated splits:\")\n",
    "            for s in splits:\n",
    "                logger.info(f\"Split {s['split_index']}:\")\n",
    "                logger.info(f\"  Train: {s['train_start'].strftime('%Y-%m-%d')} to {s['train_end'].strftime('%Y-%m-%d')}\")\n",
    "                logger.info(f\"  Test:  {s['test_start'].strftime('%Y-%m-%d')} to {s['test_end'].strftime('%Y-%m-%d')}\")\n",
    "\n",
    "        def process_splits(self):\n",
    "            if self.splits is None:\n",
    "                logger.error(\"Splits have not been generated. Call generate_splits() first.\")\n",
    "                raise ValueError(\"Splits not generated\")\n",
    "                \n",
    "            all_train_data = []\n",
    "            all_test_data = []\n",
    "            \n",
    "            for s in self.splits:\n",
    "                logger.info(f\"\\nProcessing split {s['split_index']}\")\n",
    "                \n",
    "                train_mask = (self.data[self.config.time_column] >= s[\"train_start\"]) & \\\n",
    "                            (self.data[self.config.time_column] <= s[\"train_end\"])\n",
    "                \n",
    "                test_mask = (self.data[self.config.time_column] >= s[\"test_start\"]) & \\\n",
    "                        (self.data[self.config.time_column] <= s[\"test_end\"])\n",
    "\n",
    "                train_df = self.data.loc[train_mask].copy()\n",
    "                test_df = self.data.loc[test_mask].copy()\n",
    "                \n",
    "                train_df['split_index'] = s['split_index']\n",
    "                test_df['split_index'] = s['split_index']\n",
    "                \n",
    "                all_train_data.append(train_df)\n",
    "                all_test_data.append(test_df)\n",
    "                \n",
    "                logger.info(f\"Split {s['split_index']} - Train shape: {train_df.shape}, Test shape: {test_df.shape}\")\n",
    "            \n",
    "\n",
    "            self.train = pd.concat(all_train_data, ignore_index=True)\n",
    "            self.test = pd.concat(all_test_data, ignore_index=True)\n",
    "\n",
    "        def save(self, save_train_path: str = None, save_test_path: str = None):\n",
    "            if save_train_path is None:\n",
    "                save_train_path = Path(self.config.root_dir, self.config.train_file_name)\n",
    "            if save_test_path is None:\n",
    "                save_test_path = Path(self.config.root_dir, self.config.test_file_name)\n",
    "                \n",
    "            logger.info(f\"Saving combined training data (shape: {self.train.shape}) to {save_train_path}\")\n",
    "            self.train.to_csv(save_train_path, index=False)\n",
    "            \n",
    "            logger.info(f\"Saving combined test data (shape: {self.test.shape}) to {save_test_path}\")\n",
    "            self.test.to_csv(save_test_path, index=False)\n",
    "\n",
    "        \n",
    "    config = ConfigurationManager()\n",
    "\n",
    "    time_series_cv_config = config.get_cross_validation_split_kfp_config(\n",
    "        input_dataset=input_dataset.path,\n",
    "        time_column=time_column,\n",
    "        forecast_horizon=forecast_horizon,\n",
    "    )\n",
    "\n",
    "    time_series_cv = TimeSeriesCV(time_series_cv_config)\n",
    "\n",
    "    time_series_cv.load()\n",
    "    time_series_cv.generate_splits()\n",
    "    time_series_cv.process_splits()\n",
    "    time_series_cv.save(\n",
    "        save_train_path=output_train.path,\n",
    "        save_test_path=output_test.path\n",
    "    )\n",
    "\n",
    "generate_time_series_cv_job = create_custom_training_job_from_component(\n",
    "    generate_time_series_cv_op,\n",
    "    display_name='generate-timeseries-cv-job',\n",
    "    machine_type='e2-standard-4'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86b3527e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline(\n",
    "    name=\"b2b-wf-short-term-prediction-experiments\",\n",
    "    description=\"A Kubeflow pipeline for training forecast models using AutoML Forecast on Vertex AI Pipelines from a BigQuery view.\"\n",
    ")\n",
    "def forecast_pipeline(\n",
    "    project_id: str,\n",
    "    project_location: str,\n",
    "    bq_dataset: str,\n",
    "    bq_source_table: str,\n",
    "    time_column: str,\n",
    "    target_column: str,\n",
    "    series_identifier: str,\n",
    "    attribute_columns: List[str],\n",
    "    forecast_horizon: int\n",
    "):\n",
    "    \n",
    "    data_ingestion_task = data_ingestion_job(\n",
    "        project_id=project_id,\n",
    "        project_location=project_location,\n",
    "        bq_dataset=bq_dataset,\n",
    "        bq_source_table=bq_source_table,\n",
    "        time_column=time_column,\n",
    "        target_column=target_column,\n",
    "        series_identifier=series_identifier,\n",
    "        attribute_columns=attribute_columns\n",
    "    )\n",
    "    \n",
    "    generate_time_series_cv_task = generate_time_series_cv_job(\n",
    "        input_dataset=data_ingestion_task.outputs['output_dataset'],\n",
    "        time_column=time_column,\n",
    "        forecast_horizon=forecast_horizon,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fedd6b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "[2025-04-15 20:25:15,246: INFO: base] Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/7796273458/locations/northamerica-northeast1/pipelineJobs/b2b-wf-short-term-prediction-experiments-20250415202512\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/7796273458/locations/northamerica-northeast1/pipelineJobs/b2b-wf-short-term-prediction-experiments-20250415202512')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/northamerica-northeast1/pipelines/runs/b2b-wf-short-term-prediction-experiments-20250415202512?project=7796273458\n",
      "PipelineJob projects/7796273458/locations/northamerica-northeast1/pipelineJobs/b2b-wf-short-term-prediction-experiments-20250415202512 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Job failed with:\ncode: 9\nmessage: \" The DAG failed because some tasks failed. The failed tasks are: [data-ingestion-op].; Job (project_id = wb-ai-acltr-tbs-3-pr-a62583, job_id = 2752902238044160000) is failed due to the above error.; Failed to handle the job: {project_number = 7796273458, job_id = 2752902238044160000}\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 29\u001b[0m\n\u001b[1;32m      7\u001b[0m compiler\u001b[38;5;241m.\u001b[39mCompiler()\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m      8\u001b[0m     pipeline_func\u001b[38;5;241m=\u001b[39mforecast_pipeline,\n\u001b[1;32m      9\u001b[0m     package_path\u001b[38;5;241m=\u001b[39mCONFIG\u001b[38;5;241m.\u001b[39mgeneral_setup\u001b[38;5;241m.\u001b[39mpipeline_package_path\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     13\u001b[0m job \u001b[38;5;241m=\u001b[39m pipeline_jobs\u001b[38;5;241m.\u001b[39mPipelineJob(\n\u001b[1;32m     14\u001b[0m     display_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb2b_wf_short_term_prediction_sma_pipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m     template_path\u001b[38;5;241m=\u001b[39mCONFIG\u001b[38;5;241m.\u001b[39mgeneral_setup\u001b[38;5;241m.\u001b[39mpipeline_package_path,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     }\n\u001b[1;32m     27\u001b[0m )\n\u001b[0;32m---> 29\u001b[0m \u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/python/3.10/lib/python3.10/site-packages/google/cloud/aiplatform/pipeline_jobs.py:334\u001b[0m, in \u001b[0;36mPipelineJob.run\u001b[0;34m(self, service_account, network, reserved_ip_ranges, sync, create_request_timeout, enable_preflight_validations)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run this configured PipelineJob and monitor the job until completion.\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \n\u001b[1;32m    312\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;124;03m        Optional. Whether to enable preflight validations for the PipelineJob.\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    332\u001b[0m network \u001b[38;5;241m=\u001b[39m network \u001b[38;5;129;01mor\u001b[39;00m initializer\u001b[38;5;241m.\u001b[39mglobal_config\u001b[38;5;241m.\u001b[39mnetwork\n\u001b[0;32m--> 334\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_account\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_account\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreserved_ip_ranges\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreserved_ip_ranges\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43msync\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_preflight_validations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_preflight_validations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/python/3.10/lib/python3.10/site-packages/google/cloud/aiplatform/base.py:863\u001b[0m, in \u001b[0;36moptional_sync.<locals>.optional_run_in_thread.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m    862\u001b[0m         VertexAiResourceNounWithFutureManager\u001b[38;5;241m.\u001b[39mwait(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;66;03m# callbacks to call within the Future (in same Thread)\u001b[39;00m\n\u001b[1;32m    866\u001b[0m internal_callbacks \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/python/3.10/lib/python3.10/site-packages/google/cloud/aiplatform/pipeline_jobs.py:382\u001b[0m, in \u001b[0;36mPipelineJob._run\u001b[0;34m(self, service_account, network, reserved_ip_ranges, sync, create_request_timeout, enable_preflight_validations)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Helper method to ensure network synchronization and to run\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;124;03mthe configured PipelineJob and monitor the job until completion.\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m        Optional. Whether to enable preflight validations for the PipelineJob.\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[1;32m    375\u001b[0m     service_account\u001b[38;5;241m=\u001b[39mservice_account,\n\u001b[1;32m    376\u001b[0m     network\u001b[38;5;241m=\u001b[39mnetwork,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    379\u001b[0m     enable_preflight_validations\u001b[38;5;241m=\u001b[39menable_preflight_validations,\n\u001b[1;32m    380\u001b[0m )\n\u001b[0;32m--> 382\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_block_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;66;03m# AutoSxS view model evaluations\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m details \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_details:\n",
      "File \u001b[0;32m/opt/python/3.10/lib/python3.10/site-packages/google/cloud/aiplatform/pipeline_jobs.py:793\u001b[0m, in \u001b[0;36mPipelineJob._block_until_complete\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;66;03m# Error is only populated when the job state is\u001b[39;00m\n\u001b[1;32m    791\u001b[0m \u001b[38;5;66;03m# JOB_STATE_FAILED or JOB_STATE_CANCELLED.\u001b[39;00m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;129;01min\u001b[39;00m _PIPELINE_ERROR_STATES:\n\u001b[0;32m--> 793\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJob failed with:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource\u001b[38;5;241m.\u001b[39merror)\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    795\u001b[0m     _LOGGER\u001b[38;5;241m.\u001b[39mlog_action_completed_against_resource(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Job failed with:\ncode: 9\nmessage: \" The DAG failed because some tasks failed. The failed tasks are: [data-ingestion-op].; Job (project_id = wb-ai-acltr-tbs-3-pr-a62583, job_id = 2752902238044160000) is failed due to the above error.; Failed to handle the job: {project_number = 7796273458, job_id = 2752902238044160000}\"\n"
     ]
    }
   ],
   "source": [
    "aiplatform.init(\n",
    "    project=CONFIG.general_setup.project_id,\n",
    "    location=CONFIG.general_setup.project_location,\n",
    "    staging_bucket=f\"gs://{CONFIG.general_setup.project_id}_{CONFIG.general_setup.gcs_bucket}\"\n",
    ")\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=forecast_pipeline,\n",
    "    package_path=CONFIG.general_setup.pipeline_package_path\n",
    ")\n",
    "\n",
    "\n",
    "job = pipeline_jobs.PipelineJob(\n",
    "    display_name=\"b2b_wf_short_term_prediction_sma_pipeline\",\n",
    "    template_path=CONFIG.general_setup.pipeline_package_path,\n",
    "    parameter_values={\n",
    "        'project_id':        CONFIG.general_setup.project_id,\n",
    "        'project_location':  CONFIG.general_setup.project_location,\n",
    "        'bq_dataset':        CONFIG.data_ingestion.bq_dataset,\n",
    "        'bq_source_table':   CONFIG.data_ingestion.bq_source_table,\n",
    "        'time_column':       CONFIG.general_setup.time_column,\n",
    "        'target_column':     CONFIG.general_setup.target_column,\n",
    "        'series_identifier': CONFIG.general_setup.series_identifier,\n",
    "        'attribute_columns': CONFIG.general_setup.attribute_columns,\n",
    "        'forecast_horizon':  CONFIG.general_setup.forecast_horizon\n",
    "    }\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd70954b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
