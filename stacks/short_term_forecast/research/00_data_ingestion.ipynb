{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d48ce21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/workspaces/b2b-wf-experiments/stacks/short_term_forecast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5c86eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataIngestionConfig:\n",
    "    root_dir: Path\n",
    "    local_file_name: str\n",
    "    project_id: str\n",
    "    project_location: str\n",
    "    bq_dataset: str\n",
    "    bq_source_table: str\n",
    "    time_column: str\n",
    "    target_column: str\n",
    "    series_identifier: str\n",
    "    attribute_columns: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6469b390",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ShortTermForecast.constants import *\n",
    "from src.ShortTermForecast.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003b0c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from src.ShortTermForecast.constants import *\n",
    "from src.ShortTermForecast.utils.common import read_yaml, create_directories\n",
    "from src.ShortTermForecast.entity.config_entity import DataIngestionConfig\n",
    "\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH\n",
    "    ):\n",
    "        \n",
    "        if config_filepath is not None:\n",
    "            self.config = read_yaml(config_filepath)\n",
    "            create_directories([self.config.artifacts_root])\n",
    "        \n",
    "        if params_filepath is not None:\n",
    "            self.params = read_yaml(params_filepath)\n",
    "\n",
    "\n",
    "    def get_data_ingestion_dvc_config(self) -> DataIngestionConfig:\n",
    "        \"\"\"\n",
    "        Returns a DataIngestionConfig configured for DVC/local runs.\n",
    "        \"\"\"\n",
    "\n",
    "        general_configs = self.config.general_setup\n",
    "        data_ingestion_config = self.config.data_ingestion\n",
    "\n",
    "        create_directories([data_ingestion_config.root_dir])\n",
    "\n",
    "        return DataIngestionConfig(\n",
    "            root_dir=Path(data_ingestion_config.root_dir),\n",
    "            local_file_name=data_ingestion_config.local_file_name,\n",
    "            project_id=general_configs.project_id,\n",
    "            project_location=general_configs.project_location,\n",
    "            bq_dataset=data_ingestion_config.bq_dataset,\n",
    "            bq_source_table=data_ingestion_config.bq_source_table,\n",
    "            time_column=general_configs.time_column,\n",
    "            target_column=general_configs.target_column,\n",
    "            series_identifier=general_configs.series_identifier,\n",
    "            attribute_columns=general_configs.attribute_columns\n",
    "        )\n",
    "    \n",
    "    def get_data_ingestion_kfp_config(\n",
    "            self,\n",
    "            project_id: str,\n",
    "            project_location: str,\n",
    "            bq_dataset: str,\n",
    "            bq_source_table: str,\n",
    "            time_column: str,\n",
    "            target_column: str,\n",
    "            series_identifier: str,\n",
    "            attribute_columns: List[str]\n",
    "    ) -> DataIngestionConfig:\n",
    "        \"\"\"\n",
    "        Returns a DataIngestionConfig configured for Kubeflow Pipeline runs.\n",
    "        \"\"\"\n",
    "\n",
    "        return DataIngestionConfig(\n",
    "            root_dir=None,\n",
    "            local_file_name=None,\n",
    "            project_id=project_id,\n",
    "            project_location=project_location,\n",
    "            bq_dataset=bq_dataset,\n",
    "            bq_source_table=bq_source_table,\n",
    "            time_column=time_column,\n",
    "            target_column=target_column,\n",
    "            series_identifier=series_identifier,\n",
    "            attribute_columns=attribute_columns\n",
    "        )        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a5b6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from google.cloud import bigquery\n",
    "\n",
    "from src.ShortTermForecast import logger\n",
    "\n",
    "class DataIngestion:\n",
    "    \"\"\"\n",
    "    A class for ingesting data from BigQuery, processing it, and saving it locally.\n",
    "\n",
    "    This class handles the creation of series identifiers, data loading from BigQuery,\n",
    "    and saving the processed data to a local CSV file.\n",
    "\n",
    "    Attributes:\n",
    "        config (DataIngestionConfig): Configuration object containing necessary parameters.\n",
    "        client (bigquery.Client): BigQuery client for data querying.\n",
    "        data (pandas.DataFrame): Dataframe to store the ingested data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: DataIngestionConfig):\n",
    "        \"\"\"\n",
    "        Initialize the DataIngestion class.\n",
    "\n",
    "        Args:\n",
    "            config (DataIngestionConfig): Configuration object containing necessary parameters.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.client = None\n",
    "        self.data = None\n",
    "        logger.info(\"DataIngestion instance initialized with provided configuration.\")\n",
    "\n",
    "    def _create_series_identifier(self) -> str:\n",
    "        \"\"\"\n",
    "        Create the series identifier query field based on the attribute columns.\n",
    "\n",
    "        This method generates a SQL CONCAT statement to combine multiple attribute columns\n",
    "        into a single series identifier.\n",
    "\n",
    "        Example:\n",
    "            attribute_columns = ['local', 'type']\n",
    "            series identifier string = '<local> <type>' (e.g., 'vancouver 1')\n",
    "\n",
    "        Returns:\n",
    "            str: The CONCAT query to create the series identifiers column in the BigQuery view query.\n",
    "        \"\"\"\n",
    "        coalesce_parts = [f\"COALESCE({column}, 'None')\" for column in self.config.attribute_columns]\n",
    "        separator = \"' '\"\n",
    "        series_identifier = f\"CONCAT({f', {separator}, '.join(coalesce_parts)}) AS {self.config.series_identifier}\"\n",
    "        logger.debug(f\"Created series identifier: {series_identifier}\")\n",
    "        return series_identifier\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"\n",
    "        Load data from BigQuery using the configured query.\n",
    "\n",
    "        This method initializes the BigQuery client and executes the data ingestion query,\n",
    "        storing the results in a pandas DataFrame.\n",
    "        \"\"\"\n",
    "        logger.info(\"Initializing BigQuery client and loading data.\")\n",
    "        self.client = bigquery.Client(\n",
    "            project=self.config.project_id,\n",
    "            location=self.config.project_location\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            self.data = self.client.query(self.data_ingestion_query).to_dataframe()\n",
    "            logger.info(f\"Data loaded successfully. Shape: {self.data.shape}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading data from BigQuery: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def save(self, save_path: str = None):\n",
    "        \"\"\"\n",
    "        Save the loaded data to a CSV file.\n",
    "\n",
    "        Args:\n",
    "            save_path (str, optional): The path where the CSV file will be saved.\n",
    "                If not provided, it will use the default path from the configuration.\n",
    "        \"\"\"\n",
    "        if save_path is None:\n",
    "            save_path = Path(self.config.root_dir, self.config.local_file_name)\n",
    "        \n",
    "        logger.info(f\"Saving data to {save_path}\")\n",
    "        try:\n",
    "            self.data.to_csv(save_path, index=False)\n",
    "            logger.info(\"Data saved successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving data to CSV: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    @property\n",
    "    def data_ingestion_query(self) -> str:\n",
    "        \"\"\"\n",
    "        Generate the BigQuery SQL query for data ingestion.\n",
    "\n",
    "        This property creates a SQL query that selects and aggregates data from the source table,\n",
    "        applies date filtering, and includes the series identifier.\n",
    "\n",
    "        Returns:\n",
    "            str: The complete SQL query string for data ingestion.\n",
    "        \"\"\"\n",
    "        query = f\"\"\"\n",
    "        WITH historical_table AS (\n",
    "            SELECT \n",
    "                {self.config.time_column},\n",
    "                {self.attribute_string},\n",
    "                SUM({self.config.target_column}) AS {self.config.target_column}\n",
    "            FROM \n",
    "                `{self.config.project_id}.{self.config.bq_dataset}.{self.config.bq_source_table}`\n",
    "            WHERE \n",
    "                {self.config.time_column} <= DATE('2025-03-31')\n",
    "            GROUP BY \n",
    "                {self.config.time_column},\n",
    "                {self.attribute_string}\n",
    "        )\n",
    "        SELECT \n",
    "            {self._create_series_identifier()},\n",
    "            {self.config.time_column},\n",
    "            {self.attribute_string},\n",
    "            {self.config.target_column}\n",
    "        FROM historical_table\n",
    "        \"\"\"\n",
    "        logger.debug(\"Generated data ingestion query.\")\n",
    "        return query\n",
    "\n",
    "    @property\n",
    "    def attribute_string(self) -> str:\n",
    "        \"\"\"\n",
    "        Generate a comma-separated string of attribute columns.\n",
    "\n",
    "        Returns:\n",
    "            str: A comma-separated string of attribute column names.\n",
    "        \"\"\"\n",
    "        return ','.join(self.config.attribute_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffb9af47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-15 17:53:16,535: INFO: 1731669044] >>>>>> stage Data Ingestion started <<<<<<\n",
      "[2025-04-15 17:53:16,541: INFO: common] yaml file: config/config.yaml loaded successfully\n",
      "[2025-04-15 17:53:16,550: INFO: common] Creating directory: artifacts\n",
      "[2025-04-15 17:53:16,552: INFO: common] yaml file: params.yaml loaded successfully\n",
      "[2025-04-15 17:53:16,554: INFO: common] Creating directory: artifacts/data_ingestion\n",
      "[2025-04-15 17:53:16,555: INFO: 3841732962] DataIngestion instance initialized with provided configuration.\n",
      "[2025-04-15 17:53:16,556: INFO: 3841732962] Initializing BigQuery client and loading data.\n",
      "[2025-04-15 17:53:16,565: WARNING: _default] No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n",
      "[2025-04-15 17:54:34,478: INFO: 3841732962] Data loaded successfully. Shape: (299313, 11)\n",
      "[2025-04-15 17:54:34,479: INFO: 3841732962] Saving data to artifacts/data_ingestion/fwds_daily_data.csv\n",
      "[2025-04-15 17:54:37,538: INFO: 3841732962] Data saved successfully.\n",
      "[2025-04-15 17:54:37,551: INFO: 1731669044] >>>>>> stage Data Ingestion completed <<<<<<\n",
      "[2025-04-15 17:54:37,552: INFO: 1731669044] \n",
      "x==================================================x\n"
     ]
    }
   ],
   "source": [
    "STAGE_NAME = \"Data Ingestion\"\n",
    "\n",
    "\n",
    "class DataIngestionPipeline:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def main(self):\n",
    "        config = ConfigurationManager()\n",
    "        data_ingestion = DataIngestion(config.get_data_ingestion_dvc_config())\n",
    "        data_ingestion.load()\n",
    "        data_ingestion.save()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\n",
    "        obj = DataIngestionPipeline()\n",
    "        obj.main()\n",
    "        logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\")\n",
    "        logger.info(\"\\nx\" + \"=\" * 50 + \"x\")\n",
    "    except Exception as e:\n",
    "        logger.exception(e)\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99417bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# query_and_preprocess\n",
    "@component(\n",
    "    base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/kfp-preprocess-slim:2.0.1\"\n",
    ")\n",
    "def data_ingestion_op(\n",
    "    project_id: str,\n",
    "    project_location: str,\n",
    "    bq_dataset: str,\n",
    "    bq_source_table: str,\n",
    "    time_column: str,\n",
    "    target_column: str,\n",
    "    series_identifier: str,\n",
    "    attribute_columns: List[str],\n",
    "    output_dataset: Output[Dataset]\n",
    "):\n",
    "    # import the custom models\n",
    "\n",
    "    config = ConfigurationManager(\n",
    "        config_filepath=None, \n",
    "        params_filepath=None\n",
    "    )\n",
    "\n",
    "    data_ingestion_config = config.get_data_ingestion_kfp_config(\n",
    "        project_id=project_id,\n",
    "        project_location=project_location,\n",
    "        bq_dataset=bq_dataset,\n",
    "        bq_source_table=bq_source_table,\n",
    "        time_column=time_column,\n",
    "        target_column=target_column,\n",
    "        series_identifier=series_identifier,\n",
    "        attribute_columns=attribute_columns\n",
    "    )\n",
    "\n",
    "\n",
    "    data_ingestion = DataIngestion(data_ingestion_config)\n",
    "\n",
    "    data_ingestion.load()\n",
    "    data_ingestion.save(output_dataset.path)\n",
    "\n",
    "custom_data_ingestion_job = create_custom_training_job_from_component(\n",
    "    data_ingestion_op,\n",
    "    display_name='data-ingestion-job',\n",
    "    machine_type='e2-standard-4'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736631b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
