{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "PROJECT_ID      = \"\"\n",
    "PROJECT_REGION  = \"\"\n",
    "\n",
    "GCS_BUCKET_NAME = \"\"\n",
    "\n",
    "VERTEX_DATASET_NAME    = \"\"\n",
    "VERTEX_MODEL_NAME      = \"\"\n",
    "VERTEX_PREDICTION_NAME = \"\"\n",
    "\n",
    "BQ_DATASET_NAME  = \"\"\n",
    "BQ_TRAIN_TABLE   = \"\"\n",
    "BQ_PREDICT_TABLE = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"wb-ai-acltr-tbs-3-pr-a62583\"\n",
    "GCS_BUCKET_NAME = \"bkt_b2b_wf_prediction\"\n",
    "PROJECT_REGION = \"northamerica-northeast1\"\n",
    "\n",
    "VERTEX_DATASET_NAME = \"b2b_wf_prediction_panorama\"\n",
    "VERTEX_MODEL_NAME = \"b2b_wf_prediction_panorama\"\n",
    "VERTEX_PREDICTION_NAME = \"b2b_wf_prediction_batch\"\n",
    "\n",
    "BQ_DATASET_NAME = \"b2b_wf_prediction\"\n",
    "BQ_TRAIN_TABLE = \"vw_wf_experiment_historical\"\n",
    "BQ_PREDICT_TABLE = \"bq_wf_temp_predictions\"\n",
    "BQ_FORECAST_TABLE= \"bq_wf_forecast\"\n",
    "\n",
    "TRAIN_TEST_DATA_SPLIT = \"DATE('2024-07-01')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aiplatform\n",
    "from google.cloud import bigquery\n",
    "import datetime\n",
    "\n",
    "TRAINING_DATASET_BQ_PATH   = f\"bq://{PROJECT_ID}.{BQ_DATASET_NAME}.{BQ_TRAIN_TABLE}\"\n",
    "PREDICTION_DATASET_BQ_PATH = f\"bq://{PROJECT_ID}.{BQ_DATASET_NAME}.{BQ_PREDICT_TABLE}\"\n",
    "PREDICTION_OUTPUT_PREFIX   = f\"bq://{PROJECT_ID}.{BQ_DATASET_NAME}\"\n",
    "BUCKET_URI = f\"gs://{PROJECT_ID}_{GCS_BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(\n",
    "    project=PROJECT_ID, \n",
    "    staging_bucket=BUCKET_URI,\n",
    "    location=PROJECT_REGION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client(\n",
    "    project=PROJECT_ID, \n",
    "    location=PROJECT_REGION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Experiment:\n",
    "    name: str\n",
    "    model: str\n",
    "    experiment_columns: list[str]\n",
    "    objective: str\n",
    "    forecast_horizon: int\n",
    "    context_window: int\n",
    "    data_granularity_unit: str\n",
    "    holiday_regions: list[str]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_forecast_experiment = Experiment(\n",
    "    name=\"panorama_daily_forecast\",\n",
    "    model=\"AutoML\",\n",
    "    experiment_columns=[\n",
    "        \"District\",\n",
    "        \"Region_Type\",\n",
    "        \"Product\",\n",
    "        \"Product_Grp\",\n",
    "        \"Technology\",\n",
    "        \"Work_Order_Action\",\n",
    "        \"Work_Order_Action_Grp\",\n",
    "        \"Work_Force\"],\n",
    "    objective=\"minimize-rmse\",\n",
    "    forecast_horizon=184,\n",
    "    context_window=368,\n",
    "    data_granularity_unit='day',\n",
    "    holiday_regions=[\"CA\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_experiment = daily_forecast_experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train data view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_series_identifier(columns):\n",
    "    coalesce_parts = [f\"COALESCE({column}, 'None')\" for column in columns]\n",
    "    separator = \"' '\"\n",
    "    return f\"CONCAT({f', {separator}, '.join(coalesce_parts)}) as Series_Identifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_column                   = \"Appointment_Day\"\n",
    "time_series_identifier_column = \"Series_Identifier\"\n",
    "target_column                 = \"SWT\"\n",
    "\n",
    "FORECAST_TIMESTAMP = datetime.datetime.now()\n",
    "ATTRIBUTE_COLUMNS = running_experiment.experiment_columns\n",
    "ATTRIBUTE_STRING = ','.join(ATTRIBUTE_COLUMNS)\n",
    "\n",
    "COLUMN_SPECS = {\n",
    "    time_column:             \"timestamp\",\n",
    "    target_column:           \"numeric\"\n",
    "}\n",
    "\n",
    "for category in ATTRIBUTE_COLUMNS:\n",
    "    COLUMN_SPECS[category] = \"categorical\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_data_cte = f\"\"\"\n",
    "WITH historical_table AS (\n",
    "  SELECT \n",
    "    {time_column},\n",
    "    {ATTRIBUTE_STRING},\n",
    "    SUM({target_column}) AS {target_column}\n",
    "  FROM `{BQ_DATASET_NAME}.vw_wf_historical`\n",
    "  WHERE Appointment_Day < {TRAIN_TEST_DATA_SPLIT}\n",
    "  GROUP BY {time_column},{ATTRIBUTE_STRING}\n",
    ")\"\"\"\n",
    "\n",
    "\n",
    "experiment_train_data_query = f\"\"\"\n",
    "CREATE OR REPLACE VIEW `{BQ_DATASET_NAME}.{BQ_TRAIN_TABLE}` AS \n",
    "{experiment_data_cte}\n",
    "SELECT \n",
    "  {create_series_identifier(ATTRIBUTE_COLUMNS)},\n",
    "  {time_column},\n",
    "  {ATTRIBUTE_STRING},\n",
    "  {target_column}\n",
    "FROM historical_table\n",
    "\"\"\"\n",
    "\n",
    "VERTEX_DATASET_NAME += f\"_{running_experiment.name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.query_and_wait(experiment_train_data_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list = aiplatform.TimeSeriesDataset.list(\n",
    "    filter=f\"display_name={VERTEX_DATASET_NAME}\"\n",
    ")\n",
    "\n",
    "if len(dataset_list) == 0:\n",
    "    print(\"... creating new dataset ... \")\n",
    "    dataset = aiplatform.TimeSeriesDataset.create(\n",
    "        display_name=VERTEX_DATASET_NAME,\n",
    "        bq_source=[TRAINING_DATASET_BQ_PATH],\n",
    "    )\n",
    "else:\n",
    "    print(\"... using existent dataset ... \")\n",
    "    dataset = dataset_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = aiplatform.Model.list(\n",
    "    filter=f\"display_name={VERTEX_MODEL_NAME}\"\n",
    ")\n",
    "\n",
    "if len(model_list) == 0:\n",
    "    print(\"... training a new model ... \")\n",
    "    parent_model = None\n",
    "else:\n",
    "    print(\"... using existent model ... \")\n",
    "    model = model_list[0]\n",
    "    print(model)\n",
    "    parent_model = model.resource_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job = aiplatform.AutoMLForecastingTrainingJob(\n",
    "    display_name=VERTEX_MODEL_NAME,\n",
    "    optimization_objective=running_experiment.objective,\n",
    "    column_specs=COLUMN_SPECS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = training_job.run(\n",
    "    dataset=dataset,\n",
    "    target_column=target_column,\n",
    "    time_column=time_column,\n",
    "    time_series_identifier_column=time_series_identifier_column,\n",
    "    available_at_forecast_columns=[time_column],\n",
    "    unavailable_at_forecast_columns=[target_column],\n",
    "    time_series_attribute_columns=ATTRIBUTE_COLUMNS,\n",
    "    forecast_horizon=running_experiment.forecast_horizon,\n",
    "    context_window=running_experiment.context_window,\n",
    "    data_granularity_unit=running_experiment.data_granularity_unit,\n",
    "    data_granularity_count=1,\n",
    "    weight_column=None,\n",
    "    budget_milli_node_hours=1000,\n",
    "    parent_model = parent_model,\n",
    "    model_display_name=VERTEX_MODEL_NAME,\n",
    "    is_default_version = True,\n",
    "    model_version_description = f\"{running_experiment.name} model generated on {datetime.date.today().isoformat()}\",\n",
    "    predefined_split_column_name=None,\n",
    "    holiday_regions=running_experiment.holiday_regions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range = f\"\"\"SELECT\n",
    "    (\n",
    "      DATE(DATE_TRUNC({TRAIN_TEST_DATA_SPLIT}, DAY)) + INTERVAL i DAY\n",
    "    ) AS {time_column}\n",
    "  FROM\n",
    "    UNNEST (GENERATE_ARRAY(0, {running_experiment.forecast_horizon-1})) AS i\"\"\"\n",
    "\n",
    "columns_dim = f\"\"\"SELECT DISTINCT\n",
    "    Series_Identifier,\n",
    "    {ATTRIBUTE_STRING}\n",
    "  FROM `{BQ_DATASET_NAME}.{BQ_TRAIN_TABLE}`\n",
    "  WHERE\n",
    "      {\" IS NOT NULL AND \".join(ATTRIBUTE_COLUMNS)} IS NOT NULL\n",
    "\"\"\"\n",
    "future_values = f\"\"\"SELECT\n",
    "    h.Series_Identifier,\n",
    "    CAST(d.{time_column} AS DATE) AS {time_column},\n",
    "    {','.join(map(lambda x : f'h.{x}', ATTRIBUTE_COLUMNS))},\n",
    "    NULL AS {target_column},\n",
    "    'predicted' AS {target_column}_Type\n",
    "  FROM columns_dim h,\n",
    "    date_range d\n",
    "  \"\"\"\n",
    "\n",
    "past_values = f\"\"\"SELECT\n",
    "    Series_Identifier,\n",
    "    {time_column},\n",
    "    {ATTRIBUTE_STRING},\n",
    "    {target_column},\n",
    "    'actual' AS {target_column}_Type\n",
    "  FROM `{BQ_DATASET_NAME}.{BQ_TRAIN_TABLE}`\n",
    "  WHERE\n",
    "    {\" IS NOT NULL AND \".join(ATTRIBUTE_COLUMNS)} IS NOT NULL\"\"\"\n",
    "\n",
    "\n",
    "predicton_table_query = f\"\"\"WITH date_range AS (\n",
    "  {date_range}\n",
    "), columns_dim AS (\n",
    "  {columns_dim}\n",
    "),future_values AS (\n",
    "  {future_values}\n",
    "), past_values AS (\n",
    "  {past_values}\n",
    ")\n",
    "SELECT\n",
    "  Series_Identifier,\n",
    "  {time_column},\n",
    "  {ATTRIBUTE_STRING},\n",
    "  {target_column},\n",
    "  {target_column}_Type\n",
    "FROM future_values\n",
    "UNION ALL\n",
    "SELECT\n",
    "  Series_Identifier,\n",
    "  {time_column},\n",
    "  {ATTRIBUTE_STRING},\n",
    "  {target_column},\n",
    "  {target_column}_Type\n",
    "FROM past_values\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.query_and_wait(f\"\"\"CREATE OR REPLACE TABLE `{BQ_DATASET_NAME}.{BQ_PREDICT_TABLE}` AS {predicton_table_query}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_prediction_job = model.batch_predict(\n",
    "    job_display_name=VERTEX_PREDICTION_NAME,\n",
    "    bigquery_source=PREDICTION_DATASET_BQ_PATH,\n",
    "    instances_format=\"bigquery\",\n",
    "    bigquery_destination_prefix=PREDICTION_OUTPUT_PREFIX,\n",
    "    predictions_format=\"bigquery\",\n",
    "    generate_explanation=True,\n",
    "    sync=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_table  = batch_prediction_job.output_info.bigquery_output_table\n",
    "batch_table = 'predictions_2025_02_11T02_47_34_829Z_934'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_data = f\"\"\"\n",
    "SELECT\n",
    "  CAST('{FORECAST_TIMESTAMP}' AS TIMESTAMP) AS Forecast_Date,\n",
    "  CAST(Appointment_Day AS DATE) AS Appointment_Day,\n",
    "  Series_Identifier,\n",
    "  {ATTRIBUTE_STRING},\n",
    "  predicted_SWT.value AS SWT\n",
    "FROM\n",
    "  `{BQ_DATASET_NAME}.{batch_table}`\n",
    "WHERE\n",
    "  SWT_Type = 'predicted'\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "perisist_predictions_query = f\"\"\"\n",
    "INSERT INTO `{BQ_DATASET_NAME}.{BQ_FORECAST_TABLE}`\n",
    "(\n",
    "  Model,\n",
    "  Forecast_Date,\n",
    "  Series_Identifier,\n",
    "  Appointment_Day,\n",
    "  {ATTRIBUTE_STRING},\n",
    "  SWT\n",
    ")\n",
    "WITH prediction_data AS (\n",
    "  {prediction_data}\n",
    ")\n",
    "SELECT DISTINCT\n",
    "  '{running_experiment.model}' AS Model,\n",
    "  Forecast_Date,\n",
    "  Series_Identifier,\n",
    "  Appointment_Day,\n",
    "  {ATTRIBUTE_STRING},\n",
    "  SWT\n",
    "FROM prediction_data\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.query_and_wait(perisist_predictions_query)\n",
    "client.query_and_wait(f\"DROP TABLE `{BQ_DATASET_NAME}.{batch_table}`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class DataEvaluationPreprocessor:\n",
    "\n",
    "    REQUIRED_COLUMNS = ['Appointment_Day', 'SWT', 'Product_Grp', 'Work_Order_Action_Grp', 'District', 'Region_Type']\n",
    "    DATA_RANGE = pd.to_datetime([\n",
    "            \"2024-07-01\", \"2024-08-01\", \"2024-09-01\",\n",
    "            \"2024-10-01\", \"2024-11-01\", \"2024-12-01\"\n",
    "        ])\n",
    "\n",
    "    \n",
    "    def __init__(self, data: pd.DataFrame) -> None:\n",
    "\n",
    "        self.data = data.copy()\n",
    "\n",
    "        self.__check_columns()\n",
    "        self.__sort_columns()\n",
    "        self.__ensure_all_months()\n",
    "        self.__remove_lower_priority_tiers()\n",
    "\n",
    "\n",
    "        self.data['Appointment_Day'] = pd.to_datetime(self.data['Appointment_Day'])\n",
    "\n",
    "    def __sort_columns(self):\n",
    "        self.data = self.data[self.data.columns.sort_values()]\n",
    "        \n",
    "    def get_filtered_data(self, filters: dict):\n",
    "        \"\"\"\n",
    "        Apply a set of filters to a dataframe.\n",
    "        Expected format for filters is a dict, e.g. {'Region_Type': 'Tier 1'}.\n",
    "        \"\"\"\n",
    "        filtered_data = self.data.copy()\n",
    "        for col, val in filters.items():\n",
    "            filtered_data = filtered_data[filtered_data[col] == val]\n",
    "        return filtered_data\n",
    "    \n",
    "    def get_grouped_data(self, group_by: list[str] = None, filters: dict = None):\n",
    "\n",
    "        if group_by is None:\n",
    "            group_by = self.data.columns.drop(['SWT', 'Appointment_Day'])\n",
    "\n",
    "        grouped_data = self.data.copy()\n",
    "        \n",
    "        if filters is not None:\n",
    "            grouped_data = self.get_filtered_data(filters)\n",
    "            \n",
    "        grouped_data['series_id'] = grouped_data[group_by].astype(str).agg(' '.join, axis=1)\n",
    "        grouped_data = grouped_data[['Appointment_Day', 'series_id','SWT']]\n",
    "        grouped_data.set_index('Appointment_Day', inplace=True)\n",
    "        grouped_data = grouped_data.sort_values(by=['Appointment_Day', 'series_id'])\n",
    "        grouped_data = grouped_data.groupby(['Appointment_Day', 'series_id'], as_index=True)['SWT'].sum()\n",
    "        return grouped_data\n",
    "\n",
    "    def __remove_lower_priority_tiers(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Among rows that match exactly on all columns except 'Region_Type',\n",
    "        keep only the row with the smallest numeric tier (e.g. Tier 1 < Tier 2 < Tier 3).\n",
    "        \"\"\"\n",
    "        tier_map = {\"Tier 1\": 1, \"Tier 2\": 2, \"Tier 3\": 3, \"Tier 4\": 4}\n",
    "        df = self.data.copy()\n",
    "        df[\"tier_rank\"] = df[\"Region_Type\"].map(tier_map)\n",
    "        \n",
    "        # 2) Sort by all columns that define a duplicate plus the numeric tier rank.\n",
    "        #    For duplicates, the smallest tier_rank will come first in sort order.\n",
    "        sort_cols = [c for c in df.columns if c not in [\"Region_Type\", \"tier_rank\"]]\n",
    "        df.sort_values(by=sort_cols + [\"tier_rank\"], inplace=True, ignore_index=True)\n",
    "        \n",
    "        # 3) Drop duplicates on every column **except** \"Region_Type\" (and our temporary \"tier_rank\")\n",
    "        #    This means we only keep the first row of each group (the smallest tier).\n",
    "        dedup_cols = [c for c in df.columns if c not in [\"Region_Type\", \"tier_rank\"]]\n",
    "        df.drop_duplicates(subset=dedup_cols, keep=\"first\", inplace=True, ignore_index=True)\n",
    "        \n",
    "        # 4) Drop the helper column\n",
    "        df.drop(columns=[\"tier_rank\"], inplace=True)\n",
    "        \n",
    "        self.data = df.copy()\n",
    "        return df       \n",
    "\n",
    "    def __check_columns(self):\n",
    "        \"\"\"\n",
    "            Check if the df has this columns:\n",
    "            - Appointment_Day [date]\n",
    "            - SWT [float]\n",
    "            - Product_Grp [str]\n",
    "            - Work_Order_Action_Grp [str]\n",
    "            - District [str]\n",
    "            - Region_Type [str]\n",
    "        \"\"\"\n",
    "        \n",
    "        missing_columns = set(self.REQUIRED_COLUMNS) - set(self.data.columns)\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"Missing required columns: {missing_columns}\")  \n",
    "\n",
    "        # cast the data for the right types\n",
    "        self.data['Appointment_Day'] = pd.to_datetime(self.data['Appointment_Day'])\n",
    "        self.data['SWT'] = self.data['SWT'].astype(float)\n",
    "        self.data['Product_Grp'] = self.data['Product_Grp'].astype(str)\n",
    "        self.data['Work_Order_Action_Grp'] = self.data['Work_Order_Action_Grp'].astype(str)\n",
    "        self.data['District'] = self.data['District'].astype(str)\n",
    "        self.data['Region_Type'] = self.data['Region_Type'].astype(str)\n",
    "            \n",
    "    def __has_all_months(self):\n",
    "        \"\"\"\n",
    "        Check if the df has all months from 2024-07-01 to 2024-12-01 for each series_id\n",
    "        \"\"\"\n",
    "        data = self.data.copy()\n",
    "        data['series_id'] = data[data.columns.drop(['SWT', 'Appointment_Day'])].astype(str).agg(' '.join, axis=1)\n",
    "        series_ids = data['series_id'].unique()\n",
    "        for series_id in series_ids:\n",
    "            series_data = data[data['series_id'] == series_id]\n",
    "            if len(series_data) != len(self.DATA_RANGE):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def __ensure_all_months(self):\n",
    "        \"\"\"\n",
    "        For every unique combination of columns (other than Appointment_Day and SWT),\n",
    "        this function adds rows for all months between 2024-07-01 and 2024-12-01\n",
    "        (inclusive) if they're missing. The SWT column is filled with 0 where data \n",
    "        does not exist in the original dataframe.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.__has_all_months():\n",
    "            return self.data\n",
    "        \n",
    "        df = self.data.copy()\n",
    "        df['Appointment_Day'] = pd.to_datetime(df['Appointment_Day'])\n",
    "\n",
    "        id_cols = [col for col in df.columns if col not in ['Appointment_Day', 'SWT']]\n",
    "        unique_ids = df[id_cols].drop_duplicates()\n",
    "        months_df = pd.DataFrame({'Appointment_Day': self.DATA_RANGE})\n",
    "\n",
    "        unique_ids['merge_key'] = 1\n",
    "        months_df['merge_key'] = 1\n",
    "        cross_joined = pd.merge(unique_ids, months_df, on='merge_key').drop(columns='merge_key')\n",
    "\n",
    "        merged = pd.merge(\n",
    "            cross_joined,\n",
    "            df,\n",
    "            on=id_cols + ['Appointment_Day'],\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        merged['SWT'] = merged['SWT'].fillna(0)\n",
    "\n",
    "        merged = merged.sort_values(by=['Appointment_Day'] + id_cols).reset_index(drop=True)\n",
    "\n",
    "        merged = merged[['Appointment_Day'] + sorted(set(merged.columns) - {'Appointment_Day', 'SWT'}) + ['SWT']]\n",
    "\n",
    "        self.data = merged.copy()\n",
    "        return merged\n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Evaluation:\n",
    "    \n",
    "    def __init__(self, historical: DataEvaluationPreprocessor, predicted: DataEvaluationPreprocessor) -> None:\n",
    "        \"\"\"\n",
    "        Initialize with two DataEvaluationPreprocessor objects: one for historical (actual) data\n",
    "        and one for predicted (forecast) data.\n",
    "        \"\"\"\n",
    "        self.historical = historical\n",
    "        self.predicted = predicted\n",
    "\n",
    "    def _get_data(\n",
    "        self, \n",
    "        group_by: list[str] = None, \n",
    "        filters: dict = None\n",
    "    ) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Retrieve historical and predicted data (as DataFrames), optionally filtered and/or grouped.\n",
    "        \n",
    "        - If filters is provided (as a dict of {column: value}), the data are filtered.\n",
    "        - If group is True, data are grouped (default grouping is by ['series_id', 'Appointment_Month'] \n",
    "          if no group_by is provided). In grouping, the SWT values are summed.\n",
    "        \"\"\"\n",
    "        # Reset index to get Appointment_Month as a column.\n",
    "        hist = self.historical.get_grouped_data(group_by, filters)\n",
    "        pred = self.predicted.get_grouped_data(group_by, filters)\n",
    "\n",
    "        return hist, pred\n",
    "\n",
    "    def calculate_metric(\n",
    "        self, \n",
    "        metric: str, \n",
    "        filters: dict = None, \n",
    "        group_by: list[str] = None, \n",
    "        epsilon: float = 1\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Calculate a metric between the historical and predicted data.\n",
    "        \n",
    "        Parameters:\n",
    "           metric:    str - Supported metrics: 'rmse', 'mape', or 'wape'\n",
    "           group:     bool - Whether to group the raw data before merging.\n",
    "                            If True, the data is grouped (default group_by = ['series_id', 'Appointment_Day']).\n",
    "           filters:   dict - Optional filtering criteria (e.g. {'Region_Type': 'Tier 1'}).\n",
    "           group_by:  list[str] - The columns to group by if group is True.\n",
    "           epsilon:   float - A small value to avoid division by zero in percentage calculations.\n",
    "        \n",
    "        Returns:\n",
    "           The computed metric as a float.\n",
    "        \"\"\"\n",
    "        hist, pred = self._get_data(group_by, filters)\n",
    "        \n",
    "        merged = pd.merge(\n",
    "            hist.reset_index(), pred.reset_index(), \n",
    "            on=['Appointment_Day', 'series_id'], \n",
    "            suffixes=('_hist', '_pred'), \n",
    "            how='inner'\n",
    "        )\n",
    "        \n",
    "        error = merged['SWT_pred'] - merged['SWT_hist']\n",
    "        \n",
    "        if metric.lower() == 'rmse':\n",
    "            return np.sqrt(np.mean(error ** 2))\n",
    "        elif metric.lower() == 'mape':\n",
    "            return np.mean(np.abs(error) / (merged['SWT_hist'] + epsilon)) * 100\n",
    "        elif metric.lower() == 'wape':\n",
    "            return np.sum(np.abs(error)) / (np.sum(merged['SWT_hist']) + epsilon)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported metric: {metric}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_query = f\"\"\"\n",
    "SELECT\n",
    "  DATE_TRUNC(Appointment_Day, MONTH) AS Appointment_Day,\n",
    "  Product_Grp,\n",
    "  Work_Order_Action_Grp,\n",
    "  District,\n",
    "  Region_Type,\n",
    "  SUM({target_column}) as SWT\n",
    "FROM `{BQ_DATASET_NAME}.{BQ_FORECAST_TABLE}`\n",
    "WHERE \n",
    "  Model = '{running_experiment.model}'\n",
    "  AND Forecast_Date = CAST('{FORECAST_TIMESTAMP}' AS TIMESTAMP)\n",
    "GROUP BY\n",
    "  DATE_TRUNC({time_column}, MONTH),\n",
    "  Product_Grp,\n",
    "  Work_Order_Action_Grp,\n",
    "  District,\n",
    "  Region_Type\n",
    "ORDER BY\n",
    "  {time_column},\n",
    "  Product_Grp,\n",
    "  Work_Order_Action_Grp,\n",
    "  District,\n",
    "  Region_Type\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_df = client.query_and_wait(forecast_query).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_query = f\"\"\"\n",
    "WITH historical_table AS (\n",
    "  SELECT \n",
    "    DATE_TRUNC({time_column}, MONTH) AS Appointment_Day,\n",
    "    Product_Grp,\n",
    "    Work_Order_Action_Grp,\n",
    "    District,\n",
    "    Region_Type,\n",
    "    SUM({target_column}) AS SWT\n",
    "  FROM `{BQ_DATASET_NAME}.vw_wf_historical`\n",
    "  WHERE {time_column} >= {TRAIN_TEST_DATA_SPLIT}\n",
    "  GROUP BY \n",
    "    DATE_TRUNC({time_column}, MONTH),\n",
    "    Product_Grp,\n",
    "    Work_Order_Action_Grp,\n",
    "    District,\n",
    "    Region_Type\n",
    ")\n",
    "SELECT\n",
    "  Appointment_Day,\n",
    "  Product_Grp,\n",
    "  Work_Order_Action_Grp,\n",
    "  District,\n",
    "  Region_Type,\n",
    "  SWT\n",
    "FROM historical_table\n",
    "ORDER BY\n",
    "  Appointment_Day,\n",
    "  Product_Grp,\n",
    "  Work_Order_Action_Grp,\n",
    "  District,\n",
    "  Region_Type\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_df = client.query_and_wait(historical_query).to_dataframe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_data = DataEvaluationPreprocessor(historical_df)\n",
    "forecast_data = DataEvaluationPreprocessor(forecast_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = Evaluation(historical_data, forecast_data)\n",
    "\n",
    "rmse = {\n",
    "    'overall': evaluation.calculate_metric('rmse'),\n",
    "    'Tier 1': evaluation.calculate_metric('rmse', filters={'Region_Type': 'Tier 1'}),\n",
    "    'Tier 2': evaluation.calculate_metric('rmse', filters={'Region_Type': 'Tier 2'}),\n",
    "    'Tier 3': evaluation.calculate_metric('rmse', filters={'Region_Type': 'Tier 3'})\n",
    "}\n",
    "\n",
    "wape = {\n",
    "    'overall': evaluation.calculate_metric('wape'),\n",
    "    'Tier 1': evaluation.calculate_metric('wape', filters={'Region_Type': 'Tier 1'}),\n",
    "    'Tier 2': evaluation.calculate_metric('wape', filters={'Region_Type': 'Tier 2'}),\n",
    "    'Tier 3': evaluation.calculate_metric('wape', filters={'Region_Type': 'Tier 3'})\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'overall': 97690.4974779541, 'Tier 1': 152945.9169026087, 'Tier 2': 72308.02973904515, 'Tier 3': 17695.67033368956} {'overall': 0.7180642741536833, 'Tier 1': 0.7201755315924308, 'Tier 2': 0.7182299956441255, 'Tier 3': 0.6980604280223551}\n"
     ]
    }
   ],
   "source": [
    "print(rmse, wape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_insert_query = f\"\"\"\n",
    "INSERT INTO `{BQ_DATASET_NAME}.bq_wf_evaluation`\n",
    "  (Model, Forecast_Date, WAPE, RMSE)\n",
    "VALUES (\n",
    "  '{running_experiment.model}',               \n",
    "  '{FORECAST_TIMESTAMP}',\n",
    "  STRUCT(\n",
    "    {wape['overall']} AS Overall,\n",
    "    {wape['Tier 1']} AS Tier_1,\n",
    "    {wape['Tier 2']} AS Tier_2,\n",
    "    {wape['Tier 3']} AS Tier_3\n",
    "  ),\n",
    "  STRUCT(\n",
    "    {rmse['overall']} AS Overall,\n",
    "    {rmse['Tier 1']} AS Tier_1,\n",
    "    {rmse['Tier 2']} AS Tier_2,\n",
    "    {rmse['Tier 3']} AS Tier_3\n",
    "  )\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.table.RowIterator at 0x7fff64eeefb0>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.query_and_wait(evaluation_insert_query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
