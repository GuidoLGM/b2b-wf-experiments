{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "cb87466c",
            "metadata": {},
            "outputs": [],
            "source": [
                "from kfp import dsl\n",
                "import kfp.compiler as compiler\n",
                "from kfp.dsl import component, Input, Output, Artifact, Model, Dataset\n",
                "    \n",
                "from google.cloud import aiplatform\n",
                "from google.cloud.aiplatform import pipeline_jobs\n",
                "from google_cloud_pipeline_components.v1.custom_job import create_custom_training_job_from_component\n",
                "\n",
                "\n",
                "from typing import Any, Dict, List"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "8119b735",
            "metadata": {
                "tags": [
                    "parameters"
                ]
            },
            "outputs": [],
            "source": [
                "PROJECT_ID = ''\n",
                "GCS_BUCKET_NAME = ''\n",
                "PROJECT_REGION = ''\n",
                "\n",
                "VERTEX_DATASET_NAME = ''\n",
                "VERTEX_MODEL_NAME = ''\n",
                "VERTEX_PREDICTION_NAME = ''\n",
                "\n",
                "BQ_DATASET_NAME = ''\n",
                "BQ_SOURCE_TABLE = ''\n",
                "BQ_TRAIN_TABLE = ''\n",
                "BQ_PREDICT_TABLE = ''\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "3ecde90e",
            "metadata": {},
            "outputs": [],
            "source": [
                "PROJECT_ID = 'wb-ai-acltr-tbs-3-pr-a62583'\n",
                "GCS_BUCKET_NAME = 'bkt_b2b_wf_prediction'\n",
                "PROJECT_REGION = 'northamerica-northeast1'\n",
                "\n",
                "VERTEX_DATASET_NAME = 'b2b_wf_short_term_prediction'\n",
                "VERTEX_MODEL_NAME = 'b2b_wf_short_term_model'\n",
                "\n",
                "BQ_DATASET_NAME = 'b2b_wf_prediction'\n",
                "BQ_SOURCE_TABLE = 'vw_wf_daily_historical'\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "892e04c3",
            "metadata": {},
            "outputs": [],
            "source": [
                "TRAINING_DATASET_BQ_PATH   = f\"bq://{PROJECT_ID}.{BQ_DATASET_NAME}.{BQ_TRAIN_TABLE}\"\n",
                "BUCKET_URI = f\"gs://{PROJECT_ID}_{GCS_BUCKET_NAME}\"\n",
                "PIPELINE_PACKAGE_PATH = 'short_term_pipeline.json'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "35912b62",
            "metadata": {},
            "outputs": [],
            "source": [
                "EXPERIMENT_NAME = \"b2b-wf-daily-forecast-model\"\n",
                "\n",
                "RUN_NAME = 'sma-training'\n",
                "MODEL = 'SMA' # 'SARIMA\n",
                "FORECAST_HORIZON = 91\n",
                "EXPERIMENT_FEATURES = [\n",
                "    \"District\",\n",
                "    \"Region_Type\",\n",
                "    \"Product\",\n",
                "    \"Product_Grp\",\n",
                "    \"Technology\",\n",
                "    \"Work_Order_Action\",\n",
                "    \"Work_Order_Action_Grp\",\n",
                "    \"Work_Force\"\n",
                "]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fd799159",
            "metadata": {},
            "source": [
                "# Preprocess"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "d020b73c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# query_and_preprocess\n",
                "@component(\n",
                "    base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/b2b_ai/wf_pipeline/training:1.0.2-rc\"\n",
                ")\n",
                "def query_and_preprocess(\n",
                "    project_id: str,\n",
                "    project_location: str,\n",
                "    bq_dataset: str,\n",
                "    bq_source_table: str,\n",
                "    time_column: str,\n",
                "    target_column: str,\n",
                "    series_identifier: str,\n",
                "    attribute_columns: List[str],\n",
                "    output_dataset: Output[Dataset]\n",
                "):\n",
                "    \"\"\" \n",
                "    Queries BigQuery data, performs preprocessing, and exports to a CSV dataset for training.\n",
                "    The function aggregates data by time and attributes, creates a unique series identifier,\n",
                "    and handles categorical features appropriately.\n",
                "\n",
                "    Args:\n",
                "        project_id: GCP project ID\n",
                "        project_location: GCP project location/region\n",
                "        bq_dataset: BigQuery dataset name\n",
                "        bq_source_table: Source table name\n",
                "        attribute_columns: List of categorical columns to group by\n",
                "        output_dataset: Output path for the preprocessed CSV dataset\n",
                "\n",
                "    Returns:\n",
                "        Writes a preprocessed CSV file to the output_dataset path containing:\n",
                "        - Series_Identifier: Concatenated string of attribute values\n",
                "        - Appointment_Day: Timestamp column\n",
                "        - Attribute columns: Original categorical features\n",
                "        - SWT: Aggregated target variable\n",
                "    \"\"\"\n",
                "    \n",
                "    from google.cloud import bigquery\n",
                "    \n",
                "\n",
                "    def create_series_identifier(columns, series_identifier):\n",
                "        coalesce_parts = [f\"COALESCE({column}, 'None')\" for column in columns]\n",
                "        separator = \"' '\"\n",
                "        return f\"CONCAT({f', {separator}, '.join(coalesce_parts)}) AS {series_identifier}\"\n",
                "    \n",
                "    ATTRIBUTE_STRING = ','.join(attribute_columns)\n",
                "        \n",
                "    experiment_train_data_query = f\"\"\"\n",
                "    WITH historical_table AS (\n",
                "        SELECT \n",
                "            {time_column},\n",
                "            {ATTRIBUTE_STRING},\n",
                "            SUM({target_column}) AS {target_column}\n",
                "        FROM `{project_id}.{bq_dataset}.{bq_source_table}`\n",
                "        WHERE {time_column} <= DATE('2025-03-31')\n",
                "        GROUP BY {time_column},{ATTRIBUTE_STRING}\n",
                "    )\n",
                "    SELECT \n",
                "        {create_series_identifier(attribute_columns, series_identifier)},\n",
                "        {time_column},\n",
                "        {ATTRIBUTE_STRING},\n",
                "        {target_column}\n",
                "    FROM historical_table\n",
                "    \"\"\"\n",
                "\n",
                "    client = bigquery.Client(\n",
                "        project=project_id,\n",
                "        location=project_location)\n",
                "\n",
                "    processed_data = client.query(experiment_train_data_query).to_dataframe()\n",
                "    \n",
                "    processed_data.to_csv(output_dataset.path, index=False)\n",
                "    print(f\"CSV file written to {output_dataset.path}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "62116c53",
            "metadata": {},
            "outputs": [],
            "source": [
                "# generate_time_series_cv\n",
                "@component(\n",
                "    base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/b2b_ai/wf_pipeline/training:1.0.2-rc\"\n",
                ")\n",
                "def generate_time_series_cv(\n",
                "    input_dataset: Input[Dataset],\n",
                "    time_column: str,\n",
                "    forecast_horizon: int,\n",
                "    output_train: Output[Dataset],\n",
                "    output_test: Output[Dataset]\n",
                "):\n",
                "    import pandas as pd\n",
                "    import logging\n",
                "    from datetime import timedelta\n",
                "    \"\"\"\n",
                "    Splits preprocessed time series data into rolling forecast datasets for training and evaluation.\n",
                "    \n",
                "    This component implements a rolling window cross-validation strategy for time series forecasting.\n",
                "    It reads a CSV dataset containing timestamp-based data and creates training and test sets with\n",
                "    an additional 'split_index' column to identify different temporal splits. Each split represents\n",
                "    a different forecasting period, with training data incrementally growing and test data moving\n",
                "    forward in time.\n",
                "    \n",
                "    Split Structure:\n",
                "        - All splits start training from 2022-01-01\n",
                "        - Training periods grow progressively longer\n",
                "        - Each test period is forecast_horizon days following its training period\n",
                "        - Data is labeled with split_index (1-4) to identify which split it belongs to\n",
                "    \n",
                "    Args:\n",
                "        input_dataset: Input[Dataset]\n",
                "            The preprocessed CSV dataset containing time series data\n",
                "        time_column: str\n",
                "            Name of the column containing timestamps\n",
                "        forecast_horizon: int\n",
                "            Number of days for each test period\n",
                "        output_train: Output[Dataset]\n",
                "            Output path for the combined training dataset\n",
                "            Contains all training data with a 'split_index' column\n",
                "        output_test: Output[Dataset]\n",
                "            Output path for the combined test dataset\n",
                "            Contains all test data with a 'split_index' column\n",
                "    \n",
                "    Output Dataset Structure:\n",
                "        Both training and test datasets include all original columns plus:\n",
                "        - split_index: int (1-4)\n",
                "            Identifies which temporal split the row belongs to\n",
                "            Allows filtering/grouping data by split for analysis or modeling\n",
                "    \"\"\"\n",
                "    \n",
                "    logging.basicConfig(level=logging.INFO)\n",
                "    logger = logging.getLogger(__name__)\n",
                "    \n",
                "    logger.info(f\"Reading input dataset from: {input_dataset.path}\")\n",
                "    forecast_processed_data = pd.read_csv(input_dataset.path, parse_dates=[time_column])\n",
                "    \n",
                "    train_start = pd.to_datetime(\"2022-01-01\")\n",
                "    \n",
                "    splits = []\n",
                "    current_train_end = pd.to_datetime(\"2024-03-31\")\n",
                "    \n",
                "    for split_index in range(1, 5):\n",
                "        test_start = current_train_end + timedelta(days=1)\n",
                "        test_end = test_start + timedelta(days=forecast_horizon-1)\n",
                "        \n",
                "        splits.append({\n",
                "            \"split_index\": split_index,\n",
                "            \"train_start\": train_start,\n",
                "            \"train_end\": current_train_end,\n",
                "            \"test_start\": test_start,\n",
                "            \"test_end\": test_end\n",
                "        })\n",
                "        \n",
                "        current_train_end = test_end\n",
                "    \n",
                "    logger.info(\"Generated splits:\")\n",
                "    for s in splits:\n",
                "        logger.info(f\"Split {s['split_index']}:\")\n",
                "        logger.info(f\"  Train: {s['train_start'].strftime('%Y-%m-%d')} to {s['train_end'].strftime('%Y-%m-%d')}\")\n",
                "        logger.info(f\"  Test:  {s['test_start'].strftime('%Y-%m-%d')} to {s['test_end'].strftime('%Y-%m-%d')}\")\n",
                "    \n",
                "    all_train_data = []\n",
                "    all_test_data = []\n",
                "    \n",
                "    for s in splits:\n",
                "        logger.info(f\"\\nProcessing split {s['split_index']}\")\n",
                "        \n",
                "        train_mask = (forecast_processed_data[time_column] >= s[\"train_start\"]) & \\\n",
                "                     (forecast_processed_data[time_column] <= s[\"train_end\"])\n",
                "        \n",
                "        test_mask = (forecast_processed_data[time_column] >= s[\"test_start\"]) & \\\n",
                "                    (forecast_processed_data[time_column] <= s[\"test_end\"])\n",
                "\n",
                "        train_df = forecast_processed_data.loc[train_mask].copy()\n",
                "        test_df = forecast_processed_data.loc[test_mask].copy()\n",
                "        \n",
                "        train_df['split_index'] = s['split_index']\n",
                "        test_df['split_index'] = s['split_index']\n",
                "        \n",
                "        all_train_data.append(train_df)\n",
                "        all_test_data.append(test_df)\n",
                "        \n",
                "        logger.info(f\"Split {s['split_index']} - Train shape: {train_df.shape}, Test shape: {test_df.shape}\")\n",
                "    \n",
                "    combined_train = pd.concat(all_train_data, ignore_index=True)\n",
                "    combined_test = pd.concat(all_test_data, ignore_index=True)\n",
                "    \n",
                "    logger.info(f\"\\nSaving combined training data (shape: {combined_train.shape}) to {output_train.path}\")\n",
                "    combined_train.to_csv(output_train.path, index=False)\n",
                "    \n",
                "    logger.info(f\"Saving combined test data (shape: {combined_test.shape}) to {output_test.path}\")\n",
                "    combined_test.to_csv(output_test.path, index=False)\n",
                "    \n",
                "    for split_index in range(1, 5):\n",
                "        train_count = len(combined_train[combined_train['split_index'] == split_index])\n",
                "        test_count = len(combined_test[combined_test['split_index'] == split_index])\n",
                "        logger.info(f\"\\nSplit {split_index} summary:\")\n",
                "        logger.info(f\"  Training samples: {train_count}\")\n",
                "        logger.info(f\"  Test samples: {test_count}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ac61f28c",
            "metadata": {},
            "source": [
                "# Data Metadata"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "551d1b17",
            "metadata": {},
            "outputs": [],
            "source": [
                "#generate_dataset_statistics\n",
                "@component(\n",
                "    base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/b2b_ai/wf_pipeline/training:1.0.2-rc\"\n",
                ")\n",
                "def generate_dataset_statistics(\n",
                "    train_dataset: Input[Dataset],\n",
                "    test_dataset: Input[Dataset],\n",
                "    time_column: str,\n",
                "    target_column: str,\n",
                "    attribute_columns: List[str],\n",
                "    output_statistics: Output[Artifact]\n",
                ") -> Dict[str, Dict[str, Dict[str, any]]]:\n",
                "    \"\"\"\n",
                "    Generates statistics for the training and test datasets produced by the generate_time_series_cv component.\n",
                "\n",
                "    This component analyzes the combined training and test datasets, which include a split_index column\n",
                "    identifying different temporal splits. It generates statistics for each split as well as overall statistics.\n",
                "\n",
                "    Args:\n",
                "        train_dataset: Input[Dataset]\n",
                "            Combined training dataset with split_index column\n",
                "        test_dataset: Input[Dataset]\n",
                "            Combined test dataset with split_index column\n",
                "        output_statistics: Output[Artifact]\n",
                "            Output artifact to store the generated statistics\n",
                "        time_column: str\n",
                "            Name of the timestamp column\n",
                "        target_column: str\n",
                "            Name of the target column (worked hours)\n",
                "        attribute_columns: List[str]\n",
                "            List of categorical columns (location, type of work, technology, product)\n",
                "\n",
                "    Returns:\n",
                "        Dict containing statistics for:\n",
                "        - Each training split (train_split_1 through train_split_4)\n",
                "        - Each test split (test_split_1 through test_split_4)\n",
                "        - Overall statistics combining all data\n",
                "    \"\"\"\n",
                "    import pandas as pd\n",
                "    import json\n",
                "    import logging\n",
                "    from datetime import datetime\n",
                "\n",
                "    logging.basicConfig(level=logging.INFO)\n",
                "    logger = logging.getLogger(__name__)\n",
                "\n",
                "    def calculate_statistics(df: pd.DataFrame, dataset_type: str, split_number: int) -> Dict[str, any]:\n",
                "        # Convert timestamp column to datetime if it's not already\n",
                "        if not pd.api.types.is_datetime64_any_dtype(df[time_column]):\n",
                "            df[time_column] = pd.to_datetime(df[time_column])\n",
                "            \n",
                "        # Get date range as strings to avoid timestamp arithmetic\n",
                "        date_range_start = df[time_column].min()\n",
                "        date_range_end = df[time_column].max()\n",
                "        \n",
                "        stats = {\n",
                "            \"dataset_type\": dataset_type,\n",
                "            \"split_number\": split_number,\n",
                "            \"total_rows\": len(df),\n",
                "            \"date_range\": {\n",
                "                \"start\": date_range_start.strftime(\"%Y-%m-%d\"),\n",
                "                \"end\": date_range_end.strftime(\"%Y-%m-%d\")\n",
                "            },\n",
                "            \"target_column\": {\n",
                "                \"mean\": float(df[target_column].mean()),\n",
                "                \"median\": float(df[target_column].median()),\n",
                "                \"min\": float(df[target_column].min()),\n",
                "                \"max\": float(df[target_column].max()),\n",
                "                \"std\": float(df[target_column].std()),\n",
                "                \"total\": float(df[target_column].sum())\n",
                "            },\n",
                "            \"null_counts\": df.isnull().sum().to_dict(),\n",
                "            \"categorical_columns\": {}\n",
                "        }\n",
                "\n",
                "        for col in attribute_columns:\n",
                "            value_counts = df[col].value_counts()\n",
                "            stats[\"categorical_columns\"][col] = {\n",
                "                \"unique_values\": int(df[col].nunique()),\n",
                "                \"top_5_values\": value_counts.nlargest(5).to_dict(),\n",
                "                \"null_count\": int(df[col].isnull().sum()),\n",
                "                \"total_count\": int(len(df)),\n",
                "                \"distribution_percentage\": value_counts.nlargest(5).apply(lambda x: float(x/len(df) * 100)).to_dict()\n",
                "            }\n",
                "\n",
                "        return stats\n",
                "\n",
                "    all_statistics = {}\n",
                "\n",
                "    logger.info(\"Reading input datasets\")\n",
                "    # Read CSVs with explicit datetime parsing\n",
                "    train_df = pd.read_csv(train_dataset.path)\n",
                "    test_df = pd.read_csv(test_dataset.path)\n",
                "    \n",
                "    # Convert time column to datetime after reading\n",
                "    train_df[time_column] = pd.to_datetime(train_df[time_column])\n",
                "    test_df[time_column] = pd.to_datetime(test_df[time_column])\n",
                "\n",
                "    logger.info(\"Processing training splits\")\n",
                "    for split_index in range(1, 5):\n",
                "        split_train = train_df[train_df['split_index'] == split_index]\n",
                "        logger.info(f\"Processing training split {split_index} (shape: {split_train.shape})\")\n",
                "        all_statistics[f\"train_split_{split_index}\"] = calculate_statistics(split_train, \"train\", split_index)\n",
                "\n",
                "    logger.info(\"Processing test splits\")\n",
                "    for split_index in range(1, 5):\n",
                "        split_test = test_df[test_df['split_index'] == split_index]\n",
                "        logger.info(f\"Processing test split {split_index} (shape: {split_test.shape})\")\n",
                "        all_statistics[f\"test_split_{split_index}\"] = calculate_statistics(split_test, \"test\", split_index)\n",
                "        \n",
                "    logger.info(\"Calculating overall statistics\")\n",
                "    all_data = pd.concat([train_df, test_df])\n",
                "    all_statistics[\"overall\"] = calculate_statistics(all_data, \"overall\", 0)\n",
                "\n",
                "    # Calculate summary statistics\n",
                "    earliest_date = all_data[time_column].min()\n",
                "    latest_date = all_data[time_column].max()\n",
                "    \n",
                "    all_statistics[\"summary\"] = {\n",
                "        \"total_rows\": {\n",
                "            \"train\": len(train_df),\n",
                "            \"test\": len(test_df),\n",
                "            \"total\": len(all_data)\n",
                "        },\n",
                "        \"date_range\": {\n",
                "            \"earliest\": earliest_date.strftime(\"%Y-%m-%d\"),\n",
                "            \"latest\": latest_date.strftime(\"%Y-%m-%d\")\n",
                "        },\n",
                "        \"splits_info\": {\n",
                "            f\"split_{i}\": {\n",
                "                \"train_rows\": len(train_df[train_df['split_index'] == i]),\n",
                "                \"test_rows\": len(test_df[test_df['split_index'] == i])\n",
                "            } for i in range(1, 5)\n",
                "        }\n",
                "    }\n",
                "\n",
                "    logger.info(f\"Saving statistics to {output_statistics.path}\")\n",
                "    with open(output_statistics.path, \"w\") as f:\n",
                "        json.dump(all_statistics, f, indent=2)\n",
                "\n",
                "    logger.info(\"Statistics generation completed\")\n",
                "    return all_statistics\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "9f44cd1d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# generate_statistics_visualization\n",
                "@component(\n",
                "    base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/b2b_ai/wf_pipeline/training:1.0.2-rc\"\n",
                ")\n",
                "def generate_statistics_visualization(\n",
                "    statistics_artifact: Input[Artifact],\n",
                "    train_dataset: Input[Artifact],\n",
                "    test_dataset: Input[Artifact],\n",
                "    attribute_columns: List[str],\n",
                "    time_column: str,\n",
                "    target_column: str,\n",
                "    output_visualization: Output[Artifact]\n",
                "):\n",
                "    \"\"\"\n",
                "    Generates an interactive HTML report visualizing the statistics from the dataset analysis.\n",
                "    Focuses on train split 4 (which contains all historical data) and creates specific visualizations:\n",
                "    1. Categorical variables distribution\n",
                "    2. Time series visualization for the last split\n",
                "    3. Target column statistics card\n",
                "    4. Weekly time series comparison between train and test data for split 4\n",
                "    \n",
                "    Args:\n",
                "        statistics_artifact: Input artifact containing the JSON statistics\n",
                "        train_dataset: Training dataset for additional visualizations\n",
                "        test_dataset: Test dataset for additional visualizations\n",
                "        output_visualization: Output artifact for the HTML report\n",
                "        time_column: Name of the timestamp column\n",
                "        target_column: Name of the target column (worked hours)\n",
                "    \"\"\"\n",
                "    import json\n",
                "    import pandas as pd\n",
                "    import numpy as np\n",
                "    import plotly.express as px\n",
                "    import plotly.graph_objects as go\n",
                "    from plotly.subplots import make_subplots\n",
                "    import logging\n",
                "    from datetime import datetime, timedelta\n",
                "\n",
                "    logging.basicConfig(level=logging.INFO)\n",
                "    logger = logging.getLogger(__name__)\n",
                "\n",
                "    logger.info(\"Loading statistics from JSON\")\n",
                "    with open(statistics_artifact.path, 'r') as f:\n",
                "        stats = json.load(f)\n",
                "\n",
                "    logger.info(\"Loading datasets\")\n",
                "    train_df = pd.read_csv(train_dataset.path)\n",
                "    train_df[time_column] = pd.to_datetime(train_df[time_column])\n",
                "    \n",
                "    test_df = pd.read_csv(test_dataset.path)\n",
                "    test_df[time_column] = pd.to_datetime(test_df[time_column])\n",
                "\n",
                "    train_split4 = train_df[train_df['split_index'] == 4]\n",
                "    test_split4 = test_df[test_df['split_index'] == 4]\n",
                "    split4_stats = stats['train_split_4']\n",
                "\n",
                "    figures = []\n",
                "    \n",
                "    def create_categorical_distributions():\n",
                "        logger.info(\"Creating categorical distributions chart\")\n",
                "        \n",
                "        fig = make_subplots(\n",
                "            rows=4, cols=2,\n",
                "            subplot_titles=attribute_columns,\n",
                "            vertical_spacing=0.12,\n",
                "            horizontal_spacing=0.1\n",
                "        )\n",
                "        \n",
                "        for idx, col in enumerate(attribute_columns, 1):\n",
                "            row = ((idx-1) // 2) + 1\n",
                "            col_num = ((idx-1) % 2) + 1\n",
                "            \n",
                "            col_stats = split4_stats['categorical_columns'][col]\n",
                "            values = list(col_stats['distribution_percentage'].values())\n",
                "            labels = list(col_stats['distribution_percentage'].keys())\n",
                "            \n",
                "            fig.add_trace(\n",
                "                go.Bar(\n",
                "                    x=labels,\n",
                "                    y=values,\n",
                "                    name=col,\n",
                "                    text=[f'{v:.1f}%' for v in values],\n",
                "                    textposition='auto',\n",
                "                    showlegend=False\n",
                "                ),\n",
                "                row=row, col=col_num\n",
                "            )\n",
                "            \n",
                "            fig.update_xaxes(tickangle=45, row=row, col=col_num)\n",
                "            fig.update_yaxes(title='Percentage (%)', row=row, col=col_num)\n",
                "\n",
                "        fig.update_layout(\n",
                "            height=1200,\n",
                "            title_text=\"Categorical Variables Distribution (Train Split 4)\",\n",
                "            showlegend=False\n",
                "        )\n",
                "        return fig\n",
                "\n",
                "    def create_target_stats_card():\n",
                "        logger.info(\"Creating target statistics card\")\n",
                "        target_stats = split4_stats['target_column']\n",
                "        \n",
                "        fig = make_subplots(\n",
                "            rows=1, cols=2,\n",
                "            column_widths=[0.5, 0.5],\n",
                "            specs=[[{\"type\": \"table\"}, {\"type\": \"violin\"}]],\n",
                "            horizontal_spacing=0.05\n",
                "        )\n",
                "        \n",
                "        fig.add_trace(\n",
                "            go.Table(\n",
                "                header=dict(\n",
                "                    values=['Metric', 'Value'],\n",
                "                    fill_color='lightgrey',\n",
                "                    align='left',\n",
                "                    font=dict(size=14)\n",
                "                ),\n",
                "                cells=dict(\n",
                "                    values=[\n",
                "                        ['Mean', 'Standard Deviation', 'Minimum', 'Maximum'],\n",
                "                        [\n",
                "                            f\"{target_stats['mean']:,.2f}\",\n",
                "                            f\"{target_stats['std']:,.2f}\",\n",
                "                            f\"{target_stats['min']:,.2f}\",\n",
                "                            f\"{target_stats['max']:,.2f}\",\n",
                "                        ]\n",
                "                    ],\n",
                "                    align='left',\n",
                "                    font=dict(size=13)\n",
                "                )\n",
                "            ),\n",
                "            row=1, col=1\n",
                "        )\n",
                "        \n",
                "        log_swt = np.log1p(train_split4[target_column])\n",
                "        \n",
                "        fig.add_trace(\n",
                "            go.Violin(\n",
                "                y=log_swt,\n",
                "                name=f\"log({target_column} + 1)\",\n",
                "                box_visible=False,\n",
                "                meanline_visible=True,\n",
                "                points=False,\n",
                "                fillcolor='lightblue',\n",
                "                line_color='blue',\n",
                "                showlegend=False,\n",
                "                hovertemplate=\"log(%{y:.2f})<extra></extra>\"\n",
                "            ),\n",
                "            row=1, col=2\n",
                "        )\n",
                "\n",
                "        fig.update_layout(\n",
                "            title=f\"Target Column Statistics - log({target_column} + 1) (Train Split 4)\",\n",
                "            height=400,\n",
                "            showlegend=False,\n",
                "            yaxis=dict(\n",
                "                title=f\"log({target_column} + 1)\",\n",
                "                tickformat=\".2f\",\n",
                "                side='right'\n",
                "            )\n",
                "        )\n",
                "        \n",
                "        return fig\n",
                "\n",
                "    def create_weekly_timeseries_comparison():\n",
                "        logger.info(\"Creating weekly time series comparison\")\n",
                "        \n",
                "\n",
                "        last_train_date = train_split4[time_column].max()\n",
                "        one_year_ago = last_train_date - timedelta(days=365)\n",
                "        \n",
                "\n",
                "        train_last_year = train_split4[train_split4[time_column] >= one_year_ago].copy()\n",
                "        \n",
                "\n",
                "        train_weekly = train_last_year.groupby(pd.Grouper(key=time_column, freq='W-MON'))[target_column].sum().reset_index()\n",
                "        test_weekly = test_split4.groupby(pd.Grouper(key=time_column, freq='W-MON'))[target_column].sum().reset_index()\n",
                "        \n",
                "        fig = go.Figure()\n",
                "        \n",
                "\n",
                "        fig.add_trace(\n",
                "            go.Scatter(\n",
                "                x=train_weekly[time_column],\n",
                "                y=train_weekly[target_column],\n",
                "                name='Training Data',\n",
                "                line=dict(color='blue', width=2),\n",
                "                mode='lines',\n",
                "                hovertemplate=\"Date: %{x}<br>Value: %{y:,.0f}<extra></extra>\"\n",
                "            )\n",
                "        )\n",
                "        \n",
                "\n",
                "        fig.add_trace(\n",
                "            go.Scatter(\n",
                "                x=test_weekly[time_column],\n",
                "                y=test_weekly[target_column],\n",
                "                name='Test Data',\n",
                "                line=dict(color='red', width=2),\n",
                "                mode='lines',\n",
                "                hovertemplate=\"Date: %{x}<br>Value: %{y:,.0f}<extra></extra>\"\n",
                "            )\n",
                "        )\n",
                "        \n",
                "        fig.update_layout(\n",
                "            title=\"Weekly Time Series Comparison (Split 4)\",\n",
                "            xaxis_title=\"Date\",\n",
                "            yaxis_title=target_column,\n",
                "            height=500,\n",
                "            showlegend=True,\n",
                "            legend=dict(\n",
                "                yanchor=\"top\",\n",
                "                y=0.99,\n",
                "                xanchor=\"left\",\n",
                "                x=0.01\n",
                "            ),\n",
                "            hovermode='x unified'\n",
                "        )\n",
                "        \n",
                "        return fig\n",
                "\n",
                "    figures.extend([\n",
                "        create_target_stats_card(),\n",
                "        create_weekly_timeseries_comparison(),\n",
                "        create_categorical_distributions()\n",
                "    ])\n",
                "\n",
                "    html_parts = [\n",
                "        \"<!DOCTYPE html>\",\n",
                "        \"<html>\",\n",
                "        \"<head>\",\n",
                "        \"<title>Workforce Time Series Analysis Report - Split 4</title>\",\n",
                "        \"<script src='https://cdn.plot.ly/plotly-latest.min.js'></script>\",\n",
                "        \"<style>\",\n",
                "        \"body { margin: 20px; }\",\n",
                "        \".header { text-align: center; margin-bottom: 30px; }\",\n",
                "        \".plot { margin-bottom: 40px; }\",\n",
                "        \".stats-summary { background: #f5f5f5; padding: 20px; border-radius: 5px; margin-bottom: 20px; }\",\n",
                "        \"</style>\",\n",
                "        \"</head>\",\n",
                "        \"<body>\",\n",
                "        \"<div class='header'>\",\n",
                "        \"<h1>Workforce Time Series Analysis Report - Split 4</h1>\",\n",
                "        f\"<p>Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\",\n",
                "        \"</div>\",\n",
                "        \"<div class='stats-summary'>\",\n",
                "        \"<h2>Dataset Overview</h2>\",\n",
                "        f\"<p>Total Records: {split4_stats['total_rows']:,}</p>\",\n",
                "        f\"<p>Date Range: {split4_stats['date_range']['start']} to {split4_stats['date_range']['end']}</p>\",\n",
                "        \"</div>\"\n",
                "    ]\n",
                "\n",
                "    for fig in figures:\n",
                "        html_parts.append(f\"<div class='plot'>{fig.to_html(full_html=False, include_plotlyjs=False)}</div>\")\n",
                "\n",
                "    html_parts.extend([\n",
                "        \"</body>\",\n",
                "        \"</html>\"\n",
                "    ])\n",
                "\n",
                "    logger.info(f\"Saving HTML report to {output_visualization.path}\")\n",
                "    with open(output_visualization.path, \"w\") as f:\n",
                "        f.write(\"\\n\".join(html_parts))\n",
                "\n",
                "    logger.info(\"Visualization generation completed\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "279abcac",
            "metadata": {},
            "source": [
                "# Models training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "8e2079f9",
            "metadata": {},
            "outputs": [],
            "source": [
                "#sma_trainer_component\n",
                "@component(\n",
                "    base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/b2b_ai/wf_pipeline/training:1.0.2-rc\"\n",
                ")\n",
                "def sma_trainer_component(\n",
                "    dataset: Input[Dataset],\n",
                "    experiment_name: str,\n",
                "    window_size: int,\n",
                "    project_id: str,\n",
                "    project_location: str,\n",
                "    time_column: str,\n",
                "    target_column: str,\n",
                "    series_identifier: str,\n",
                "    forecast_horizon: int,\n",
                "    run_name: str,\n",
                "    output_model: Output[Model],\n",
                "    experiment_run: Output[Artifact]\n",
                "):\n",
                "    import pickle\n",
                "    from google.cloud import aiplatform\n",
                "    import pandas as pd\n",
                "    import numpy as np\n",
                "    from datetime import datetime, timedelta\n",
                "    import json\n",
                "    import logging\n",
                "\n",
                "    logging.basicConfig(\n",
                "        level=logging.INFO,\n",
                "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
                "    )\n",
                "    logger = logging.getLogger(__name__)\n",
                "\n",
                "    def generate_rolling_predictions(values, window_size, horizon):\n",
                "        \"\"\"Generate predictions using a rolling window approach\"\"\"\n",
                "        predictions = []\n",
                "        rolling_window = np.array(values[-window_size:])\n",
                "        \n",
                "        for _ in range(horizon):\n",
                "            next_pred = np.mean(rolling_window)\n",
                "            predictions.append(next_pred)\n",
                "            rolling_window = np.roll(rolling_window, -1)\n",
                "            rolling_window[-1] = next_pred\n",
                "            \n",
                "        return predictions\n",
                "\n",
                "    logger.info(f\"Starting SMA trainer component with window size: {window_size}\")\n",
                "    \n",
                "    logger.info(f\"Initializing Vertex AI SDK for project: {project_id}, location: {project_location}\")\n",
                "    aiplatform.init(project=project_id, location=project_location, experiment=experiment_name)\n",
                "\n",
                "    logger.info(f\"Reading dataset from: {dataset.path}\")\n",
                "    df = pd.read_csv(dataset.path, parse_dates=[time_column])\n",
                "    logger.info(f\"Dataset shape: {df.shape}\")\n",
                "\n",
                "    model_output = {\n",
                "        'model_type': 'SMA',\n",
                "        'parameters': {'window_size': window_size},\n",
                "        'predictions': {}\n",
                "    }\n",
                "\n",
                "    timestamp = datetime.now().strftime('%Y-%m-%d-%H%M')\n",
                "    run = aiplatform.start_run(f\"{run_name}-{timestamp}\")\n",
                "\n",
                "    unique_series = df[series_identifier].unique()\n",
                "    unique_splits = df['split_index'].unique()\n",
                "    \n",
                "    total_combinations = len(unique_series) * len(unique_splits)\n",
                "    successful_predictions = 0\n",
                "    \n",
                "    for series_id in unique_series:\n",
                "        for split_index in unique_splits:\n",
                "            series_data = df[\n",
                "                (df[series_identifier] == series_id) & \n",
                "                (df['split_index'] == split_index)\n",
                "            ].sort_values(time_column)\n",
                "            \n",
                "            if len(series_data) == 0:\n",
                "                logger.warning(f\"No data for series {series_id}, split {split_index}\")\n",
                "                continue\n",
                "            \n",
                "            try:\n",
                "                historical_values = series_data[target_column].values\n",
                "                future_predictions = generate_rolling_predictions(\n",
                "                    historical_values,\n",
                "                    window_size,\n",
                "                    forecast_horizon\n",
                "                )\n",
                "                \n",
                "                last_date = series_data[time_column].iloc[-1]\n",
                "                future_dates = [(last_date + timedelta(days=i+1)).strftime('%Y-%m-%d')\n",
                "                              for i in range(forecast_horizon)]\n",
                "                \n",
                "                model_output['predictions'][(series_id, split_index)] = {\n",
                "                    'series_id': series_id,\n",
                "                    'split_index': split_index,\n",
                "                    'timestamps': future_dates,\n",
                "                    'values': future_predictions,\n",
                "                    'metadata': {\n",
                "                        'last_training_date': last_date.strftime('%Y-%m-%d'),\n",
                "                        'window_size': window_size\n",
                "                    }\n",
                "                }\n",
                "                successful_predictions += 1\n",
                "            except Exception as e:\n",
                "                logger.error(f\"Error processing series {series_id}, split {split_index}: {str(e)}\")\n",
                "    \n",
                "    logger.info(f\"Successfully processed {successful_predictions} out of {total_combinations} combinations\")\n",
                "    \n",
                "    with open(output_model.path, \"wb\") as f:\n",
                "        pickle.dump(model_output, f)\n",
                "\n",
                "    run.log_params({\n",
                "        \"window_size\": window_size,\n",
                "        \"model_uri\": output_model.path,\n",
                "        \"forecast_horizon\": forecast_horizon,\n",
                "        \"total_series\": len(unique_series),\n",
                "        \"total_splits\": len(unique_splits)\n",
                "    })\n",
                "    \n",
                "    run.log_metrics({\n",
                "        \"total_combinations\": total_combinations,\n",
                "        \"successful_predictions\": successful_predictions,\n",
                "        \"completion_rate\": successful_predictions / total_combinations\n",
                "    })\n",
                "\n",
                "    run_info = {\n",
                "        \"run_name\": run.name,\n",
                "        \"experiment\": experiment_name,\n",
                "        \"project_id\": project_id,\n",
                "        \"location\": project_location\n",
                "    }\n",
                "    with open(experiment_run.path, 'w') as f:\n",
                "        json.dump(run_info, f)\n",
                "\n",
                "    logger.info(\"SMA trainer component completed successfully\")\n",
                "\n",
                "sma_trainer_component_job = create_custom_training_job_from_component(\n",
                "    sma_trainer_component,\n",
                "    display_name='sma-model-training',\n",
                "    machine_type='e2-highcpu-16',\n",
                "    service_account='notebook-service-account@wb-ai-acltr-tbs-3-pr-a62583.iam.gserviceaccount.com'\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "019cdcfb",
            "metadata": {},
            "outputs": [],
            "source": [
                "#sarima_trainer_component\n",
                "@component(\n",
                "    base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/b2b_ai/wf_pipeline/training:1.0.2-rc\"\n",
                ")\n",
                "def sarima_trainer_component(\n",
                "    dataset: Input[Dataset],\n",
                "    experiment_name: str,\n",
                "    project_id: str,\n",
                "    project_location: str,\n",
                "    time_column: str,\n",
                "    target_column: str,\n",
                "    series_identifier: str,\n",
                "    forecast_horizon: int,\n",
                "    run_name: str,\n",
                "    output_model: Output[Model],\n",
                "    experiment_run: Output[Artifact]\n",
                "):\n",
                "\n",
                "    import pickle\n",
                "    from google.cloud import aiplatform\n",
                "    import pandas as pd\n",
                "    import numpy as np\n",
                "    from datetime import datetime, timedelta\n",
                "    import json\n",
                "    import logging\n",
                "    from concurrent.futures import ProcessPoolExecutor\n",
                "    import os\n",
                "    from pmdarima import auto_arima\n",
                "\n",
                "    # Configure logging to be multiprocessing-safe\n",
                "    logging.basicConfig(\n",
                "        level=logging.INFO,\n",
                "        format='%(asctime)s - %(processName)s - %(levelname)s - %(message)s'\n",
                "    )\n",
                "    logger = logging.getLogger(__name__)\n",
                "\n",
                "    def fit_sarima_model(values):\n",
                "        \"\"\"Fit SARIMA model using auto_arima\"\"\"\n",
                "        if len(values) == 0:\n",
                "            logger.warning(\"Empty array passed to fit_sarima_model. Returning None.\")\n",
                "            return None\n",
                "        try:\n",
                "            model = auto_arima(values, seasonal=True, m=7, suppress_warnings=True, error_action=\"ignore\")\n",
                "            return model\n",
                "        except Exception as e:\n",
                "            logger.error(f\"Error in fit_sarima_model: {str(e)}\")\n",
                "            return None\n",
                "\n",
                "    def generate_sarima_predictions(model, horizon):\n",
                "        \"\"\"Generate predictions using the fitted SARIMA model\"\"\"\n",
                "        if model is None:\n",
                "            logger.warning(\"None model passed to generate_sarima_predictions. Returning zeros.\")\n",
                "            return np.zeros(horizon)\n",
                "        try:\n",
                "            return model.predict(n_periods=horizon)\n",
                "        except Exception as e:\n",
                "            logger.error(f\"Error in generate_sarima_predictions: {str(e)}\")\n",
                "            return np.zeros(horizon)\n",
                "\n",
                "    def process_series_split_sarima(args):\n",
                "        \"\"\"Worker function to process a single series-split combination for SARIMA\"\"\"\n",
                "        series_id, split_index, series_data, forecast_horizon, time_column, target_column = args\n",
                "        \n",
                "        try:\n",
                "            if len(series_data) == 0:\n",
                "                return None\n",
                "            \n",
                "            historical_values = series_data[target_column].values\n",
                "            \n",
                "            sarima_model = fit_sarima_model(historical_values)\n",
                "            if sarima_model is None:\n",
                "                logger.warning(f\"Could not fit SARIMA model for series {series_id} in split {split_index}\")\n",
                "                return None\n",
                "            \n",
                "            future_predictions = generate_sarima_predictions(sarima_model, forecast_horizon)\n",
                "            \n",
                "            last_date = series_data[time_column].iloc[-1]\n",
                "            future_dates = [(last_date + timedelta(days=i+1)).strftime('%Y-%m-%d')\n",
                "                            for i in range(forecast_horizon)]\n",
                "            \n",
                "            return {\n",
                "                'series_id': series_id,\n",
                "                'split_index': split_index,\n",
                "                'timestamps': future_dates,\n",
                "                'values': future_predictions.tolist(),\n",
                "                'metadata': {\n",
                "                    'last_training_date': last_date.strftime('%Y-%m-%d'),\n",
                "                    'model_order': sarima_model.order(),\n",
                "                    'seasonal_order': sarima_model.seasonal_order()\n",
                "                },\n",
                "                'parameters': {\n",
                "                    'order': sarima_model.order(),\n",
                "                    'seasonal_order': sarima_model.seasonal_order()\n",
                "                }\n",
                "            }\n",
                "        except Exception as e:\n",
                "            logger.error(f\"Error processing series {series_id}, split {split_index}: {str(e)}\")\n",
                "            return None\n",
                "    \n",
                "    logger.info(\"Starting optimized SARIMA trainer component\")\n",
                "    \n",
                "    logger.info(f\"Initializing Vertex AI SDK for project: {project_id}, location: {project_location}\")\n",
                "    aiplatform.init(project=project_id, location=project_location, experiment=experiment_name)\n",
                "\n",
                "    logger.info(f\"Reading dataset from: {dataset.path}\")\n",
                "    df = pd.read_csv(dataset.path, parse_dates=[time_column])\n",
                "    logger.info(f\"Dataset shape: {df.shape}\")\n",
                "\n",
                "    model_output = {\n",
                "        'model_type': 'SARIMA',\n",
                "        'parameters': {},\n",
                "        'predictions': {}\n",
                "    }\n",
                "\n",
                "    timestamp = datetime.now().strftime('%Y-%m-%d-%H%M')\n",
                "    run = aiplatform.start_run(f\"{run_name}-{timestamp}\")\n",
                "\n",
                "    unique_series = df[series_identifier].unique()\n",
                "    unique_splits = df['split_index'].unique()\n",
                "\n",
                "    # Prepare work items\n",
                "    work_items = []\n",
                "    for series_id in unique_series:\n",
                "        for split_index in unique_splits:\n",
                "            series_split_df = df[(df[series_identifier] == series_id) & \n",
                "                               (df['split_index'] == split_index)].sort_values(time_column)\n",
                "            \n",
                "            work_items.append((\n",
                "                series_id,\n",
                "                split_index,\n",
                "                series_split_df,\n",
                "                forecast_horizon,\n",
                "                time_column,\n",
                "                target_column\n",
                "            ))\n",
                "\n",
                "    total_combinations = len(work_items)\n",
                "    logger.info(f\"Processing {total_combinations} series-split combinations using multiprocessing\")\n",
                "    \n",
                "    # Use all available CPUs except one for system processes\n",
                "    num_processes = os.cpu_count() - 1\n",
                "    logger.info(f\"Using {num_processes} processes for parallel processing\")\n",
                "    \n",
                "    # Process work items in parallel\n",
                "    with ProcessPoolExecutor(max_workers=num_processes) as executor:\n",
                "        results = list(executor.map(process_series_split_sarima, work_items))\n",
                "\n",
                "    # Collect results\n",
                "    successful_predictions = 0\n",
                "    for result in results:\n",
                "        if result is not None:\n",
                "            series_id = result['series_id']\n",
                "            split_index = result['split_index']\n",
                "            model_output['predictions'][f\"{series_id}_{split_index}\"] = result\n",
                "            model_output['parameters'][f\"{series_id}_{split_index}\"] = result['parameters']\n",
                "            successful_predictions += 1\n",
                "\n",
                "    logger.info(f\"Successfully processed {successful_predictions} out of {total_combinations} combinations\")\n",
                "\n",
                "    with open(output_model.path, \"wb\") as f:\n",
                "        pickle.dump(model_output, f)\n",
                "\n",
                "    run.log_params({\n",
                "        \"model_type\": \"SARIMA\",\n",
                "        \"model_uri\": output_model.path,\n",
                "        \"forecast_horizon\": forecast_horizon,\n",
                "        \"total_series\": len(unique_series),\n",
                "        \"total_splits\": len(unique_splits),\n",
                "        \"num_processes\": num_processes\n",
                "    })\n",
                "\n",
                "    run.log_metrics({\n",
                "        \"total_combinations\": total_combinations,\n",
                "        \"successful_predictions\": successful_predictions,\n",
                "        \"completion_rate\": successful_predictions / total_combinations\n",
                "    })\n",
                "\n",
                "    run_info = {\n",
                "        \"run_name\": run.name,\n",
                "        \"experiment\": experiment_name,\n",
                "        \"project_id\": project_id,\n",
                "        \"location\": project_location\n",
                "    }\n",
                "    with open(experiment_run.path, 'w') as f:\n",
                "        json.dump(run_info, f)\n",
                "\n",
                "    logger.info(\"Optimized SARIMA trainer component completed successfully\")\n",
                "\n",
                "sarima_trainer_component_job = create_custom_training_job_from_component(\n",
                "    sarima_trainer_component,\n",
                "    display_name='sarima-model-training',\n",
                "    machine_type='e2-highcpu-16',\n",
                "    service_account='notebook-service-account@wb-ai-acltr-tbs-3-pr-a62583.iam.gserviceaccount.com'\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "948d6684",
            "metadata": {},
            "source": [
                "# Model Eval"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "f1d55cda",
            "metadata": {},
            "outputs": [],
            "source": [
                "#model_evaluator_component\n",
                "@component(\n",
                "    base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/b2b_ai/wf_pipeline/training:1.0.2-rc\"\n",
                ")\n",
                "def model_evaluator_component(\n",
                "    test_dataset: Input[Dataset],\n",
                "    trained_model: Input[Model],\n",
                "    experiment_run: Input[Artifact],\n",
                "    time_column: str,\n",
                "    target_column: str,\n",
                "    series_identifier: str,\n",
                "    output_metrics: Output[Artifact]\n",
                "):\n",
                "    import pickle\n",
                "    from google.cloud import aiplatform\n",
                "    import pandas as pd\n",
                "    import numpy as np\n",
                "    import logging\n",
                "    from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
                "    import json\n",
                "    \n",
                "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
                "    logger = logging.getLogger(__name__)\n",
                "    \n",
                "    def get_tier(series_id):\n",
                "        if \"Tier 1\" in series_id:\n",
                "            return \"Tier 1\"\n",
                "        elif \"Tier 2\" in series_id:\n",
                "            return \"Tier 2\"\n",
                "        elif \"Tier 3\" in series_id:\n",
                "            return \"Tier 3\"\n",
                "        return \"Unknown\"\n",
                "    \n",
                "    def calculate_metrics(y_true, y_pred):\n",
                "        mae = mean_absolute_error(y_true, y_pred)\n",
                "        wape = np.sum(np.abs(y_true - y_pred)) / (np.sum(np.abs(y_true)) + 1e-8)\n",
                "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
                "        bias = np.mean(y_pred - y_true)\n",
                "        \n",
                "        return {\n",
                "            'MAE': mae,\n",
                "            'WAPE': wape,\n",
                "            'RMSE': rmse,\n",
                "            'Bias': bias\n",
                "        }\n",
                "\n",
                "    logger.info(\"Starting model evaluator component\")\n",
                "    \n",
                "    with open(experiment_run.path, 'r') as f:\n",
                "        run_info = json.load(f)\n",
                "    \n",
                "    logger.info(f\"Initializing Vertex AI SDK and continuing experiment run\")\n",
                "    aiplatform.init(project=run_info['project_id'], location=run_info['location'], experiment=run_info['experiment'])\n",
                "    run = aiplatform.start_run(run_info['run_name'], resume=True)\n",
                "\n",
                "    logger.info(f\"Reading test dataset from: {test_dataset.path}\")\n",
                "    test_df = pd.read_csv(test_dataset.path)\n",
                "    test_df[time_column] = pd.to_datetime(test_df[time_column])\n",
                "    \n",
                "    logger.info(f\"Loading model predictions from: {trained_model.path}\")\n",
                "    with open(trained_model.path, \"rb\") as f:\n",
                "        model_output = pickle.load(f)\n",
                "\n",
                "    tier_predictions = {\n",
                "        \"Tier 1\": {\"true\": [], \"pred\": []},\n",
                "        \"Tier 2\": {\"true\": [], \"pred\": []},\n",
                "        \"Tier 3\": {\"true\": [], \"pred\": []},\n",
                "        \"Overall\": {\"true\": [], \"pred\": []}\n",
                "    }\n",
                "\n",
                "    logger.info(f\"Evaluating model type: {model_output['model_type']}\")\n",
                "    \n",
                "    for (series_id, split), predictions in model_output['predictions'].items():\n",
                "        tier = get_tier(series_id)\n",
                "        \n",
                "        series_test = test_df[(test_df[series_identifier] == series_id) & \n",
                "                              (test_df['split_index'] == split)].sort_values(time_column)\n",
                "        \n",
                "        if len(series_test) == 0:\n",
                "            logger.warning(f\"No test data for series {series_id}, split {split}\")\n",
                "            continue\n",
                "\n",
                "        pred_dates = pd.to_datetime(predictions['timestamps'])\n",
                "        y_true = series_test[series_test[time_column].isin(pred_dates)][target_column].values\n",
                "        y_pred = np.array(predictions['values'])[:len(y_true)]\n",
                "        \n",
                "        if len(y_true) == 0 or len(y_pred) == 0:\n",
                "            logger.warning(f\"No matching data for series {series_id}, split {split}\")\n",
                "            continue\n",
                "\n",
                "        logger.info(f\"Series {series_id}, Split {split}: True shape: {y_true.shape}, Pred shape: {y_pred.shape}\")\n",
                "\n",
                "        if tier in tier_predictions:\n",
                "            tier_predictions[tier][\"true\"].extend(y_true)\n",
                "            tier_predictions[tier][\"pred\"].extend(y_pred)\n",
                "        tier_predictions[\"Overall\"][\"true\"].extend(y_true)\n",
                "        tier_predictions[\"Overall\"][\"pred\"].extend(y_pred)\n",
                "\n",
                "    all_metrics = {}\n",
                "    \n",
                "    for tier, values in tier_predictions.items():\n",
                "        if not len(values['true']):\n",
                "            logger.warning(f\"No data for {tier}\")\n",
                "            continue\n",
                "\n",
                "        y_true = np.array(values[\"true\"])\n",
                "        y_pred = np.array(values[\"pred\"])\n",
                "        \n",
                "        logger.info(f\"{tier} - True shape: {y_true.shape}, Pred shape: {y_pred.shape}\")\n",
                "        \n",
                "        metrics = calculate_metrics(y_true, y_pred)\n",
                "        \n",
                "        prefix = \"Overall_\" if tier == \"Overall\" else f\"{tier}_\"\n",
                "        all_metrics.update({\n",
                "            f\"{prefix}{metric_name}\": value \n",
                "            for metric_name, value in metrics.items()\n",
                "        })\n",
                "        \n",
                "        logger.info(f\"\\n{tier} Metrics:\")\n",
                "        for metric_name, value in metrics.items():\n",
                "            logger.info(f\"{metric_name}: {value:.4f}\")\n",
                "\n",
                "    # Log metrics and parameters dynamically based on model output\n",
                "    run.log_metrics(all_metrics)\n",
                "\n",
                "    # Log model parameters dynamically\n",
                "    model_params = {\n",
                "        \"model_type\": model_output['model_type'],\n",
                "        \"time_column\": time_column,\n",
                "        \"target_column\": target_column,\n",
                "        \"series_identifier\": series_identifier\n",
                "    }\n",
                "    \n",
                "    # Add any model-specific parameters from the model output\n",
                "    if 'parameters' in model_output:\n",
                "        if isinstance(model_output['parameters'], dict):\n",
                "            # For models with global parameters (like SMA)\n",
                "            model_params.update(model_output['parameters'])\n",
                "        else:\n",
                "            # For models with per-series parameters (like SARIMA)\n",
                "            # Log the parameters of the first series as an example\n",
                "            first_series_params = next(iter(model_output['parameters'].values()))\n",
                "            model_params.update({\n",
                "                f\"example_series_params_{k}\": str(v) \n",
                "                for k, v in first_series_params.items()\n",
                "            })\n",
                "\n",
                "    run.log_params(model_params)\n",
                "    \n",
                "    logger.info(\"Model evaluator component completed successfully\")\n",
                "\n",
                "    with open(output_metrics.path, 'w') as f:\n",
                "        json.dump(all_metrics, f)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "97ed7366",
            "metadata": {},
            "outputs": [],
            "source": [
                "#generate_model_report_card\n",
                "@component(\n",
                "    base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/b2b_ai/wf_pipeline/training:1.0.2-rc\"\n",
                ")\n",
                "def generate_model_report_card(\n",
                "    evaluation_metrics: Input[Artifact],\n",
                "    statistics_artifact: Input[Artifact],\n",
                "    trained_model: Input[Model],  # Added to access model parameters\n",
                "    time_column: str,\n",
                "    target_column: str,\n",
                "    series_identifier: str,\n",
                "    output_report: Output[Artifact]\n",
                "):\n",
                "    import json\n",
                "    import pandas as pd\n",
                "    import plotly.express as px\n",
                "    import plotly.graph_objects as go\n",
                "    from plotly.subplots import make_subplots\n",
                "    import logging\n",
                "    from datetime import datetime\n",
                "    import pickle\n",
                "\n",
                "    logging.basicConfig(level=logging.INFO)\n",
                "    logger = logging.getLogger(__name__)\n",
                "\n",
                "    logger.info(\"Loading metrics, statistics, and model\")\n",
                "    with open(evaluation_metrics.path, 'r') as f:\n",
                "        metrics = json.load(f)\n",
                "    with open(statistics_artifact.path, 'r') as f:\n",
                "        stats = json.load(f)\n",
                "    with open(trained_model.path, 'rb') as f:\n",
                "        model_output = pickle.load(f)\n",
                "\n",
                "    def create_metrics_comparison():\n",
                "        logger.info(\"Creating metrics comparison plots\")\n",
                "        \n",
                "        tiers = ['Overall', 'Tier 1', 'Tier 2', 'Tier 3']\n",
                "        metric_types = ['MAE', 'WAPE', 'RMSE', 'Bias']\n",
                "        \n",
                "        fig = make_subplots(\n",
                "            rows=2, cols=2,\n",
                "            subplot_titles=metric_types,\n",
                "            vertical_spacing=0.15,\n",
                "            horizontal_spacing=0.1\n",
                "        )\n",
                "        \n",
                "        colors = px.colors.qualitative.Set3\n",
                "        \n",
                "        for idx, metric in enumerate(metric_types, 1):\n",
                "            row = ((idx-1) // 2) + 1\n",
                "            col = ((idx-1) % 2) + 1\n",
                "            \n",
                "            values = []\n",
                "            for tier in tiers:\n",
                "                prefix = 'Overall_' if tier == 'Overall' else f\"{tier}_\"\n",
                "                key = f\"{prefix}{metric}\"\n",
                "                if key in metrics:\n",
                "                    values.append(metrics[key])\n",
                "                else:\n",
                "                    values.append(None)\n",
                "            \n",
                "            fig.add_trace(\n",
                "                go.Bar(\n",
                "                    x=tiers,\n",
                "                    y=values,\n",
                "                    name=metric,\n",
                "                    text=[f'{v:.2f}' if v is not None else 'N/A' for v in values],\n",
                "                    textposition='auto',\n",
                "                    marker_color=colors[idx-1],\n",
                "                    showlegend=False\n",
                "                ),\n",
                "                row=row, col=col\n",
                "            )\n",
                "            \n",
                "            fig.update_xaxes(title='Tier', row=row, col=col)\n",
                "            fig.update_yaxes(title=metric, row=row, col=col)\n",
                "\n",
                "        fig.update_layout(\n",
                "            height=800,\n",
                "            title_text=\"Model Performance Metrics by Tier\",\n",
                "            showlegend=False\n",
                "        )\n",
                "        return fig\n",
                "\n",
                "    def create_dataset_summary():\n",
                "        logger.info(\"Creating dataset summary\")\n",
                "        \n",
                "        summary = stats['summary']\n",
                "        split_info = summary['splits_info']\n",
                "        \n",
                "        fig = go.Figure(data=[go.Table(\n",
                "            header=dict(\n",
                "                values=['Metric', 'Value'],\n",
                "                fill_color='lightgrey',\n",
                "                align='left',\n",
                "                font=dict(size=14)\n",
                "            ),\n",
                "            cells=dict(\n",
                "                values=[\n",
                "                    [\n",
                "                        'Total Training Rows',\n",
                "                        'Total Test Rows',\n",
                "                        'Date Range',\n",
                "                        'Split 4 Training Rows',\n",
                "                        'Split 4 Test Rows'\n",
                "                    ],\n",
                "                    [\n",
                "                        f\"{summary['total_rows']['train']:,}\",\n",
                "                        f\"{summary['total_rows']['test']:,}\",\n",
                "                        f\"{summary['date_range']['earliest']} to {summary['date_range']['latest']}\",\n",
                "                        f\"{split_info['split_4']['train_rows']:,}\",\n",
                "                        f\"{split_info['split_4']['test_rows']:,}\"\n",
                "                    ]\n",
                "                ],\n",
                "                align='left',\n",
                "                font=dict(size=13)\n",
                "            )\n",
                "        )])\n",
                "        \n",
                "        fig.update_layout(\n",
                "            title=\"Dataset Summary\",\n",
                "            height=300\n",
                "        )\n",
                "        return fig\n",
                "\n",
                "    def create_feature_summary():\n",
                "        logger.info(\"Creating detailed feature summary\")\n",
                "        \n",
                "        split_stats = stats['train_split_4']\n",
                "        \n",
                "        feature_names = []\n",
                "        null_counts = []\n",
                "        unique_values = []\n",
                "        \n",
                "        feature_names.append(time_column)\n",
                "        null_counts.append(split_stats['null_counts'].get(time_column, 'N/A'))\n",
                "        unique_values.append('N/A (Date column)')\n",
                "        \n",
                "        feature_names.append(target_column)\n",
                "        null_counts.append(split_stats['null_counts'].get(target_column, 'N/A'))\n",
                "        unique_values.append('N/A (Continuous)')\n",
                "        \n",
                "        feature_names.append(series_identifier)\n",
                "        null_counts.append(split_stats['null_counts'].get(series_identifier, 'N/A'))\n",
                "        unique_values.append(split_stats['categorical_columns'].get(series_identifier, {}).get('unique_values', 'N/A'))\n",
                "        \n",
                "        for col, col_stats in split_stats['categorical_columns'].items():\n",
                "            if col != series_identifier:\n",
                "                feature_names.append(col)\n",
                "                null_counts.append(split_stats['null_counts'].get(col, 'N/A'))\n",
                "                unique_values.append(col_stats['unique_values'])\n",
                "        \n",
                "        fig = go.Figure(data=[go.Table(\n",
                "            header=dict(\n",
                "                values=['Feature', 'Null Count', 'Unique Values'],\n",
                "                fill_color='lightgrey',\n",
                "                align='left',\n",
                "                font=dict(size=14)\n",
                "            ),\n",
                "            cells=dict(\n",
                "                values=[feature_names, null_counts, unique_values],\n",
                "                align='left',\n",
                "                font=dict(size=13)\n",
                "            )\n",
                "        )])\n",
                "        \n",
                "        fig.update_layout(\n",
                "            title=\"Feature Summary\",\n",
                "            height=400\n",
                "        )\n",
                "        return fig\n",
                "\n",
                "    def get_model_parameters_description():\n",
                "        \"\"\"Dynamically generate model parameters description based on model type\"\"\"\n",
                "        model_type = model_output['model_type']\n",
                "        parameters = model_output.get('parameters', {})\n",
                "        \n",
                "        if model_type == 'SMA':\n",
                "            return f\"The Simple Moving Average model uses a window size of {parameters.get('window_size')} to generate predictions based on historical values.\"\n",
                "        elif model_type == 'SARIMA':\n",
                "            # For SARIMA, we'll show parameters from the first series as an example\n",
                "            first_series_params = next(iter(parameters.values()))\n",
                "            return (f\"The SARIMA model uses automatically determined parameters for each series. \"\n",
                "                   f\"Example parameters from one series - Order: {first_series_params.get('order')}, \"\n",
                "                   f\"Seasonal Order: {first_series_params.get('seasonal_order')}\")\n",
                "        else:\n",
                "            return f\"This is a {model_type} model. Refer to model documentation for specific parameter details.\"\n",
                "\n",
                "    figures = [\n",
                "        create_metrics_comparison(),\n",
                "        create_dataset_summary(),\n",
                "        create_feature_summary()\n",
                "    ]\n",
                "\n",
                "    html_parts = [\n",
                "        \"<!DOCTYPE html>\",\n",
                "        \"<html>\",\n",
                "        \"<head>\",\n",
                "        \"<title>Model Report Card</title>\",\n",
                "        \"<script src='https://cdn.plot.ly/plotly-latest.min.js'></script>\",\n",
                "        \"<style>\",\n",
                "        \"body { margin: 20px; font-family: Arial, sans-serif; }\",\n",
                "        \".header { text-align: center; margin-bottom: 30px; }\",\n",
                "        \".plot { margin-bottom: 40px; }\",\n",
                "        \".metrics-summary { background: #f5f5f5; padding: 20px; border-radius: 5px; margin-bottom: 20px; }\",\n",
                "        \".section { margin-bottom: 40px; }\",\n",
                "        \"h2 { color: #2c3e50; }\",\n",
                "        \"</style>\",\n",
                "        \"</head>\",\n",
                "        \"<body>\",\n",
                "        \"<div class='header'>\",\n",
                "        \"<h1>Model Performance Report Card</h1>\",\n",
                "        f\"<p>Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\",\n",
                "        \"</div>\",\n",
                "        \"<div class='section'>\",\n",
                "        \"<h2>Model Overview</h2>\",\n",
                "        f\"<p>This report presents the performance metrics of the {model_output['model_type']} model across different tiers, \",\n",
                "        \"along with dataset statistics and detailed feature information.</p>\",\n",
                "        f\"<p>{get_model_parameters_description()}</p>\",\n",
                "        \"</div>\"\n",
                "    ]\n",
                "\n",
                "    for fig in figures:\n",
                "        html_parts.append(f\"<div class='plot'>{fig.to_html(full_html=False, include_plotlyjs=False)}</div>\")\n",
                "\n",
                "    html_parts.extend([\n",
                "        \"<div class='section'>\",\n",
                "        \"<h2>Metrics Interpretation</h2>\",\n",
                "        \"<ul>\",\n",
                "        \"<li><strong>MAE (Mean Absolute Error):</strong> Average absolute difference between predicted and actual values.</li>\",\n",
                "        \"<li><strong>WAPE (Weighted Absolute Percentage Error):</strong> Percentage error weighted by the magnitude of actual values.</li>\",\n",
                "        \"<li><strong>RMSE (Root Mean Square Error):</strong> Square root of the average squared differences, penalizing larger errors.</li>\",\n",
                "        \"<li><strong>Bias:</strong> Average difference between predicted and actual values, indicating systematic over/under-prediction.</li>\",\n",
                "        \"</ul>\",\n",
                "        \"</div>\",\n",
                "        \"<div class='section'>\",\n",
                "        \"<h2>Feature Information</h2>\",\n",
                "        \"<ul>\",\n",
                "        f\"<li><strong>Time Column ({time_column}):</strong> Used to order the time series data.</li>\",\n",
                "        f\"<li><strong>Target Column ({target_column}):</strong> The variable being predicted by the model.</li>\",\n",
                "        f\"<li><strong>Series Identifier ({series_identifier}):</strong> Used to distinguish between different time series within the dataset.</li>\",\n",
                "        \"</ul>\",\n",
                "        \"</div>\",\n",
                "        \"</body>\",\n",
                "        \"</html>\"\n",
                "    ])\n",
                "\n",
                "    logger.info(f\"Saving HTML report to {output_report.path}\")\n",
                "    with open(output_report.path, \"w\") as f:\n",
                "        f.write(\"\\n\".join(html_parts))\n",
                "\n",
                "    logger.info(\"Report card generation completed\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "3b87af2f",
            "metadata": {},
            "outputs": [],
            "source": [
                "#log_reports\n",
                "@component(\n",
                "    base_image='northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/b2b_ai/wf_pipeline/training:1.0.2-rc'\n",
                ")\n",
                "def log_reports(\n",
                "    data_report: Input[Artifact],\n",
                "    eval_report: Input[Artifact],\n",
                "    experiment_run: Input[Artifact]\n",
                "):\n",
                "    from google.cloud import aiplatform\n",
                "    import logging\n",
                "    import json\n",
                "    \n",
                "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
                "    logger = logging.getLogger(__name__)\n",
                "    \n",
                "    with open(experiment_run.path, 'r') as f:\n",
                "        run_info = json.load(f)\n",
                "        \n",
                "    logger.info(f\"Initializing Vertex AI SDK and continuing experiment run\")\n",
                "    aiplatform.init(project=run_info['project_id'], location=run_info['location'], experiment=run_info['experiment'])\n",
                "    run = aiplatform.start_run(run_info['run_name'], resume=True)\n",
                "    \n",
                "    run.log_params({\n",
                "        'dataset_report_uri': data_report.path,\n",
                "        'model_eval_uri': eval_report.path\n",
                "    })\n",
                "    \n",
                "    run.end_run()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e85e5cc5",
            "metadata": {},
            "source": [
                "# Build Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "ce3ba943",
            "metadata": {},
            "outputs": [],
            "source": [
                "@dsl.pipeline(\n",
                "    name=\"b2b-wf-short-term-prediction-experiments\",\n",
                "    description=\"A Kubeflow pipeline for training forecast models using AutoML Forecast on Vertex AI Pipelines from a BigQuery view.\"\n",
                ")\n",
                "def forecast_pipeline(\n",
                "    project_id: str,\n",
                "    project_location: str,\n",
                "    bq_dataset: str,\n",
                "    bq_source_table: str,\n",
                "    time_column: str,\n",
                "    target_column: str,\n",
                "    series_identifier: str,\n",
                "    window_size: int,\n",
                "    experiment_name: str,\n",
                "    forecast_horizon: int,\n",
                "    run_name: str,\n",
                "    attribute_columns: List[str]\n",
                "):\n",
                "    query_and_preprocess_task = query_and_preprocess(\n",
                "        project_id=project_id,\n",
                "        project_location=project_location,\n",
                "        bq_dataset=bq_dataset,\n",
                "        bq_source_table=bq_source_table,\n",
                "        time_column=time_column,\n",
                "        target_column=target_column,\n",
                "        series_identifier=series_identifier,\n",
                "        attribute_columns=attribute_columns\n",
                "    )\n",
                "    \n",
                "    generate_time_series_cv_task = generate_time_series_cv(\n",
                "        time_column=time_column,\n",
                "        input_dataset=query_and_preprocess_task.outputs['output_dataset'],\n",
                "        forecast_horizon=forecast_horizon\n",
                "    )\n",
                "    \n",
                "    generate_dataset_statistics_task = generate_dataset_statistics(\n",
                "        train_dataset=generate_time_series_cv_task.outputs['output_train'],\n",
                "        test_dataset=generate_time_series_cv_task.outputs['output_test'],\n",
                "        time_column=time_column,\n",
                "        target_column=target_column,\n",
                "        attribute_columns=attribute_columns\n",
                "    )\n",
                "    \n",
                "    generate_statistics_visualization_task = generate_statistics_visualization(\n",
                "        statistics_artifact=generate_dataset_statistics_task.outputs['output_statistics'],\n",
                "        train_dataset=generate_time_series_cv_task.outputs['output_train'],\n",
                "        test_dataset=generate_time_series_cv_task.outputs['output_test'],\n",
                "        attribute_columns=attribute_columns,\n",
                "        time_column=time_column,\n",
                "        target_column=target_column\n",
                "    )\n",
                "    \n",
                "    if MODEL == 'SMA':\n",
                "        model_trainer_task = sma_trainer_component_job(\n",
                "            project=project_id,\n",
                "            location=project_location,\n",
                "            dataset=generate_time_series_cv_task.outputs['output_train'],\n",
                "            experiment_name=experiment_name,\n",
                "            window_size=window_size,\n",
                "            project_id=project_id,\n",
                "            project_location=project_location,\n",
                "            time_column=time_column,\n",
                "            target_column=target_column,\n",
                "            forecast_horizon=forecast_horizon,\n",
                "            run_name=run_name,\n",
                "            series_identifier=series_identifier\n",
                "        )\n",
                "    elif MODEL == 'SARIMA':\n",
                "        model_trainer_task = sarima_trainer_component_job(\n",
                "            project=project_id,\n",
                "            location=project_location,\n",
                "            dataset=generate_time_series_cv_task.outputs['output_train'],\n",
                "            experiment_name=experiment_name,\n",
                "            project_id=project_id,\n",
                "            project_location=project_location,\n",
                "            time_column=time_column,\n",
                "            target_column=target_column,\n",
                "            series_identifier=series_identifier,\n",
                "            forecast_horizon=forecast_horizon,\n",
                "            run_name=run_name\n",
                "        )\n",
                "    \n",
                "    model_evaluator_task = model_evaluator_component(\n",
                "        test_dataset=generate_time_series_cv_task.outputs['output_test'],\n",
                "        trained_model=model_trainer_task.outputs['output_model'],\n",
                "        experiment_run=model_trainer_task.outputs['experiment_run'],\n",
                "        time_column=time_column,\n",
                "        target_column=target_column,\n",
                "        series_identifier=series_identifier\n",
                "    )\n",
                "    model_evaluator_task.set_cpu_limit('16')\n",
                "    \n",
                "    generate_model_report_card_task = generate_model_report_card(\n",
                "        evaluation_metrics=model_evaluator_task.outputs['output_metrics'],\n",
                "        statistics_artifact=generate_dataset_statistics_task.outputs['output_statistics'],\n",
                "        trained_model=model_trainer_task.outputs['output_model'],\n",
                "        time_column=time_column,\n",
                "        target_column=target_column,\n",
                "        series_identifier=series_identifier\n",
                "    )\n",
                "    \n",
                "    log_report_task = log_reports(\n",
                "        data_report=generate_statistics_visualization_task.outputs['output_visualization'],\n",
                "        eval_report=generate_model_report_card_task.outputs['output_report'],\n",
                "        experiment_run=model_trainer_task.outputs['experiment_run']\n",
                "    )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "67017e6a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Creating PipelineJob\n",
                        "PipelineJob created. Resource name: projects/7796273458/locations/northamerica-northeast1/pipelineJobs/b2b-wf-short-term-prediction-experiments-20250416182535\n",
                        "To use this PipelineJob in another session:\n",
                        "pipeline_job = aiplatform.PipelineJob.get('projects/7796273458/locations/northamerica-northeast1/pipelineJobs/b2b-wf-short-term-prediction-experiments-20250416182535')\n",
                        "View Pipeline Job:\n",
                        "https://console.cloud.google.com/vertex-ai/locations/northamerica-northeast1/pipelines/runs/b2b-wf-short-term-prediction-experiments-20250416182535?project=7796273458\n",
                        "PipelineJob run completed. Resource name: projects/7796273458/locations/northamerica-northeast1/pipelineJobs/b2b-wf-short-term-prediction-experiments-20250416182535\n"
                    ]
                }
            ],
            "source": [
                "aiplatform.init(\n",
                "    project=PROJECT_ID,\n",
                "    location=PROJECT_REGION,\n",
                "    staging_bucket=BUCKET_URI\n",
                ")\n",
                "\n",
                "compiler.Compiler().compile(\n",
                "    pipeline_func=forecast_pipeline,\n",
                "    package_path=PIPELINE_PACKAGE_PATH\n",
                ")\n",
                "\n",
                "job = pipeline_jobs.PipelineJob(\n",
                "    display_name=\"b2b_wf_short_term_prediction_sma_pipeline\",\n",
                "    template_path=PIPELINE_PACKAGE_PATH,\n",
                "    parameter_values={\n",
                "            'project_id': PROJECT_ID,\n",
                "            'project_location': PROJECT_REGION,\n",
                "            'bq_dataset': BQ_DATASET_NAME,\n",
                "            'bq_source_table': BQ_SOURCE_TABLE,\n",
                "            'time_column': \"Appointment_Day\",\n",
                "            'target_column': \"SWT\",\n",
                "            'series_identifier': \"Series_Identifier\",\n",
                "            'window_size': 183,\n",
                "            'run_name': RUN_NAME,\n",
                "            'forecast_horizon': FORECAST_HORIZON,\n",
                "            'attribute_columns': EXPERIMENT_FEATURES,\n",
                "            'experiment_name': EXPERIMENT_NAME\n",
                "        }\n",
                ")\n",
                "\n",
                "job.run()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cdc2b060",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.17"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
