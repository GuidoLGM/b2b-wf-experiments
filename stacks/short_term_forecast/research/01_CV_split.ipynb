{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e9ab5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/workspaces/b2b-wf-experiments/stacks/short_term_forecast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beb9857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class CrossValSplitConfig:\n",
    "    root_dir: Path\n",
    "    input_dataset: Path\n",
    "    time_column: str\n",
    "    forecast_horizon: int\n",
    "    train_file_name: str\n",
    "    test_file_name: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab59303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from src.ShortTermForecast.constants import *\n",
    "from src.ShortTermForecast.utils.common import read_yaml, create_directories\n",
    "#from src.ShortTermForecast.entity.config_entity import CrossValSplitConfig\n",
    "\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH\n",
    "    ):\n",
    "        \n",
    "        if config_filepath is not None:\n",
    "            self.config = read_yaml(config_filepath)\n",
    "            create_directories([self.config.artifacts_root])\n",
    "        \n",
    "        if params_filepath is not None:\n",
    "            self.params = read_yaml(params_filepath)\n",
    "\n",
    "\n",
    "    def get_cross_validation_split_dvc_config(self) -> CrossValSplitConfig:\n",
    "        \n",
    "        general_configs = self.config.general_setup\n",
    "        data_ingestion_config = self.config.data_ingestion\n",
    "        cross_validation_config = self.config.cross_validation\n",
    "\n",
    "        create_directories([cross_validation_config.root_dir])\n",
    "\n",
    "        return CrossValSplitConfig(\n",
    "            root_dir=Path(cross_validation_config.root_dir),\n",
    "            input_dataset=Path(data_ingestion_config.root_dir, data_ingestion_config.local_file_name),\n",
    "            time_column=general_configs.target_column,\n",
    "            forecast_horizon=general_configs.forecast_horizon,\n",
    "            train_file_name=cross_validation_config.train_file_name,\n",
    "            test_file_name=cross_validation_config.test_file_name\n",
    "        )\n",
    "    \n",
    "    def get_cross_validation_split_kfp_config(\n",
    "            self,\n",
    "            input_dataset: str,\n",
    "            time_column: str,\n",
    "            forecast_horizon: int\n",
    "    ) -> CrossValSplitConfig:\n",
    "\n",
    "        return CrossValSplitConfig(\n",
    "            root_dir=None,\n",
    "            input_dataset=input_dataset,\n",
    "            time_column=time_column,\n",
    "            forecast_horizon=forecast_horizon,\n",
    "            train_file_name=None,\n",
    "            test_file_name=None\n",
    "        )        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8610aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "from src.ShortTermForecast import logger\n",
    "#\\from src.ShortTermForecast.entity.config_entity import CrossValSplitConfig\n",
    "\n",
    "class TimeSeriesCV:\n",
    "    \"\"\"\n",
    "    A class for implementing time series cross-validation with rolling forecast windows.\n",
    "    \n",
    "    This class handles the creation of training and test datasets using a rolling window\n",
    "    cross-validation strategy for time series forecasting. It reads timestamp-based data\n",
    "    and creates splits with an additional 'split_index' column to identify different\n",
    "    temporal splits.\n",
    "    \n",
    "    Attributes:\n",
    "        config (CrossValSplitConfig): Configuration object containing necessary parameters.\n",
    "        data (pandas.DataFrame): DataFrame to store the input data.\n",
    "        splits (list): List of dictionaries containing split configurations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: CrossValSplitConfig) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the TimeSeriesCV class.\n",
    "\n",
    "        Args:\n",
    "            config (CrossValSplitConfig): Configuration object containing necessary parameters.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.data = None\n",
    "        self.splits = None\n",
    "        logger.info(\"TimeSeriesCV instance initialized with provided configuration.\")\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"\n",
    "        Load data from the input dataset specified in the configuration.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Reading input dataset from: {self.config.input_dataset}\")\n",
    "        try:\n",
    "            self.data = pd.read_csv(self.config.input_dataset)\n",
    "            # Ensure time column is properly converted to datetime\n",
    "            self.data[self.config.time_column] = pd.to_datetime(self.data[self.config.time_column])\n",
    "            logger.info(f\"Data loaded successfully. Shape: {self.data.shape}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading data: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def generate_splits(self):\n",
    "        \"\"\"\n",
    "        Generate time series cross-validation splits.\n",
    "        \n",
    "        This method creates a list of split configurations, where each split represents\n",
    "        a different forecasting period. All splits start training from 2022-01-01, with\n",
    "        training periods growing progressively longer and test periods moving forward in time.\n",
    "        \"\"\"\n",
    "        logger.info(\"Generating time series cross-validation splits\")\n",
    "        \n",
    "        train_start = pd.to_datetime(\"2022-01-01\")\n",
    "        splits = []\n",
    "        current_train_end = pd.to_datetime(\"2024-03-31\")\n",
    "        \n",
    "        for split_index in range(1, 5):\n",
    "            test_start = current_train_end + timedelta(days=1)\n",
    "            test_end = test_start + timedelta(days=self.config.forecast_horizon-1)\n",
    "            \n",
    "            splits.append({\n",
    "                \"split_index\": split_index,\n",
    "                \"train_start\": train_start,\n",
    "                \"train_end\": current_train_end,\n",
    "                \"test_start\": test_start,\n",
    "                \"test_end\": test_end\n",
    "            })\n",
    "            \n",
    "            current_train_end = test_end\n",
    "        \n",
    "        self.splits = splits\n",
    "        \n",
    "        logger.info(\"Generated splits:\")\n",
    "        for s in splits:\n",
    "            logger.info(f\"Split {s['split_index']}:\")\n",
    "            logger.info(f\"  Train: {s['train_start'].strftime('%Y-%m-%d')} to {s['train_end'].strftime('%Y-%m-%d')}\")\n",
    "            logger.info(f\"  Test:  {s['test_start'].strftime('%Y-%m-%d')} to {s['test_end'].strftime('%Y-%m-%d')}\")\n",
    "\n",
    "    def process_splits(self):\n",
    "        \"\"\"\n",
    "        Process the generated splits to create training and test datasets.\n",
    "        \n",
    "        This method applies the split configurations to the input data, creating\n",
    "        separate training and test datasets with a 'split_index' column to identify\n",
    "        different temporal splits.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (combined_train, combined_test) DataFrames containing all splits\n",
    "        \"\"\"\n",
    "        if self.splits is None:\n",
    "            logger.error(\"Splits have not been generated. Call generate_splits() first.\")\n",
    "            raise ValueError(\"Splits not generated\")\n",
    "            \n",
    "        all_train_data = []\n",
    "        all_test_data = []\n",
    "        \n",
    "        for s in self.splits:\n",
    "            logger.info(f\"\\nProcessing split {s['split_index']}\")\n",
    "            \n",
    "            train_mask = (self.data[self.config.time_column] >= s[\"train_start\"]) & \\\n",
    "                        (self.data[self.config.time_column] <= s[\"train_end\"])\n",
    "            \n",
    "            test_mask = (self.data[self.config.time_column] >= s[\"test_start\"]) & \\\n",
    "                       (self.data[self.config.time_column] <= s[\"test_end\"])\n",
    "\n",
    "            train_df = self.data.loc[train_mask].copy()\n",
    "            test_df = self.data.loc[test_mask].copy()\n",
    "            \n",
    "            train_df['split_index'] = s['split_index']\n",
    "            test_df['split_index'] = s['split_index']\n",
    "            \n",
    "            all_train_data.append(train_df)\n",
    "            all_test_data.append(test_df)\n",
    "            \n",
    "            logger.info(f\"Split {s['split_index']} - Train shape: {train_df.shape}, Test shape: {test_df.shape}\")\n",
    "        \n",
    "\n",
    "        self.train = pd.concat(all_train_data, ignore_index=True)\n",
    "        self.test = pd.concat(all_test_data, ignore_index=True)\n",
    "\n",
    "    def save(self, save_train_path: str = None, save_test_path: str = None):\n",
    "        \"\"\"\n",
    "        Save the processed training and test datasets to CSV files.\n",
    "\n",
    "        Args:\n",
    "            train_df (pd.DataFrame): Combined training dataset\n",
    "            test_df (pd.DataFrame): Combined test dataset\n",
    "        \"\"\"\n",
    "\n",
    "        if save_train_path is None:\n",
    "            save_train_path = Path(self.config.root_dir, self.config.train_file_name)\n",
    "        if save_test_path is None:\n",
    "            save_test_path = Path(self.config.root_dir, self.config.test_file_name)\n",
    "            \n",
    "        logger.info(f\"Saving combined training data (shape: {self.train.shape}) to {save_train_path}\")\n",
    "        self.train.to_csv(save_train_path, index=False)\n",
    "        \n",
    "        logger.info(f\"Saving combined test data (shape: {self.test.shape}) to {save_test_path}\")\n",
    "        self.test.to_csv(save_test_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3da30e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-15 19:02:08,961: INFO: 3118263232] >>>>>> stage Cross Validation Split started <<<<<<\n",
      "[2025-04-15 19:02:08,968: INFO: common] yaml file: config/config.yaml loaded successfully\n",
      "[2025-04-15 19:02:08,970: INFO: common] Creating directory: artifacts\n",
      "[2025-04-15 19:02:08,971: INFO: common] yaml file: params.yaml loaded successfully\n",
      "[2025-04-15 19:02:08,972: INFO: common] Creating directory: cross_validation\n",
      "[2025-04-15 19:02:08,972: INFO: 2254608862] TimeSeriesCV instance initialized with provided configuration.\n",
      "[2025-04-15 19:02:08,973: INFO: 2254608862] Reading input dataset from: artifacts/data_ingestion/fwds_daily_data.csv\n",
      "[2025-04-15 19:02:09,491: INFO: 2254608862] Data loaded successfully. Shape: (299313, 11)\n",
      "[2025-04-15 19:02:09,492: INFO: 2254608862] Generating time series cross-validation splits\n",
      "[2025-04-15 19:02:09,494: INFO: 2254608862] Generated splits:\n",
      "[2025-04-15 19:02:09,494: INFO: 2254608862] Split 1:\n",
      "[2025-04-15 19:02:09,495: INFO: 2254608862]   Train: 2022-01-01 to 2024-03-31\n",
      "[2025-04-15 19:02:09,497: INFO: 2254608862]   Test:  2024-04-01 to 2024-06-30\n",
      "[2025-04-15 19:02:09,498: INFO: 2254608862] Split 2:\n",
      "[2025-04-15 19:02:09,499: INFO: 2254608862]   Train: 2022-01-01 to 2024-06-30\n",
      "[2025-04-15 19:02:09,500: INFO: 2254608862]   Test:  2024-07-01 to 2024-09-29\n",
      "[2025-04-15 19:02:09,501: INFO: 2254608862] Split 3:\n",
      "[2025-04-15 19:02:09,502: INFO: 2254608862]   Train: 2022-01-01 to 2024-09-29\n",
      "[2025-04-15 19:02:09,503: INFO: 2254608862]   Test:  2024-09-30 to 2024-12-29\n",
      "[2025-04-15 19:02:09,504: INFO: 2254608862] Split 4:\n",
      "[2025-04-15 19:02:09,505: INFO: 2254608862]   Train: 2022-01-01 to 2024-12-29\n",
      "[2025-04-15 19:02:09,506: INFO: 2254608862]   Test:  2024-12-30 to 2025-03-30\n",
      "[2025-04-15 19:02:09,507: INFO: 2254608862] \n",
      "Processing split 1\n",
      "[2025-04-15 19:02:09,520: INFO: 2254608862] Split 1 - Train shape: (0, 12), Test shape: (0, 12)\n",
      "[2025-04-15 19:02:09,521: INFO: 2254608862] \n",
      "Processing split 2\n",
      "[2025-04-15 19:02:09,526: INFO: 2254608862] Split 2 - Train shape: (0, 12), Test shape: (0, 12)\n",
      "[2025-04-15 19:02:09,527: INFO: 2254608862] \n",
      "Processing split 3\n",
      "[2025-04-15 19:02:09,532: INFO: 2254608862] Split 3 - Train shape: (0, 12), Test shape: (0, 12)\n",
      "[2025-04-15 19:02:09,533: INFO: 2254608862] \n",
      "Processing split 4\n",
      "[2025-04-15 19:02:09,537: INFO: 2254608862] Split 4 - Train shape: (0, 12), Test shape: (0, 12)\n",
      "[2025-04-15 19:02:09,541: INFO: 2254608862] Saving combined training data (shape: (0, 12)) to cross_validation/cv_processed_train.csv\n",
      "[2025-04-15 19:02:09,546: INFO: 2254608862] Saving combined test data (shape: (0, 12)) to cross_validation/cv_processed_test.csv\n",
      "[2025-04-15 19:02:09,562: INFO: 3118263232] >>>>>> stage Cross Validation Split completed <<<<<<\n",
      "[2025-04-15 19:02:09,563: INFO: 3118263232] \n",
      "x==================================================x\n"
     ]
    }
   ],
   "source": [
    "STAGE_NAME = \"Cross Validation Split\"\n",
    "\n",
    "\n",
    "class TimeSeriesCVPipeline:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def main(self):\n",
    "        config = ConfigurationManager()\n",
    "        time_series_cv = TimeSeriesCV(config.get_cross_validation_split_dvc_config())\n",
    "        time_series_cv.load()\n",
    "        time_series_cv.generate_splits()\n",
    "        time_series_cv.process_splits()\n",
    "        time_series_cv.save()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\n",
    "        obj = TimeSeriesCVPipeline()\n",
    "        obj.main()\n",
    "        logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\")\n",
    "        logger.info(\"\\nx\" + \"=\" * 50 + \"x\")\n",
    "    except Exception as e:\n",
    "        logger.exception(e)\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdf33f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.dsl import component, Input, Output, Dataset\n",
    "from google_cloud_pipeline_components.v1.custom_job import create_custom_training_job_from_component\n",
    "\n",
    "@component(\n",
    "    base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/b2b_ai/wf_pipeline/training:1.0.1-rc\"\n",
    ")\n",
    "def generate_time_series_cv_op(\n",
    "    input_dataset: Input[Dataset],\n",
    "    time_column: str,\n",
    "    forecast_horizon: int,\n",
    "    output_train: Output[Dataset],\n",
    "    output_test: Output[Dataset]\n",
    "):\n",
    "    from src.ShortTermForecast.components.data_ingestion import TimeSeriesCV\n",
    "    from src.ShortTermForecast.config.configuration import ConfigurationManager\n",
    "\n",
    "    config = ConfigurationManager(\n",
    "        config_filepath=None, \n",
    "        params_filepath=None\n",
    "    )\n",
    "\n",
    "    time_series_cv_config = config.get_cross_validation_split_kfp_config(\n",
    "        input_dataset=input_dataset.path,\n",
    "        time_column=time_column,\n",
    "        forecast_horizon=forecast_horizon,\n",
    "    )\n",
    "\n",
    "    time_series_cv = TimeSeriesCV(time_series_cv_config)\n",
    "\n",
    "    time_series_cv.load()\n",
    "    time_series_cv.generate_splits()\n",
    "    time_series_cv.process_splits()\n",
    "    time_series_cv.save(\n",
    "        save_train_path=output_train.path,\n",
    "        save_test_path=output_test.path\n",
    "    )\n",
    "\n",
    "generate_time_series_cv_job = create_custom_training_job_from_component(\n",
    "    generate_time_series_cv_op,\n",
    "    display_name='generate-timeseries-cv-job',\n",
    "    machine_type='e2-standard-4'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feea205f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
