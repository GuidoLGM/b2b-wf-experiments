{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "PROJECT_ID      = \"\"\n",
    "PROJECT_REGION  = \"\"\n",
    "\n",
    "GCS_BUCKET_NAME = \"\"\n",
    "\n",
    "VERTEX_DATASET_NAME    = \"\"\n",
    "VERTEX_MODEL_NAME      = \"\"\n",
    "VERTEX_PREDICTION_NAME = \"\"\n",
    "\n",
    "BQ_DATASET_NAME  = \"\"\n",
    "BQ_TRAIN_TABLE   = \"\"\n",
    "BQ_PREDICT_TABLE = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"wb-ai-acltr-tbs-3-pr-a62583\"\n",
    "GCS_BUCKET_NAME = \"bkt_b2b_wf_prediction\"\n",
    "PROJECT_REGION = \"northamerica-northeast1\"\n",
    "\n",
    "VERTEX_DATASET_NAME = \"b2b_wf_prediction_panorama\"\n",
    "VERTEX_MODEL_NAME = \"b2b_wf_prediction_panorama\"\n",
    "VERTEX_PREDICTION_NAME = \"b2b_wf_prediction_batch\"\n",
    "\n",
    "BQ_DATASET_NAME = \"b2b_wf_prediction\"\n",
    "BQ_TRAIN_TABLE = \"vw_wf_experiment_historical\"\n",
    "BQ_PREDICT_TABLE = \"bq_wf_temp_predictions\"\n",
    "\n",
    "TRAIN_TEST_DATA_SPLIT = \"DATE('2024-07-01')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aiplatform\n",
    "from google.cloud import bigquery\n",
    "import datetime\n",
    "\n",
    "TRAINING_DATASET_BQ_PATH   = f\"bq://{PROJECT_ID}.{BQ_DATASET_NAME}.{BQ_TRAIN_TABLE}\"\n",
    "PREDICTION_DATASET_BQ_PATH = f\"bq://{PROJECT_ID}.{BQ_DATASET_NAME}.{BQ_PREDICT_TABLE}\"\n",
    "PREDICTION_OUTPUT_PREFIX   = f\"bq://{PROJECT_ID}.{BQ_DATASET_NAME}\"\n",
    "BUCKET_URI = f\"gs://{PROJECT_ID}_{GCS_BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(\n",
    "    project=PROJECT_ID, \n",
    "    staging_bucket=BUCKET_URI,\n",
    "    location=PROJECT_REGION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "client = bigquery.Client(\n",
    "    project=PROJECT_ID, \n",
    "    location=PROJECT_REGION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Experiment:\n",
    "    name: str\n",
    "    experiment_columns: list[str]\n",
    "    group_by_columns: list[str]\n",
    "    objective: str\n",
    "    forecast_horizon: int\n",
    "    context_window: int\n",
    "    data_granularity_unit: str\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_forecast_experiment = Experiment(\n",
    "    name=\"panorama_daily_forecast\",\n",
    "    experiment_columns=[\n",
    "        \"Appointment_Day\",\n",
    "        \"District\",\n",
    "        \"Region_Type\",\n",
    "        \"Product\",\n",
    "        \"Product_Grp\",\n",
    "        \"Technology\",\n",
    "        \"Work_Order_Action\",\n",
    "        \"Work_Order_Action_Grp\",\n",
    "        \"Work_Force\",\n",
    "        \"SWT\"\n",
    "    ],\n",
    "    group_by_columns=[],\n",
    "    objective=\"minimize-rmse\",\n",
    "    forecast_horizon=184,\n",
    "    context_window=552,\n",
    "    data_granularity_unit='day'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_experiment = daily_forecast_experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train data view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_series_identifier(columns):\n",
    "    coalesce_parts = [f\"COALESCE({column}, 'None')\" for column in columns]\n",
    "    separator = \"' '\"\n",
    "    return f\"CONCAT({f', {separator}, '.join(coalesce_parts)}) as Series_Identifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_columns = ','.join(running_experiment.experiment_columns)\n",
    "\n",
    "\n",
    "groupy_by = ''\n",
    "if len(running_experiment.group_by_columns):\n",
    "  groupy_by = \"GROUP BY \" + ','.join(running_experiment.group_by_columns)\n",
    "\n",
    "\n",
    "\n",
    "time_column                   = \"Appointment_Day\"\n",
    "time_series_identifier_column = \"Series_Identifier\"\n",
    "target_column                 = \"SWT\"\n",
    "\n",
    "ATTRIBUTE_COLUMNS = running_experiment.experiment_columns.copy()\n",
    "ATTRIBUTE_COLUMNS.remove(time_column)\n",
    "ATTRIBUTE_COLUMNS.remove(target_column)\n",
    "\n",
    "\n",
    "COLUMN_SPECS = {\n",
    "    time_column:             \"timestamp\",\n",
    "    target_column:           \"numeric\"\n",
    "}\n",
    "\n",
    "for category in ATTRIBUTE_COLUMNS:\n",
    "    COLUMN_SPECS[category] = \"categorical\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_data_cte = f\"\"\"\n",
    "WITH historical_table AS (\n",
    "  SELECT \n",
    "    {query_columns}\n",
    "  FROM `b2b_wf_prediction.vw_wf_historical`\n",
    "  WHERE Appointment_Day < {TRAIN_TEST_DATA_SPLIT}\n",
    "  {groupy_by}\n",
    ")\"\"\"\n",
    "\n",
    "\n",
    "experiment_train_data_query = f\"\"\"\n",
    "CREATE OR REPLACE VIEW `b2b_wf_prediction.{BQ_TRAIN_TABLE}` AS \n",
    "{experiment_data_cte}\n",
    "SELECT \n",
    "  {create_series_identifier(ATTRIBUTE_COLUMNS)},\n",
    "  {query_columns}\n",
    "FROM historical_table\n",
    "\"\"\"\n",
    "\n",
    "VERTEX_DATASET_NAME += f\"_{running_experiment.name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.table.RowIterator at 0x7fffa7d853f0>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.query_and_wait(experiment_train_data_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... using existent dataset ... \n"
     ]
    }
   ],
   "source": [
    "dataset_list = aiplatform.TimeSeriesDataset.list(\n",
    "    filter=f\"display_name={VERTEX_DATASET_NAME}\"\n",
    ")\n",
    "\n",
    "if len(dataset_list) == 0:\n",
    "    print(\"... creating new dataset ... \")\n",
    "    dataset = aiplatform.TimeSeriesDataset.create(\n",
    "        display_name=VERTEX_DATASET_NAME,\n",
    "        bq_source=[TRAINING_DATASET_BQ_PATH],\n",
    "    )\n",
    "else:\n",
    "    print(\"... using existent dataset ... \")\n",
    "    dataset = dataset_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... training a new model ... \n"
     ]
    }
   ],
   "source": [
    "model_list = aiplatform.Model.list(\n",
    "    filter=f\"display_name={VERTEX_MODEL_NAME}\"\n",
    ")\n",
    "\n",
    "if len(model_list) == 0:\n",
    "    print(\"... training a new model ... \")\n",
    "    parent_model = None\n",
    "else:\n",
    "    print(\"... using existent model ... \")\n",
    "    model = model_list[0]\n",
    "    print(model)\n",
    "    parent_model = model.resource_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job = aiplatform.AutoMLForecastingTrainingJob(\n",
    "    display_name=VERTEX_MODEL_NAME,\n",
    "    optimization_objective=running_experiment.objective,\n",
    "    column_specs=COLUMN_SPECS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = training_job.run(\n",
    "    dataset=dataset,\n",
    "    target_column=target_column,\n",
    "    time_column=time_column,\n",
    "    time_series_identifier_column=time_series_identifier_column,\n",
    "    available_at_forecast_columns=[time_column],\n",
    "    unavailable_at_forecast_columns=[target_column],\n",
    "    time_series_attribute_columns=ATTRIBUTE_COLUMNS,\n",
    "    forecast_horizon=running_experiment.forecast_horizon,\n",
    "    context_window=running_experiment.context_window,\n",
    "    data_granularity_unit=running_experiment.data_granularity_unit,\n",
    "    data_granularity_count=1,\n",
    "    weight_column=None,\n",
    "    budget_milli_node_hours=1000,\n",
    "    parent_model = parent_model,\n",
    "    model_display_name=VERTEX_MODEL_NAME,\n",
    "    is_default_version = True,\n",
    "    model_version_description = f\"Model generated on {datetime.date.today().isoformat()}\",\n",
    "    predefined_split_column_name=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create the temp table for the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_prediction_job = model.batch_predict(\n",
    "    job_display_name=VERTEX_PREDICTION_NAME,\n",
    "    bigquery_source=PREDICTION_DATASET_BQ_PATH,\n",
    "    instances_format=\"bigquery\",\n",
    "    bigquery_destination_prefix=PREDICTION_OUTPUT_PREFIX,\n",
    "    predictions_format=\"bigquery\",\n",
    "    generate_explanation=True,\n",
    "    sync=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Clean up the temp predictions table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "batch_table  = batch_prediction_job.output_info.bigquery_output_table\n",
    "\n",
    "# TODO: manually fill the fields based on the fields on the dataset table\n",
    "query_job = client.query(\n",
    "    f\"\"\"\n",
    "        QUERY\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "# TODO: delete the batch table\n",
    "\n",
    "query_job.result(timeout=3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: calculate the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: save the metrics on the bq_wf_evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
