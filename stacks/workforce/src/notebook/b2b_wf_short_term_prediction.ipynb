{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cb87466c",
            "metadata": {},
            "outputs": [],
            "source": [
                "from kfp.v2 import dsl\n",
                "import kfp.v2.compiler as compiler\n",
                "from kfp.v2.dsl import component, Input, Output, Artifact, Model, Dataset\n",
                "\n",
                "from google.cloud import aiplatform\n",
                "from google.cloud.aiplatform import pipeline_jobs\n",
                "\n",
                "from typing import Any, Dict, List"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8119b735",
            "metadata": {
                "tags": [
                    "parameters"
                ]
            },
            "outputs": [],
            "source": [
                "PROJECT_ID = ''\n",
                "GCS_BUCKET_NAME = ''\n",
                "PROJECT_REGION = ''\n",
                "\n",
                "VERTEX_DATASET_NAME = ''\n",
                "VERTEX_MODEL_NAME = ''\n",
                "VERTEX_PREDICTION_NAME = ''\n",
                "\n",
                "BQ_DATASET_NAME = ''\n",
                "BQ_SOURCE_TABLE = ''\n",
                "BQ_TRAIN_TABLE = ''\n",
                "BQ_PREDICT_TABLE = ''\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3ecde90e",
            "metadata": {},
            "outputs": [],
            "source": [
                "PROJECT_ID = 'wb-ai-acltr-tbs-3-pr-a62583'\n",
                "GCS_BUCKET_NAME = 'bkt_b2b_wf_prediction'\n",
                "PROJECT_REGION = 'northamerica-northeast1'\n",
                "\n",
                "VERTEX_DATASET_NAME = 'b2b_wf_short_term_prediction'\n",
                "VERTEX_MODEL_NAME = 'b2b_wf_short_term_model'\n",
                "\n",
                "BQ_DATASET_NAME = 'b2b_wf_prediction'\n",
                "BQ_SOURCE_TABLE = 'vw_wf_daily_historical'\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "892e04c3",
            "metadata": {},
            "outputs": [],
            "source": [
                "TRAINING_DATASET_BQ_PATH   = f\"bq://{PROJECT_ID}.{BQ_DATASET_NAME}.{BQ_TRAIN_TABLE}\"\n",
                "BUCKET_URI = f\"gs://{PROJECT_ID}_{GCS_BUCKET_NAME}\"\n",
                "PIPELINE_PACKAGE_PATH = 'short_term_pipeline.json'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "35912b62",
            "metadata": {},
            "outputs": [],
            "source": [
                "EXPERIMENT_NAME = \"sma-full-no-hype-tuning\"\n",
                "EXPERIMENT_FEATURES = [\n",
                "    \"District\",\n",
                "    \"Region_Type\",\n",
                "    \"Product\",\n",
                "    \"Product_Grp\",\n",
                "    \"Technology\",\n",
                "    \"Work_Order_Action\",\n",
                "    \"Work_Order_Action_Grp\",\n",
                "    \"Work_Force\"\n",
                "]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "00b7c341",
            "metadata": {},
            "outputs": [],
            "source": [
                "aiplatform.init(\n",
                "    project=PROJECT_ID,\n",
                "    location=PROJECT_REGION,\n",
                "    staging_bucket=BUCKET_URI)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d020b73c",
            "metadata": {},
            "outputs": [],
            "source": [
                "@component(\n",
                "    base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/kfp-preprocess-slim:2.0.1\"\n",
                ")\n",
                "def query_and_preprocess(\n",
                "    project_id: str,\n",
                "    project_location: str,\n",
                "    bq_dataset: str,\n",
                "    bq_source_table: str,\n",
                "    time_column: str,\n",
                "    target_column: str,\n",
                "    series_identifier: str,\n",
                "    attribute_columns: List[str],\n",
                "    output_dataset: Output[Dataset]\n",
                "):\n",
                "    \"\"\" \n",
                "    Queries BigQuery data, performs preprocessing, and exports to a CSV dataset for training.\n",
                "    The function aggregates data by time and attributes, creates a unique series identifier,\n",
                "    and handles categorical features appropriately.\n",
                "\n",
                "    Args:\n",
                "        project_id: GCP project ID\n",
                "        project_location: GCP project location/region\n",
                "        bq_dataset: BigQuery dataset name\n",
                "        bq_source_table: Source table name\n",
                "        attribute_columns: List of categorical columns to group by\n",
                "        output_dataset: Output path for the preprocessed CSV dataset\n",
                "\n",
                "    Returns:\n",
                "        Writes a preprocessed CSV file to the output_dataset path containing:\n",
                "        - Series_Identifier: Concatenated string of attribute values\n",
                "        - Appointment_Day: Timestamp column\n",
                "        - Attribute columns: Original categorical features\n",
                "        - SWT: Aggregated target variable\n",
                "    \"\"\"\n",
                "    \n",
                "    import datetime\n",
                "    import pandas as pd\n",
                "    from google.cloud import bigquery\n",
                "    \n",
                "\n",
                "    def create_series_identifier(columns, series_identifier):\n",
                "        coalesce_parts = [f\"COALESCE({column}, 'None')\" for column in columns]\n",
                "        separator = \"' '\"\n",
                "        return f\"CONCAT({f', {separator}, '.join(coalesce_parts)}) AS {series_identifier}\"\n",
                "    \n",
                "    time_column = \"Appointment_Day\"\n",
                "    target_column = \"SWT\"\n",
                "    \n",
                "    ATTRIBUTE_STRING = ','.join(attribute_columns)\n",
                "\n",
                "    COLUMN_SPECS = {\n",
                "        time_column:             \"timestamp\",\n",
                "        target_column:           \"numeric\"\n",
                "    }\n",
                "\n",
                "    for category in attribute_columns:\n",
                "        COLUMN_SPECS[category] = \"categorical\"\n",
                "        \n",
                "    experiment_train_data_query = f\"\"\"\n",
                "    WITH historical_table AS (\n",
                "        SELECT \n",
                "            {time_column},\n",
                "            {ATTRIBUTE_STRING},\n",
                "            SUM({target_column}) AS {target_column}\n",
                "        FROM `{project_id}.{bq_dataset}.{bq_source_table}`\n",
                "        WHERE {time_column} <= DATE('2025-03-31')\n",
                "        GROUP BY {time_column},{ATTRIBUTE_STRING}\n",
                "    )\n",
                "    SELECT \n",
                "        {create_series_identifier(attribute_columns, series_identifier)},\n",
                "        {time_column},\n",
                "        {ATTRIBUTE_STRING},\n",
                "        {target_column}\n",
                "    FROM historical_table\n",
                "    \"\"\"\n",
                "\n",
                "    client = bigquery.Client(\n",
                "        project=project_id,\n",
                "        location=project_location)\n",
                "\n",
                "    processed_data = client.query(experiment_train_data_query).to_dataframe()\n",
                "    \n",
                "    processed_data.to_csv(output_dataset.path, index=False)\n",
                "    print(f\"CSV file written to {output_dataset.path}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "62116c53",
            "metadata": {},
            "outputs": [],
            "source": [
                "@component(\n",
                "    base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/mlops/kfp-2.0.0/kfp-load-model-slim:1.0.0\"\n",
                ")\n",
                "def generate_time_series_cv(\n",
                "    input_dataset: Input[Dataset],\n",
                "    time_column: str,\n",
                "    output_train: Output[Dataset],\n",
                "    output_test: Output[Dataset]\n",
                "):\n",
                "    \"\"\"\n",
                "    Splits preprocessed time series data into rolling forecast datasets for training and evaluation.\n",
                "    \n",
                "    This component implements a rolling window cross-validation strategy for time series forecasting.\n",
                "    It reads a CSV dataset containing timestamp-based data and creates training and test sets with\n",
                "    an additional 'split_index' column to identify different temporal splits. Each split represents\n",
                "    a different forecasting period, with training data incrementally growing and test data moving\n",
                "    forward in time.\n",
                "    \n",
                "    Split Structure:\n",
                "        - All splits start training from 2022-01-01\n",
                "        - Training periods grow progressively longer\n",
                "        - Each test period is a 3-month window following its training period\n",
                "        - Data is labeled with split_index (1-4) to identify which split it belongs to\n",
                "    \n",
                "    Split Periods:\n",
                "        Split 1: Train (2022-01-01 to 2024-03-31), Test (2024-04-01 to 2024-06-30)\n",
                "        Split 2: Train (2022-01-01 to 2024-06-30), Test (2024-07-01 to 2024-09-30)\n",
                "        Split 3: Train (2022-01-01 to 2024-09-30), Test (2024-10-01 to 2024-12-31)\n",
                "        Split 4: Train (2022-01-01 to 2024-12-31), Test (2025-01-01 to 2025-03-31)\n",
                "    \n",
                "    Args:\n",
                "        input_dataset: Input[Dataset]\n",
                "            The preprocessed CSV dataset containing time series data\n",
                "        time_column: str\n",
                "            Name of the column containing timestamps\n",
                "        output_train: Output[Dataset]\n",
                "            Output path for the combined training dataset\n",
                "            Contains all training data with a 'split_index' column\n",
                "        output_test: Output[Dataset]\n",
                "            Output path for the combined test dataset\n",
                "            Contains all test data with a 'split_index' column\n",
                "    \n",
                "    Output Dataset Structure:\n",
                "        Both training and test datasets include all original columns plus:\n",
                "        - split_index: int (1-4)\n",
                "            Identifies which temporal split the row belongs to\n",
                "            Allows filtering/grouping data by split for analysis or modeling\n",
                "    \n",
                "    Note:\n",
                "        This implementation combines all splits into two files (train and test) with a\n",
                "        split_index column, rather than creating separate files for each split. This\n",
                "        approach simplifies data handling while maintaining the ability to analyze\n",
                "        individual splits through the split_index column.\n",
                "    \"\"\"\n",
                "    \n",
                "    import pandas as pd\n",
                "    import logging\n",
                "    from datetime import datetime\n",
                "    \n",
                "    logging.basicConfig(level=logging.INFO)\n",
                "    logger = logging.getLogger(__name__)\n",
                "    \n",
                "    logger.info(f\"Reading input dataset from: {input_dataset.path}\")\n",
                "    forecast_processed_data = pd.read_csv(input_dataset.path, parse_dates=[time_column])\n",
                "    \n",
                "    splits = [\n",
                "        {\n",
                "            \"split_index\": 1,\n",
                "            \"train_start\": \"2022-01-01\",\n",
                "            \"train_end\":   \"2024-03-31\",\n",
                "            \"test_start\":  \"2024-04-01\",\n",
                "            \"test_end\":    \"2024-06-30\"\n",
                "        },\n",
                "        {\n",
                "            \"split_index\": 2,\n",
                "            \"train_start\": \"2022-01-01\",\n",
                "            \"train_end\":   \"2024-06-30\",\n",
                "            \"test_start\":  \"2024-07-01\",\n",
                "            \"test_end\":    \"2024-09-30\"\n",
                "        },\n",
                "        {\n",
                "            \"split_index\": 3,\n",
                "            \"train_start\": \"2022-01-01\",\n",
                "            \"train_end\":   \"2024-09-30\",\n",
                "            \"test_start\":  \"2024-10-01\",\n",
                "            \"test_end\":    \"2024-12-31\"\n",
                "        },\n",
                "        {\n",
                "            \"split_index\": 4,\n",
                "            \"train_start\": \"2022-01-01\",\n",
                "            \"train_end\":   \"2024-12-31\",\n",
                "            \"test_start\":  \"2025-01-01\",\n",
                "            \"test_end\":    \"2025-03-31\"\n",
                "        },\n",
                "    ]\n",
                "    \n",
                "    for s in splits:\n",
                "        for key in ['train_start', 'train_end', 'test_start', 'test_end']:\n",
                "            s[key] = pd.to_datetime(s[key])\n",
                "    \n",
                "    all_train_data = []\n",
                "    all_test_data = []\n",
                "    \n",
                "    for s in splits:\n",
                "        logger.info(f\"Processing split {s['split_index']}\")\n",
                "        \n",
                "        train_mask = (forecast_processed_data[time_column] >= s[\"train_start\"]) & \\\n",
                "                     (forecast_processed_data[time_column] <= s[\"train_end\"])\n",
                "        \n",
                "        test_mask = (forecast_processed_data[time_column] >= s[\"test_start\"]) & \\\n",
                "                    (forecast_processed_data[time_column] <= s[\"test_end\"])\n",
                "\n",
                "        train_df = forecast_processed_data.loc[train_mask].copy()\n",
                "        test_df = forecast_processed_data.loc[test_mask].copy()\n",
                "        \n",
                "        train_df['split_index'] = s['split_index']\n",
                "        test_df['split_index'] = s['split_index']\n",
                "        \n",
                "        all_train_data.append(train_df)\n",
                "        all_test_data.append(test_df)\n",
                "        \n",
                "        logger.info(f\"Split {s['split_index']} - Train shape: {train_df.shape}, Test shape: {test_df.shape}\")\n",
                "    \n",
                "    combined_train = pd.concat(all_train_data, ignore_index=True)\n",
                "    combined_test = pd.concat(all_test_data, ignore_index=True)\n",
                "    \n",
                "    logger.info(f\"Saving combined training data (shape: {combined_train.shape}) to {output_train.path}\")\n",
                "    combined_train.to_csv(output_train.path, index=False)\n",
                "    \n",
                "    logger.info(f\"Saving combined test data (shape: {combined_test.shape}) to {output_test.path}\")\n",
                "    combined_test.to_csv(output_test.path, index=False)\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "551d1b17",
            "metadata": {},
            "outputs": [],
            "source": [
                "@component(\n",
                "    base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/mlops/kfp-2.0.0/kfp-load-model-slim:1.0.0\"\n",
                ")\n",
                "def generate_dataset_statistics(\n",
                "    train_dataset: Input[Dataset],\n",
                "    test_dataset: Input[Dataset],\n",
                "    time_column: str,\n",
                "    target_column: str,\n",
                "    attribute_columns: List[str],\n",
                "    output_statistics: Output[Artifact]\n",
                ") -> Dict[str, Dict[str, Dict[str, any]]]:\n",
                "    \"\"\"\n",
                "    Generates statistics for the training and test datasets produced by the generate_time_series_cv component.\n",
                "\n",
                "    This component analyzes the combined training and test datasets, which include a split_index column\n",
                "    identifying different temporal splits. It generates statistics for each split as well as overall statistics.\n",
                "\n",
                "    Args:\n",
                "        train_dataset: Input[Dataset]\n",
                "            Combined training dataset with split_index column\n",
                "        test_dataset: Input[Dataset]\n",
                "            Combined test dataset with split_index column\n",
                "        output_statistics: Output[Artifact]\n",
                "            Output artifact to store the generated statistics\n",
                "        time_column: str\n",
                "            Name of the timestamp column\n",
                "        target_column: str\n",
                "            Name of the target column (worked hours)\n",
                "        attribute_columns: List[str]\n",
                "            List of categorical columns (location, type of work, technology, product)\n",
                "\n",
                "    Returns:\n",
                "        Dict containing statistics for:\n",
                "        - Each training split (train_split_1 through train_split_4)\n",
                "        - Each test split (test_split_1 through test_split_4)\n",
                "        - Overall statistics combining all data\n",
                "    \"\"\"\n",
                "    import pandas as pd\n",
                "    import numpy as np\n",
                "    import json\n",
                "    import logging\n",
                "\n",
                "    logging.basicConfig(level=logging.INFO)\n",
                "    logger = logging.getLogger(__name__)\n",
                "\n",
                "    def calculate_statistics(df: pd.DataFrame, dataset_type: str, split_number: int) -> Dict[str, any]:\n",
                "        stats = {\n",
                "            \"dataset_type\": dataset_type,\n",
                "            \"split_number\": split_number,\n",
                "            \"total_rows\": len(df),\n",
                "            \"date_range\": {\n",
                "                \"start\": df[time_column].min().strftime(\"%Y-%m-%d\"),\n",
                "                \"end\": df[time_column].max().strftime(\"%Y-%m-%d\")\n",
                "            },\n",
                "            \"target_column\": {\n",
                "                \"mean\": float(df[target_column].mean()),\n",
                "                \"median\": float(df[target_column].median()),\n",
                "                \"min\": float(df[target_column].min()),\n",
                "                \"max\": float(df[target_column].max()),\n",
                "                \"std\": float(df[target_column].std()),\n",
                "                \"total\": float(df[target_column].sum())\n",
                "            },\n",
                "            \"null_counts\": df.isnull().sum().to_dict(),\n",
                "            \"categorical_columns\": {}\n",
                "        }\n",
                "\n",
                "        for col in attribute_columns:\n",
                "            value_counts = df[col].value_counts()\n",
                "            stats[\"categorical_columns\"][col] = {\n",
                "                \"unique_values\": int(df[col].nunique()),\n",
                "                \"top_5_values\": value_counts.nlargest(5).to_dict(),\n",
                "                \"null_count\": int(df[col].isnull().sum()),\n",
                "                \"total_count\": int(len(df)),\n",
                "                \"distribution_percentage\": value_counts.nlargest(5).apply(lambda x: float(x/len(df) * 100)).to_dict()\n",
                "            }\n",
                "\n",
                "        return stats\n",
                "\n",
                "    all_statistics = {}\n",
                "\n",
                "    # Read datasets\n",
                "    logger.info(\"Reading input datasets\")\n",
                "    train_df = pd.read_csv(train_dataset.path, parse_dates=[time_column])\n",
                "    test_df = pd.read_csv(test_dataset.path, parse_dates=[time_column])\n",
                "\n",
                "    # Process each split in training data\n",
                "    logger.info(\"Processing training splits\")\n",
                "    for split_index in range(1, 5):\n",
                "        split_train = train_df[train_df['split_index'] == split_index]\n",
                "        logger.info(f\"Processing training split {split_index} (shape: {split_train.shape})\")\n",
                "        all_statistics[f\"train_split_{split_index}\"] = calculate_statistics(split_train, \"train\", split_index)\n",
                "\n",
                "    # Process each split in test data\n",
                "    logger.info(\"Processing test splits\")\n",
                "    for split_index in range(1, 5):\n",
                "        split_test = test_df[test_df['split_index'] == split_index]\n",
                "        logger.info(f\"Processing test split {split_index} (shape: {split_test.shape})\")\n",
                "        all_statistics[f\"test_split_{split_index}\"] = calculate_statistics(split_test, \"test\", split_index)\n",
                "\n",
                "    # Calculate overall statistics\n",
                "    logger.info(\"Calculating overall statistics\")\n",
                "    all_data = pd.concat([train_df, test_df])\n",
                "    all_statistics[\"overall\"] = calculate_statistics(all_data, \"overall\", 0)\n",
                "\n",
                "    # Add summary statistics\n",
                "    all_statistics[\"summary\"] = {\n",
                "        \"total_rows\": {\n",
                "            \"train\": len(train_df),\n",
                "            \"test\": len(test_df),\n",
                "            \"total\": len(all_data)\n",
                "        },\n",
                "        \"date_range\": {\n",
                "            \"earliest\": all_data[time_column].min().strftime(\"%Y-%m-%d\"),\n",
                "            \"latest\": all_data[time_column].max().strftime(\"%Y-%m-%d\")\n",
                "        },\n",
                "        \"splits_info\": {\n",
                "            f\"split_{i}\": {\n",
                "                \"train_rows\": len(train_df[train_df['split_index'] == i]),\n",
                "                \"test_rows\": len(test_df[test_df['split_index'] == i])\n",
                "            } for i in range(1, 5)\n",
                "        }\n",
                "    }\n",
                "\n",
                "    # Save statistics to a JSON file\n",
                "    logger.info(f\"Saving statistics to {output_statistics.path}\")\n",
                "    with open(output_statistics.path, \"w\") as f:\n",
                "        json.dump(all_statistics, f, indent=2)\n",
                "\n",
                "    logger.info(\"Statistics generation completed\")\n",
                "    return all_statistics\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9f44cd1d",
            "metadata": {},
            "outputs": [],
            "source": [
                "@component(\n",
                "    base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/kfp-plot-slim:2.0.0\"\n",
                ")\n",
                "def generate_statistics_visualization(\n",
                "    statistics_artifact: Input[Artifact],\n",
                "    train_dataset: Input[Artifact],\n",
                "    test_dataset: Input[Artifact],\n",
                "    attribute_columns: List[str],\n",
                "    time_column: str,\n",
                "    target_column: str,\n",
                "    output_visualization: Output[Artifact]\n",
                "):\n",
                "    \"\"\"\n",
                "    Generates an interactive HTML report visualizing the statistics from the dataset analysis.\n",
                "    Focuses on train split 4 (which contains all historical data) and creates specific visualizations:\n",
                "    1. Categorical variables distribution\n",
                "    2. Time series visualization for the last split\n",
                "    3. Target column statistics card\n",
                "    \n",
                "    Args:\n",
                "        statistics_artifact: Input artifact containing the JSON statistics\n",
                "        train_dataset: Training dataset for additional visualizations\n",
                "        test_dataset: Test dataset for additional visualizations\n",
                "        output_visualization: Output artifact for the HTML report\n",
                "        time_column: Name of the timestamp column\n",
                "        target_column: Name of the target column (worked hours)\n",
                "    \"\"\"\n",
                "    import json\n",
                "    import pandas as pd\n",
                "    import numpy as np\n",
                "    import plotly.express as px\n",
                "    import plotly.graph_objects as go\n",
                "    from plotly.subplots import make_subplots\n",
                "    import logging\n",
                "    from datetime import datetime\n",
                "\n",
                "    logging.basicConfig(level=logging.INFO)\n",
                "    logger = logging.getLogger(__name__)\n",
                "\n",
                "    # Read statistics\n",
                "    logger.info(\"Loading statistics from JSON\")\n",
                "    with open(statistics_artifact.path, 'r') as f:\n",
                "        stats = json.load(f)\n",
                "\n",
                "    # Read datasets\n",
                "    logger.info(\"Loading datasets\")\n",
                "    \n",
                "    train_df = pd.read_csv(train_dataset.path)\n",
                "    train_df[time_column] = pd.to_datetime(train_df[time_column])\n",
                "    \n",
                "    test_df = pd.read_csv(test_dataset.path)\n",
                "    test_df[time_column] = pd.to_datetime(test_df[time_column])\n",
                "\n",
                "    # Filter for split 4\n",
                "    train_split4 = train_df[train_df['split_index'] == 4]\n",
                "    test_split4 = test_df[test_df['split_index'] == 4]\n",
                "    split4_stats = stats['train_split_4']\n",
                "\n",
                "    figures = []\n",
                "    \n",
                "    def create_categorical_distributions():\n",
                "        logger.info(\"Creating categorical distributions chart\")\n",
                "        \n",
                "        fig = make_subplots(\n",
                "            rows=4, cols=2,\n",
                "            subplot_titles=attribute_columns,\n",
                "            vertical_spacing=0.12,\n",
                "            horizontal_spacing=0.1\n",
                "        )\n",
                "        \n",
                "        for idx, col in enumerate(attribute_columns, 1):\n",
                "            row = ((idx-1) // 2) + 1\n",
                "            col_num = ((idx-1) % 2) + 1\n",
                "            \n",
                "            col_stats = split4_stats['categorical_columns'][col]\n",
                "            values = list(col_stats['distribution_percentage'].values())\n",
                "            labels = list(col_stats['distribution_percentage'].keys())\n",
                "            \n",
                "            fig.add_trace(\n",
                "                go.Bar(\n",
                "                    x=labels,\n",
                "                    y=values,\n",
                "                    name=col,\n",
                "                    text=[f'{v:.1f}%' for v in values],\n",
                "                    textposition='auto',\n",
                "                    showlegend=False\n",
                "                ),\n",
                "                row=row, col=col_num\n",
                "            )\n",
                "            \n",
                "            fig.update_xaxes(tickangle=45, row=row, col=col_num)\n",
                "            fig.update_yaxes(title='Percentage (%)', row=row, col=col_num)\n",
                "\n",
                "        fig.update_layout(\n",
                "            height=1200,\n",
                "            title_text=\"Categorical Variables Distribution (Train Split 4)\",\n",
                "            showlegend=False\n",
                "        )\n",
                "        return fig\n",
                "\n",
                "    def create_target_stats_card():\n",
                "        logger.info(\"Creating target statistics card\")\n",
                "        target_stats = split4_stats['target_column']\n",
                "        \n",
                "        # Create subplot with 1 row and 2 columns\n",
                "        fig = make_subplots(\n",
                "            rows=1, cols=2,\n",
                "            column_widths=[0.5, 0.5],\n",
                "            specs=[[{\"type\": \"table\"}, {\"type\": \"violin\"}]],\n",
                "            horizontal_spacing=0.05\n",
                "        )\n",
                "        \n",
                "        # Add table\n",
                "        fig.add_trace(\n",
                "            go.Table(\n",
                "                header=dict(\n",
                "                    values=['Metric', 'Value'],\n",
                "                    fill_color='lightgrey',\n",
                "                    align='left',\n",
                "                    font=dict(size=14)\n",
                "                ),\n",
                "                cells=dict(\n",
                "                    values=[\n",
                "                        ['Mean', 'Standard Deviation', 'Minimum', 'Maximum'],\n",
                "                        [\n",
                "                            f\"{target_stats['mean']:,.2f}\",\n",
                "                            f\"{target_stats['std']:,.2f}\",\n",
                "                            f\"{target_stats['min']:,.2f}\",\n",
                "                            f\"{target_stats['max']:,.2f}\",\n",
                "                        ]\n",
                "                    ],\n",
                "                    align='left',\n",
                "                    font=dict(size=13)\n",
                "                )\n",
                "            ),\n",
                "            row=1, col=1\n",
                "        )\n",
                "        \n",
                "        # Compute log(SWT + 1)\n",
                "        log_swt = np.log1p(train_split4[target_column])\n",
                "        \n",
                "        # Add violin plot (density plot) with log(SWT)\n",
                "        fig.add_trace(\n",
                "            go.Violin(\n",
                "                y=log_swt,\n",
                "                name=f\"log({target_column} + 1)\",\n",
                "                box_visible=False,\n",
                "                meanline_visible=True,\n",
                "                points=False,\n",
                "                fillcolor='lightblue',\n",
                "                line_color='blue',\n",
                "                showlegend=False,\n",
                "                hovertemplate=\"log(%{y:.2f})<extra></extra>\"\n",
                "            ),\n",
                "            row=1, col=2\n",
                "        )\n",
                "        \n",
                "        # Update layout\n",
                "        fig.update_layout(\n",
                "            title=f\"Target Column Statistics - log({target_column} + 1) (Train Split 4)\",\n",
                "            height=400,\n",
                "            showlegend=False,\n",
                "            yaxis=dict(\n",
                "                title=f\"log({target_column} + 1)\",\n",
                "                tickformat=\".2f\",\n",
                "                side='right'\n",
                "            )\n",
                "        )\n",
                "        \n",
                "        return fig\n",
                "\n",
                "\n",
                "\n",
                "    # Generate figures\n",
                "    figures.extend([\n",
                "        create_target_stats_card(),\n",
                "        create_categorical_distributions()\n",
                "    ])\n",
                "\n",
                "    # Create HTML content\n",
                "    html_parts = [\n",
                "        \"<!DOCTYPE html>\",\n",
                "        \"<html>\",\n",
                "        \"<head>\",\n",
                "        \"<title>Workforce Time Series Analysis Report - Split 4</title>\",\n",
                "        \"<script src='https://cdn.plot.ly/plotly-latest.min.js'></script>\",\n",
                "        \"<style>\",\n",
                "        \"body { margin: 20px; }\",\n",
                "        \".header { text-align: center; margin-bottom: 30px; }\",\n",
                "        \".plot { margin-bottom: 40px; }\",\n",
                "        \".stats-summary { background: #f5f5f5; padding: 20px; border-radius: 5px; margin-bottom: 20px; }\",\n",
                "        \"</style>\",\n",
                "        \"</head>\",\n",
                "        \"<body>\",\n",
                "        \"<div class='header'>\",\n",
                "        \"<h1>Workforce Time Series Analysis Report - Split 4</h1>\",\n",
                "        f\"<p>Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\",\n",
                "        \"</div>\",\n",
                "        \"<div class='stats-summary'>\",\n",
                "        \"<h2>Dataset Overview</h2>\",\n",
                "        f\"<p>Total Records: {split4_stats['total_rows']:,}</p>\",\n",
                "        f\"<p>Date Range: {split4_stats['date_range']['start']} to {split4_stats['date_range']['end']}</p>\",\n",
                "        \"</div>\"\n",
                "    ]\n",
                "\n",
                "    # Add figures\n",
                "    for fig in figures:\n",
                "        html_parts.append(f\"<div class='plot'>{fig.to_html(full_html=False, include_plotlyjs=False)}</div>\")\n",
                "\n",
                "    # Close HTML\n",
                "    html_parts.extend([\n",
                "        \"</body>\",\n",
                "        \"</html>\"\n",
                "    ])\n",
                "\n",
                "    # Save HTML report\n",
                "    logger.info(f\"Saving HTML report to {output_visualization.path}\")\n",
                "    with open(output_visualization.path, \"w\") as f:\n",
                "        f.write(\"\\n\".join(html_parts))\n",
                "\n",
                "    logger.info(\"Visualization generation completed\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8e2079f9",
            "metadata": {},
            "outputs": [],
            "source": [
                "@component(\n",
                "    base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/components/kfp-2.0.0/kfp-pycaret-slim:1.1.3\"\n",
                ")\n",
                "def sma_trainer_component(\n",
                "    dataset: Input[Dataset],\n",
                "    experiment_name: str,\n",
                "    window_size: int,\n",
                "    project_id: str,\n",
                "    project_location: str,\n",
                "    time_column: str,\n",
                "    target_column: str,\n",
                "    series_identifier: str,\n",
                "    output_model: Output[Model],\n",
                "):\n",
                "    import pickle\n",
                "    from google.cloud import aiplatform\n",
                "    import pandas as pd\n",
                "    from datetime import datetime\n",
                "    from tqdm.auto import tqdm\n",
                "    import logging\n",
                "    \n",
                "    # Configure logging\n",
                "    logging.basicConfig(\n",
                "        level=logging.INFO,\n",
                "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
                "    )\n",
                "    logger = logging.getLogger(__name__)\n",
                "    \n",
                "    # Custom tqdm class that logs to our logger\n",
                "    class TqdmToLogger(object):\n",
                "        def __init__(self, desc=None, total=None, logger=None):\n",
                "            self.logger = logger or logging.getLogger(__name__)\n",
                "            self.desc = desc\n",
                "            self.total = total\n",
                "            self.current = 0\n",
                "            \n",
                "        def update(self, n=1):\n",
                "            self.current += n\n",
                "            if self.desc:\n",
                "                self.logger.info(f'{self.desc}: {self.current}/{self.total}')\n",
                "            else:\n",
                "                self.logger.info(f'Progress: {self.current}/{self.total}')\n",
                "    \n",
                "    logger.info(f\"Starting SMA trainer component with window size: {window_size}\")\n",
                "    \n",
                "    # Initialize Vertex AI SDK\n",
                "    logger.info(f\"Initializing Vertex AI SDK for project: {project_id}, location: {project_location}\")\n",
                "    aiplatform.init(project=project_id, location=project_location, experiment=experiment_name)\n",
                "\n",
                "    logger.info(f\"Reading dataset from: {dataset.path}\")\n",
                "    df = pd.read_csv(dataset.path)\n",
                "    logger.info(f\"Dataset shape: {df.shape}\")\n",
                "\n",
                "    models = {}\n",
                "    timestamp = datetime.now()\n",
                "    timestamp = f'{timestamp.year}-{timestamp.month}-{timestamp.day}-{timestamp.hour}{timestamp.minute}'\n",
                "    run = aiplatform.start_run(\"sma-training-\" + timestamp)\n",
                "\n",
                "    unique_splits = sorted(df[\"split_index\"].unique())\n",
                "    total_combinations = sum(len(df[df[\"split_index\"] == split][series_identifier].unique()) \n",
                "                           for split in unique_splits)\n",
                "    \n",
                "    logger.info(f\"Processing {total_combinations} series across {len(unique_splits)} splits\")\n",
                "    \n",
                "    # Main progress tracking\n",
                "    overall_progress = TqdmToLogger(desc=\"Overall Progress\", total=total_combinations, logger=logger)\n",
                "    \n",
                "    for split in unique_splits:\n",
                "        split_df = df[df[\"split_index\"] == split]\n",
                "        unique_series = split_df[series_identifier].unique()\n",
                "        \n",
                "        logger.info(f\"Processing Split {split} ({len(unique_series)} series)\")\n",
                "        \n",
                "        for series_id in unique_series:\n",
                "            series_df = split_df[split_df[series_identifier] == series_id].sort_values(time_column)\n",
                "            sma_series = series_df[target_column].rolling(window=window_size).mean()\n",
                "            models[(series_id, split)] = sma_series.dropna().tolist()\n",
                "            overall_progress.update(1)\n",
                "    \n",
                "    logger.info(f\"Total models created: {len(models)}\")\n",
                "    logger.info(f\"Saving models to: {output_model.path}\")\n",
                "    \n",
                "    with open(output_model.path, \"wb\") as f:\n",
                "        pickle.dump(models, f)\n",
                "\n",
                "    logger.info(\"Logging to Vertex AI\")\n",
                "    run.log_params({\"window_size\": window_size, \"model_uri\": output_model.path})\n",
                "    run.log_metrics({\"total_models\": len(models)})\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b4b6ef1c",
            "metadata": {},
            "outputs": [],
            "source": [
                "#TODO: \n",
                "# implement the evaluation step\n",
                "# generate the model's card"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ce3ba943",
            "metadata": {},
            "outputs": [],
            "source": [
                "@dsl.pipeline(\n",
                "    name=\"b2b-wf-short-term-prediction-experiments\",\n",
                "    description=\"A Kubeflow pipeline for training forecast models using AutoML Forecast on Vertex AI Pipelines from a BigQuery view.\"\n",
                ")\n",
                "def forecast_pipeline(\n",
                "    project_id: str,\n",
                "    project_location: str,\n",
                "    bq_dataset: str,\n",
                "    bq_source_table: str,\n",
                "    time_column: str,\n",
                "    target_column: str,\n",
                "    series_identifier: str,\n",
                "    window_size: int,\n",
                "    experiment_name: str,\n",
                "    attribute_columns: List[str]\n",
                "):\n",
                "    query_and_preprocess_task = query_and_preprocess(\n",
                "        project_id=project_id,\n",
                "        project_location=project_location,\n",
                "        bq_dataset=bq_dataset,\n",
                "        bq_source_table=bq_source_table,\n",
                "        time_column=time_column,\n",
                "        target_column=target_column,\n",
                "        series_identifier=series_identifier,\n",
                "        attribute_columns=attribute_columns\n",
                "    )\n",
                "    \n",
                "    generate_time_series_cv_task = generate_time_series_cv(\n",
                "        time_column=time_column,\n",
                "        input_dataset=query_and_preprocess_task.outputs['output_dataset']\n",
                "    )\n",
                "    \n",
                "    generate_dataset_statistics_task = generate_dataset_statistics(\n",
                "        train_dataset=generate_time_series_cv_task.outputs['output_train'],\n",
                "        test_dataset=generate_time_series_cv_task.outputs['output_test'],\n",
                "        time_column=time_column,\n",
                "        target_column=target_column,\n",
                "        attribute_columns=attribute_columns\n",
                "    )\n",
                "    \n",
                "    generate_statistics_visualization_task = generate_statistics_visualization(\n",
                "        statistics_artifact=generate_dataset_statistics_task.outputs['output_statistics'],\n",
                "        train_dataset=generate_time_series_cv_task.outputs['output_train'],\n",
                "        test_dataset=generate_time_series_cv_task.outputs['output_test'],\n",
                "        attribute_columns=attribute_columns,\n",
                "        time_column=time_column,\n",
                "        target_column=target_column\n",
                "    )\n",
                "    \n",
                "    sma_trainer_component_task = sma_trainer_component(\n",
                "        dataset=generate_time_series_cv_task.outputs['output_train'],\n",
                "        experiment_name=experiment_name,\n",
                "        window_size=window_size,\n",
                "        project_id=project_id,\n",
                "        project_location=project_location,\n",
                "        time_column=time_column,\n",
                "        target_column=target_column,\n",
                "        series_identifier=series_identifier\n",
                "    )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "67017e6a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "PipelineJob created. Resource name: projects/7796273458/locations/northamerica-northeast1/pipelineJobs/b2b-wf-short-term-prediction-experiments-20250409220140\n",
                        "To use this PipelineJob in another session:\n",
                        "pipeline_job = aiplatform.PipelineJob.get('projects/7796273458/locations/northamerica-northeast1/pipelineJobs/b2b-wf-short-term-prediction-experiments-20250409220140')\n",
                        "View Pipeline Job:\n",
                        "https://console.cloud.google.com/vertex-ai/locations/northamerica-northeast1/pipelines/runs/b2b-wf-short-term-prediction-experiments-20250409220140?project=7796273458\n"
                    ]
                }
            ],
            "source": [
                "compiler.Compiler().compile(\n",
                "    pipeline_func=forecast_pipeline,\n",
                "    package_path=PIPELINE_PACKAGE_PATH\n",
                ")\n",
                "\n",
                "job = pipeline_jobs.PipelineJob(\n",
                "    display_name=\"b2b_wf_short_term_prediction\",\n",
                "    template_path=PIPELINE_PACKAGE_PATH,\n",
                "    parameter_values={\n",
                "        'project_id': PROJECT_ID,\n",
                "        'project_location': PROJECT_REGION,\n",
                "        'bq_dataset': BQ_DATASET_NAME,\n",
                "        'bq_source_table': BQ_SOURCE_TABLE,\n",
                "        'time_column': \"Appointment_Day\",\n",
                "        'target_column': \"SWT\",\n",
                "        'series_identifier': \"Series_Identifier\",\n",
                "        'window_size': 183,\n",
                "        'attribute_columns': EXPERIMENT_FEATURES,\n",
                "        'experiment_name': EXPERIMENT_NAME\n",
                "    }\n",
                ")\n",
                "\n",
                "job.run()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4997b41c",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
