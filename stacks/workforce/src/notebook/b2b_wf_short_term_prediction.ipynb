{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "cb87466c",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/tmp/ipykernel_48530/3110292531.py:1: DeprecationWarning: The module `kfp.v2` is deprecated and will be removed in a futureversion. Please import directly from the `kfp` namespace, instead of `kfp.v2`.\n",
                        "  from kfp.v2 import dsl\n"
                    ]
                }
            ],
            "source": [
                "from kfp.v2 import dsl\n",
                "import kfp.v2.compiler as compiler\n",
                "from kfp.v2.dsl import component, Input, Output, Artifact, Model, Dataset\n",
                "\n",
                "from google.cloud import aiplatform\n",
                "from google.cloud.aiplatform import pipeline_jobs\n",
                "\n",
                "from typing import List, NamedTuple"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "8119b735",
            "metadata": {
                "tags": [
                    "parameters"
                ]
            },
            "outputs": [],
            "source": [
                "PROJECT_ID = ''\n",
                "GCS_BUCKET_NAME = ''\n",
                "PROJECT_REGION = ''\n",
                "\n",
                "VERTEX_DATASET_NAME = ''\n",
                "VERTEX_MODEL_NAME = ''\n",
                "VERTEX_PREDICTION_NAME = ''\n",
                "\n",
                "BQ_DATASET_NAME = ''\n",
                "BQ_SOURCE_TABLE = ''\n",
                "BQ_TRAIN_TABLE = ''\n",
                "BQ_PREDICT_TABLE = ''\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "3ecde90e",
            "metadata": {},
            "outputs": [],
            "source": [
                "PROJECT_ID = 'wb-ai-acltr-tbs-3-pr-a62583'\n",
                "GCS_BUCKET_NAME = 'bkt_b2b_wf_prediction'\n",
                "PROJECT_REGION = 'northamerica-northeast1'\n",
                "\n",
                "VERTEX_DATASET_NAME = 'b2b_wf_short_term_prediction'\n",
                "VERTEX_MODEL_NAME = 'b2b_wf_short_term_model'\n",
                "\n",
                "BQ_DATASET_NAME = 'b2b_wf_prediction'\n",
                "BQ_SOURCE_TABLE = 'vw_wf_daily_historical'\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "892e04c3",
            "metadata": {},
            "outputs": [],
            "source": [
                "TRAINING_DATASET_BQ_PATH   = f\"bq://{PROJECT_ID}.{BQ_DATASET_NAME}.{BQ_TRAIN_TABLE}\"\n",
                "BUCKET_URI = f\"gs://{PROJECT_ID}_{GCS_BUCKET_NAME}\"\n",
                "PIPELINE_PACKAGE_PATH = 'short_term_pipeline.json'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "35912b62",
            "metadata": {},
            "outputs": [],
            "source": [
                "EXPERIMENT_FEATURES = [\n",
                "    \"District\",\n",
                "    \"Region_Type\",\n",
                "    \"Product\",\n",
                "    \"Product_Grp\",\n",
                "    \"Technology\",\n",
                "    \"Work_Order_Action\",\n",
                "    \"Work_Order_Action_Grp\",\n",
                "    \"Work_Force\"\n",
                "]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "00b7c341",
            "metadata": {},
            "outputs": [],
            "source": [
                "aiplatform.init(\n",
                "    project=PROJECT_ID,\n",
                "    location=PROJECT_REGION,\n",
                "    staging_bucket=BUCKET_URI)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "d020b73c",
            "metadata": {},
            "outputs": [],
            "source": [
                "@component(\n",
                "    base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/kfp-preprocess-slim:2.0.1\"\n",
                ")\n",
                "def query_and_preprocess(\n",
                "    project_id: str,\n",
                "    project_location: str,\n",
                "    bq_dataset: str,\n",
                "    bq_source_table: str,\n",
                "    time_column: str,\n",
                "    target_column: str,\n",
                "    attribute_columns: List[str],\n",
                "    output_dataset: Output[Dataset]\n",
                "):\n",
                "    \"\"\" \n",
                "    Queries BigQuery data, performs preprocessing, and exports to a CSV dataset for training.\n",
                "    The function aggregates data by time and attributes, creates a unique series identifier,\n",
                "    and handles categorical features appropriately.\n",
                "\n",
                "    Args:\n",
                "        project_id: GCP project ID\n",
                "        project_location: GCP project location/region\n",
                "        bq_dataset: BigQuery dataset name\n",
                "        bq_source_table: Source table name\n",
                "        attribute_columns: List of categorical columns to group by\n",
                "        output_dataset: Output path for the preprocessed CSV dataset\n",
                "\n",
                "    Returns:\n",
                "        Writes a preprocessed CSV file to the output_dataset path containing:\n",
                "        - Series_Identifier: Concatenated string of attribute values\n",
                "        - Appointment_Day: Timestamp column\n",
                "        - Attribute columns: Original categorical features\n",
                "        - SWT: Aggregated target variable\n",
                "    \"\"\"\n",
                "    \n",
                "    import datetime\n",
                "    import pandas as pd\n",
                "    from google.cloud import bigquery\n",
                "    from google.cloud import aiplatform\n",
                "    \n",
                "\n",
                "    def create_series_identifier(columns):\n",
                "        coalesce_parts = [f\"COALESCE({column}, 'None')\" for column in columns]\n",
                "        separator = \"' '\"\n",
                "        return f\"CONCAT({f', {separator}, '.join(coalesce_parts)}) AS Series_Identifier\"\n",
                "    \n",
                "    time_column = \"Appointment_Day\"\n",
                "    target_column = \"SWT\"\n",
                "    \n",
                "    FORECAST_TIMESTAMP = datetime.datetime.now()\n",
                "    ATTRIBUTE_STRING = ','.join(attribute_columns)\n",
                "\n",
                "    COLUMN_SPECS = {\n",
                "        time_column:             \"timestamp\",\n",
                "        target_column:           \"numeric\"\n",
                "    }\n",
                "\n",
                "    for category in attribute_columns:\n",
                "        COLUMN_SPECS[category] = \"categorical\"\n",
                "        \n",
                "    experiment_train_data_query = f\"\"\"\n",
                "    WITH historical_table AS (\n",
                "        SELECT \n",
                "            {time_column},\n",
                "            {ATTRIBUTE_STRING},\n",
                "            SUM({target_column}) AS {target_column}\n",
                "        FROM `{project_id}.{bq_dataset}.{bq_source_table}`\n",
                "        WHERE {time_column} <= DATE('2025-03-31')\n",
                "        GROUP BY {time_column},{ATTRIBUTE_STRING}\n",
                "    )\n",
                "    SELECT \n",
                "        {create_series_identifier(attribute_columns)},\n",
                "        {time_column},\n",
                "        {ATTRIBUTE_STRING},\n",
                "        {target_column}\n",
                "    FROM historical_table\n",
                "    \"\"\"\n",
                "\n",
                "    client = bigquery.Client(\n",
                "        project=project_id,\n",
                "        location=project_location)\n",
                "\n",
                "    processed_data = client.query(experiment_train_data_query).to_dataframe()\n",
                "    \n",
                "    processed_data.to_csv(output_dataset.path, index=False)\n",
                "    print(f\"CSV file written to {output_dataset.path}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "62116c53",
            "metadata": {},
            "outputs": [],
            "source": [
                "@component(\n",
                "    base_image=\"northamerica-northeast1-docker.pkg.dev/cio-workbench-image-np-0ddefe/bi-platform/bi-aaaie/images/mlops/kfp-2.0.0/kfp-load-model-slim:1.0.0\"\n",
                ")\n",
                "def split_rolling_forecast(\n",
                "    input_dataset: Input[Dataset],\n",
                "    time_column: str,\n",
                "    output_train1: Output[Dataset],\n",
                "    output_test1: Output[Dataset],\n",
                "    output_train2: Output[Dataset],\n",
                "    output_test2: Output[Dataset],\n",
                "    output_train3: Output[Dataset],\n",
                "    output_test3: Output[Dataset],\n",
                "    output_train4: Output[Dataset],\n",
                "    output_test4: Output[Dataset],\n",
                "):\n",
                "    \"\"\"\n",
                "    Splits the preprocessed data into rolling forecast datasets for model training.\n",
                "\n",
                "    Args:\n",
                "        input_dataset: Input path for the preprocessed CSV dataset\n",
                "        time_column: Name of the timestamp column\n",
                "    \"\"\"\n",
                "    \n",
                "    import pandas as pd\n",
                "    \n",
                "    forecast_processed_data = pd.read_csv(input_dataset.path, index_col=False)\n",
                "    forecast_processed_data[time_column] = pd.to_datetime(forecast_processed_data[time_column])\n",
                "    \n",
                "    splits = [\n",
                "        {\n",
                "            \"train_start\": \"2022-01-01\",\n",
                "            \"train_end\":   \"2024-03-31\",\n",
                "            \"test_start\":  \"2024-04-01\",\n",
                "            \"test_end\":    \"2024-06-30\"\n",
                "        },\n",
                "        {\n",
                "            \"train_start\": \"2022-01-01\",\n",
                "            \"train_end\":   \"2024-06-30\",\n",
                "            \"test_start\":  \"2024-07-01\",\n",
                "            \"test_end\":    \"2024-09-30\"\n",
                "        },\n",
                "        {\n",
                "            \"train_start\": \"2022-01-01\",\n",
                "            \"train_end\":   \"2024-09-30\",\n",
                "            \"test_start\":  \"2024-10-01\",\n",
                "            \"test_end\":    \"2024-12-31\"\n",
                "        },\n",
                "        {\n",
                "            \"train_start\": \"2022-01-01\",\n",
                "            \"train_end\":   \"2024-12-31\",\n",
                "            \"test_start\":  \"2025-01-01\",\n",
                "            \"test_end\":    \"2025-03-31\"\n",
                "        },\n",
                "    ]\n",
                "    \n",
                "    for s in splits:\n",
                "        s[\"train_start\"] = pd.to_datetime(s[\"train_start\"])\n",
                "        s[\"train_end\"]   = pd.to_datetime(s[\"train_end\"])\n",
                "        s[\"test_start\"]  = pd.to_datetime(s[\"test_start\"])\n",
                "        s[\"test_end\"]    = pd.to_datetime(s[\"test_end\"])\n",
                "    \n",
                "    train_test_pairs = []\n",
                "    for s in splits:\n",
                "        train_mask = (forecast_processed_data[time_column] >= s[\"train_start\"]) \\\n",
                "                   & (forecast_processed_data[time_column] <= s[\"train_end\"])\n",
                "        \n",
                "        test_mask = (forecast_processed_data[time_column] >= s[\"test_start\"]) \\\n",
                "                  & (forecast_processed_data[time_column] <= s[\"test_end\"])\n",
                "\n",
                "        train_df = forecast_processed_data.loc[train_mask].copy()\n",
                "        test_df = forecast_processed_data.loc[test_mask].copy()\n",
                "        train_test_pairs.append((train_df, test_df))\n",
                "        \n",
                "    \n",
                "    train1_df, test1_df = train_test_pairs[0]\n",
                "    train2_df, test2_df = train_test_pairs[1]\n",
                "    train3_df, test3_df = train_test_pairs[2]\n",
                "    train4_df, test4_df = train_test_pairs[3]\n",
                "    \n",
                "    train1_df.to_csv(output_train1.path, index=False)\n",
                "    test1_df.to_csv(output_test1.path, index=False)\n",
                "    \n",
                "    train2_df.to_csv(output_train2.path, index=False)\n",
                "    test2_df.to_csv(output_test2.path, index=False)\n",
                "    \n",
                "    train3_df.to_csv(output_train3.path, index=False)\n",
                "    test3_df.to_csv(output_test3.path, index=False)\n",
                "    \n",
                "    train4_df.to_csv(output_train4.path, index=False)\n",
                "    test4_df.to_csv(output_test4.path, index=False)\n",
                "    \n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "ce3ba943",
            "metadata": {},
            "outputs": [],
            "source": [
                "@dsl.pipeline(\n",
                "    name=\"forecast-training-pipeline\",\n",
                "    description=\"A Kubeflow pipeline for training forecast models using AutoML Forecast on Vertex AI Pipelines from a BigQuery view.\"\n",
                ")\n",
                "def forecast_pipeline(\n",
                "    project_id: str,\n",
                "    project_location: str,\n",
                "    bq_dataset: str,\n",
                "    bq_source_table: str,\n",
                "    time_column: str,\n",
                "    target_column: str,\n",
                "    attribute_columns: List[str]\n",
                "):\n",
                "    query_and_preprocess_task = query_and_preprocess(\n",
                "        project_id=project_id,\n",
                "        project_location=project_location,\n",
                "        bq_dataset=bq_dataset,\n",
                "        bq_source_table=bq_source_table,\n",
                "        time_column=time_column,\n",
                "        target_column=target_column,\n",
                "        attribute_columns=attribute_columns\n",
                "    )\n",
                "    \n",
                "    split_rolling_forecast_task = split_rolling_forecast(\n",
                "        time_column=time_column,\n",
                "        input_dataset=query_and_preprocess_task.outputs['output_dataset']\n",
                "    )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "67017e6a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Creating PipelineJob\n",
                        "PipelineJob created. Resource name: projects/7796273458/locations/northamerica-northeast1/pipelineJobs/forecast-training-pipeline-20250408144422\n",
                        "To use this PipelineJob in another session:\n",
                        "pipeline_job = aiplatform.PipelineJob.get('projects/7796273458/locations/northamerica-northeast1/pipelineJobs/forecast-training-pipeline-20250408144422')\n",
                        "View Pipeline Job:\n",
                        "https://console.cloud.google.com/vertex-ai/locations/northamerica-northeast1/pipelines/runs/forecast-training-pipeline-20250408144422?project=7796273458\n",
                        "PipelineJob projects/7796273458/locations/northamerica-northeast1/pipelineJobs/forecast-training-pipeline-20250408144422 current state:\n",
                        "3\n",
                        "PipelineJob projects/7796273458/locations/northamerica-northeast1/pipelineJobs/forecast-training-pipeline-20250408144422 current state:\n",
                        "3\n",
                        "PipelineJob projects/7796273458/locations/northamerica-northeast1/pipelineJobs/forecast-training-pipeline-20250408144422 current state:\n",
                        "3\n",
                        "PipelineJob projects/7796273458/locations/northamerica-northeast1/pipelineJobs/forecast-training-pipeline-20250408144422 current state:\n",
                        "3\n",
                        "PipelineJob projects/7796273458/locations/northamerica-northeast1/pipelineJobs/forecast-training-pipeline-20250408144422 current state:\n",
                        "3\n",
                        "PipelineJob projects/7796273458/locations/northamerica-northeast1/pipelineJobs/forecast-training-pipeline-20250408144422 current state:\n",
                        "3\n",
                        "PipelineJob run completed. Resource name: projects/7796273458/locations/northamerica-northeast1/pipelineJobs/forecast-training-pipeline-20250408144422\n"
                    ]
                }
            ],
            "source": [
                "compiler.Compiler().compile(\n",
                "    pipeline_func=forecast_pipeline,\n",
                "    package_path=PIPELINE_PACKAGE_PATH\n",
                ")\n",
                "\n",
                "job = pipeline_jobs.PipelineJob(\n",
                "    display_name=\"b2b_wf_short_term_prediction\",\n",
                "    template_path=PIPELINE_PACKAGE_PATH,\n",
                "    parameter_values={\n",
                "        'project_id': PROJECT_ID,\n",
                "        'project_location': PROJECT_REGION,\n",
                "        'bq_dataset': BQ_DATASET_NAME,\n",
                "        'bq_source_table': BQ_SOURCE_TABLE,\n",
                "        'time_column': \"Appointment_Day\",\n",
                "        \"target_column\": \"SWT\",\n",
                "        'attribute_columns': EXPERIMENT_FEATURES\n",
                "    }\n",
                ")\n",
                "\n",
                "job.run()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
